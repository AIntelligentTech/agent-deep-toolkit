{
  "schemaVersion": 3,
  "toolkit": "agent-deep-toolkit",
  "version": "0.3.0",
  "tools": [
    {
      "id": "deep-architect",
      "windsurf": {
        "description": "Act as a world-class software architect and principal engineer to design robust systems using 2026 best practices",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Architect Workflow\n\nThis workflow instructs Cascade to think and act like a principal engineer / software architect, applying modern (2026) architecture practices, computer science fundamentals, and structured trade-off analysis.\n\n## 1. Clarify Problem, Context, and Constraints\n\n- Restate the problem or task in architectural terms.\n- Identify business goals and primary user journeys this work serves.\n- Elicit and list key quality attributes (performance, scalability, reliability, security, operability, compliance, cost, time-to-market, maintainability).\n- Capture hard constraints:\n  - Existing systems and data that must be integrated.\n  - Regulatory / data residency limits.\n  - Team skills, tech stack preferences, hosting/platform constraints.\n- Classify the problem space using the Cynefin lens (simple, complicated, complex, chaotic) to decide whether to favor best practices, analysis, experimentation, or stabilization.\n\n## 2. Understand the Domain and Boundaries (DDD-Inspired)\n\n- Build a concise domain model from the description and available artifacts:\n  - Identify core entities, value objects, aggregates, and key workflows.\n  - Distinguish core, supporting, and generic subdomains.\n- Propose candidate bounded contexts and their responsibilities.\n- Note where data and language differ between contexts (ubiquitous language and translation boundaries).\n- Map external systems and upstream/downstream dependencies.\n\n## 3. System Decomposition Using the C4 Model\n\n- Think through the system at three main C4 levels (textual, no diagrams required):\n  - **System Context**\n    - Who are the primary actors (users, external systems)?\n    - What responsibilities does this system have relative to them?\n  - **Containers**\n    - Propose containers (e.g., SPA, API, background workers, databases, caches, message brokers).\n    - Describe each container’s responsibilities, tech candidates, and communication styles (HTTP/REST, gRPC, events, queues).\n  - **Components**\n    - Within key containers, sketch major components/modules and their responsibilities (e.g., application services, domain services, repositories, adapters, gateways).\n- Highlight layering or hexagonal boundaries (UI / application / domain / infrastructure) and how dependencies should flow.\n\n## 4. Choose Architectural Styles and Key Patterns\n\n- Enumerate candidate architectural styles (e.g., layered monolith, modular monolith, microservices, event-driven, CQRS+ES) and briefly assess:\n  - Fit to domain complexity and team size.\n  - Operational overhead and failure modes.\n  - Alignment with quality attributes (e.g., independent scaling, deployment frequency, data consistency needs).\n- When relevant, apply:\n  - **Hexagonal / Clean Architecture** to isolate domain from frameworks.\n  - **CQRS** where read/write concerns diverge strongly.\n  - **Event-driven** patterns when decoupling and eventual consistency are valuable.\n- Map design patterns where they materially help:\n  - Structural (Adapter, Facade, Proxy) for integration and seams.\n  - Behavioral (Strategy, Observer, Command) for variability and workflows.\n  - Integration/resilience patterns (Circuit Breaker, Retry, Bulkhead, Saga, Outbox) for distributed systems.\n\n## 5. Analyze Algorithms, Data, and Scaling\n\n- Identify hot paths and data-intensive workflows.\n- For critical operations, reason explicitly about:\n  - Time and space complexity (Big-O) of key algorithms.\n  - Expected data volumes, throughput, and latency targets.\n- Propose data storage and indexing strategies aligned with access patterns.\n- Consider caching layers (client, edge, application, database) and invalidation strategies.\n- Outline horizontal vs vertical scaling strategies and where statefulness may constrain scaling.\n\n## 6. Cross-Cutting Concerns: Security, Reliability, and Observability\n\n- **Security / Privacy**\n  - Perform a lightweight STRIDE-style pass over major data flows (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege).\n  - Note authn/authz model, least-privilege boundaries, data classification, and encryption (in transit/at rest).\n  - Reference OWASP Top 10 (2025) for likely risks given the stack.\n- **Reliability & Resilience**\n  - Define SLO-ish expectations (e.g., uptime, acceptable error rates) where possible.\n  - Identify single points of failure and propose mitigations (redundancy, graceful degradation, backpressure).\n- **Observability**\n  - Describe logging, metrics, and tracing strategy.\n  - Specify key health checks and dashboards required for safe operation and incident response.\n\n## 7. Trade-off Evaluation and Option Comparison\n\n- When there are competing architecture options, apply a structured comparison:\n  - Use a simple **decision matrix** or RICE/ICE-style scoring across criteria like value, risk, complexity, cost, and reversibility.\n  - For major architectural decisions, outline quality attribute scenarios (stimulus, environment, response, response measure) and reason which option best satisfies them (ATAM-style thinking).\n- Make trade-offs explicit:\n  - What are we optimizing for now vs later?\n  - What debts are we intentionally taking on, and how will we service them?\n\n## 8. Incremental Delivery, Risks, and Documentation\n\n- Propose an incremental path:\n  - Identify a “walking skeleton” or thin vertical slices to validate the architecture early.\n  - Call out spikes/experiments needed to de-risk unknowns (performance, new tech, vendor choices).\n- List top architecture risks and mitigations.\n- Capture decisions as a concise ADR-style summary:\n  - Context, decision, options considered, rationale, and consequences.\n- Summarize the recommended architecture in clear language for both technical and non-technical stakeholders, including open questions to clarify with the user.\n\n## 9. Evolve and Safeguard the Architecture\n\n- Treat the architecture as **evolutionary**, not static:\n  - Expect requirements, scale, and team structure to change over time.\n  - Prefer options that keep future choices open (high reversibility, low lock-in) when uncertainty is high.\n- Define lightweight **fitness functions** for key quality attributes:\n  - Automated checks or metrics that continuously assert properties such as performance thresholds, dependency rules, security baselines, or latency budgets.\n  - Integrate these checks into CI where feasible so architectural regressions are caught early.\n- Periodically review architecture against real usage:\n  - Compare assumed vs actual traffic patterns, data growth, and failure modes.\n  - Use production telemetry (logs, metrics, traces) to validate that the architecture is behaving as intended.\n- Use code- and architecture-level safeguards:\n  - Enforce boundaries with module/dependency rules or architecture tests.\n  - Watch for erosion indicators (growing God modules, cyclic dependencies, repeated ad-hoc integrations) and schedule targeted refactors.\n- Encourage regular, time-boxed architecture reviews or katas:\n  - Revisit earlier decisions in light of new information.\n  - Capture outcomes as updated ADRs so the evolution path remains explicit and explainable.\n"
    },
    {
      "id": "deep-audit",
      "windsurf": {
        "description": "Perform a structured audit of a codebase or system across architecture, quality, security, and operational readiness",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Audit Workflow\n\nThis workflow instructs Cascade to conduct a systematic review, producing a clear, prioritized findings report rather than ad-hoc comments.\n\n## 1. Define Scope, Objectives, and Standards\n\n- Clarify the audit scope:\n  - Entire system vs specific service, module, or feature.\n- Identify objectives:\n  - Security posture, code quality, architecture health, performance readiness, compliance.\n- Select reference standards and checklists:\n  - OWASP Top 10, secure coding guidelines, internal style guides, architecture principles, testing standards.\n\n## 2. Collect Artifacts and Context\n\n- Gather:\n  - Code, configs, infrastructure definitions, CI/CD pipelines, logs/metrics, existing docs.\n- Understand constraints:\n  - Regulatory requirements, SLAs/SLOs, uptime expectations, data residency.\n- Use `search_web` sparingly to confirm current best practices for any unfamiliar frameworks or technologies.\n\n## 3. Assess Architecture and Design\n\n- Evaluate alignment with intended architecture (e.g., C4 diagrams, documented patterns).\n- Look for:\n  - Excessive coupling, unclear boundaries, and God components.\n  - Data ownership clarity and cross-context dependencies.\n  - Use (or misuse) of patterns like microservices, event-driven design, or CQRS.\n\n## 4. Assess Code Quality and Maintainability\n\n- Sample representative areas of the codebase:\n  - Hot paths, complex modules, and recently changed files.\n- Check for:\n  - Readability, clear naming, and small, focused functions.\n  - Encapsulation, duplication, and adherence to established conventions.\n  - Test coverage and test quality around critical behavior.\n\n## 5. Assess Security and Data Protection\n\n- Review authentication and authorization flows.\n- Check for common classes of vulnerabilities:\n  - Injection, broken access control, insecure deserialization, weak crypto, secrets in code or configs.\n- Verify logging and monitoring for security-relevant events.\n\n## 6. Assess Operational Readiness\n\n- Examine deployment and rollback mechanisms.\n- Review observability:\n  - Metrics, logging, tracing, alerts, and dashboards.\n- Consider resilience and failure handling:\n  - Timeouts, retries, backoff, circuit breakers, graceful degradation.\n\n## 7. Synthesize Findings and Recommendations\n\n- Group findings by category and severity (e.g., Critical, High, Medium, Low).\n- For each finding:\n  - Describe the issue, its impact, and supporting evidence.\n  - Propose concrete remediation steps or next investigations.\n- Summarize systemic themes (e.g., testing gaps, architectural drift, repeated security smells).\n- Produce a concise report that can be tracked as work items in the backlog.\n"
    },
    {
      "id": "deep-code",
      "windsurf": {
        "description": "Implement high-quality code using solid design principles, refactoring, and thorough testing",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Code Workflow\n\nThis workflow instructs Cascade to focus on implementation quality: clear design, clean code, strong tests, safe integration, and disciplined version control following the **Automated Iterative Development (AID)** methodology.\n\n## 1. Understand Behavioral and Interface Requirements\n\n- Restate what the code must do in precise terms:\n  - Inputs, outputs, side effects, error conditions, and performance expectations.\n- Identify callers and consumers:\n  - Public API surface, internal modules, external systems.\n- Note constraints:\n  - Backwards compatibility, security/privacy requirements, latency budgets.\n- **Decompose complex work into committable phases** if the task spans multiple logical units.\n\n## 2. Shape the Design at Code Level\n\n- Choose appropriate abstractions:\n  - Functions, classes, modules, interfaces, or patterns that map cleanly to the domain.\n- Apply core design principles where they help:\n  - Single Responsibility, Separation of Concerns, DRY, explicit dependencies.\n- Define clear boundaries:\n  - Pure vs impure code, domain vs infrastructure, sync vs async.\n\n## 3. Plan Tests and Contracts\n\n- Decide on test strategy:\n  - Unit, integration, and/or property-based tests as appropriate.\n- Identify key scenarios and edge cases:\n  - Happy paths, invalid inputs, boundary values, external failures, concurrency issues.\n- Where relevant, specify invariants and contracts:\n  - Preconditions, postconditions, and assertions at module boundaries.\n\n## 4. Implement in Small, Verifiable Steps\n\n- Work in small increments:\n  - Add or update a test, then implement or adjust code to satisfy it.\n- Keep functions and methods focused:\n  - Prefer composable, readable code over cleverness.\n- Use clear naming and structure to make intent obvious.\n- **Code quality checklist:**\n  - [ ] Follows existing project conventions\n  - [ ] Handles error cases gracefully\n  - [ ] Includes necessary imports at file top\n  - [ ] No hardcoded values that should be configurable\n  - [ ] Compatible with strict modes (`set -e` for bash, `strict: true` for TypeScript)\n\n## 5. Validate Before Committing\n\n- **Apply validation hierarchy** before any commit:\n  1. **Syntax**: Does the code parse/compile?\n  2. **Unit**: Do individual functions work?\n  3. **Integration**: Do components work together?\n  4. **End-to-end**: Does the full flow work?\n- **Validation commands by type:**\n  | Artifact Type | Validation Method                                      |\n  |---------------|--------------------------------------------------------|\n  | Bash scripts  | `bash -n script.sh` (syntax), then execute with args   |\n  | TypeScript    | `tsc --noEmit` or `bun run typecheck`                  |\n  | Python        | `python -m py_compile file.py`, then test execution    |\n  | JSON          | `jq . file.json`                                       |\n- **When validation fails:**\n  1. Diagnose root cause, not just the symptom\n  2. Fix minimally\n  3. Re-validate\n  4. Document what went wrong for future reference\n\n## 6. Refactor Continuously with Safety Nets\n\n- After functionality is in place and tests pass:\n  - Look for opportunities to simplify, remove duplication, and improve structure.\n- Apply well-known refactorings (extract function, introduce parameter object, move method, etc.) in small steps.\n- Run tests frequently to ensure behavior remains correct.\n\n## 7. Commit at Logical Boundaries\n\n- **Atomic commits**: Each commit represents one logical, working change.\n- **Descriptive messages**: Explain what and why, not just what.\n- **Phase boundaries**: Commit at the end of each logical phase.\n- **Never commit broken or partial code.**\n- **Commit message template:**\n  ```text\n  [Type]: [Brief description]\n\n  - [Specific change 1]\n  - [Specific change 2]\n\n  [Context if part of larger work]\n  ```\n\n## 8. Integrate and Handle Cross-Cutting Concerns\n\n- Ensure the new or changed code:\n  - Fits existing error-handling and logging conventions.\n  - Respects security constraints (input validation, data sanitization, least privilege).\n- Consider observability:\n  - Add or update logs/metrics where they will help diagnose future issues.\n- Verify interactions with external systems:\n  - API contracts, database schemas, message formats.\n\n## 9. Final Review and Clean-Up\n\n- Self-review the changes:\n  - Readability, consistency with surrounding code, adherence to project standards.\n- Confirm tests:\n  - Ensure new tests are meaningful and not brittle.\n- Remove temporary instrumentation or debugging code before finalizing.\n- Summarize key implementation decisions briefly in comments or linked specs/ADRs where appropriate.\n- **Provide summary** of commits made, deliverables created, and validation results.\n"
    },
    {
      "id": "deep-consider",
      "windsurf": {
        "description": "Carefully consider complex decisions, options, and edge cases using formal decision-making frameworks and scenario analysis",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Consider Workflow\n\nThis workflow instructs Cascade to slow down and apply structured decision-making and foresight techniques before recommending a path.\n\n## 1. Frame the Decision and Purpose\n\n- Restate the decision as a concrete question (including time horizon and scope).\n- Apply a **Golden Circle** pass:\n  - **Why** – underlying purpose, goals, and principles.\n  - **How** – broad strategies that could achieve the why.\n  - **What** – concrete options or actions under consideration.\n- Enumerate explicit options, including:\n  - The status quo (do nothing/change nothing).\n  - At least one “minimal” and one “maximal” option where applicable.\n\n## 2. Map Context, Uncertainty, and Complexity\n\n- Build a quick **CSD Matrix**:\n  - Certainties (facts, constraints, invariants).\n  - Suppositions (assumptions that need validation).\n  - Doubts (unknowns and knowledge gaps).\n- Classify the situation via the **Cynefin framework**:\n  - Simple, Complicated, Complex, or Chaotic.\n- Choose decision posture based on Cynefin:\n  - Simple: follow proven best practices.\n  - Complicated: analyze and consult expertise.\n  - Complex: run safe-to-fail experiments and probe-sense-respond.\n  - Chaotic: act to stabilize first, then reassess.\n\n## 3. Expand and Refine Options\n\n- Check whether the option set is too narrow:\n  - Derive variants that trade cost, speed, and quality differently.\n  - Include options that defer or split the decision (phased approaches, pilots).\n- For each option, capture enabling and blocking constraints (funding, skills, dependencies, risk appetite).\n\n## 4. Define Evaluation Criteria\n\n- Co-create or infer criteria aligned with the **Why**:\n  - Value/impact, cost, risk, reversibility, strategic alignment, learning potential, user experience, team health.\n- Distinguish **must-have** vs **nice-to-have** criteria.\n- If monetary implications are central, plan for a cost-benefit style analysis.\n\n## 5. Evaluate Options with Structured Tools\n\n- For multi-criteria choices, use a **decision matrix**:\n  - List options vs criteria.\n  - Assign weights to criteria based on importance.\n  - Score options and compute weighted totals.\n- For roadmap / initiative prioritization, optionally apply **RICE/ICE**:\n  - Reach, Impact, Confidence, Effort (RICE) or Impact, Confidence, Ease (ICE).\n- When economics dominate, outline a **Cost-Benefit Analysis**:\n  - Frame the question and current situation.\n  - List and categorize costs/benefits (direct, indirect, short/long term).\n  - Estimate magnitudes (even if rough) and compare net benefit.\n\n## 6. Explore Edge Cases, Failure Modes, and Second-Order Effects\n\n- Perform a lightweight **pre-mortem**:\n  - Assume the chosen option failed badly in 6–12 months – list reasons why.\n- For each major option, consider:\n  - Technical failure modes (scalability, security, maintainability, data integrity).\n  - Organizational and product impacts (team workload, UX, customer trust, legal/compliance).\n  - Second-order effects and path dependence (lock-in, opportunity cost, future flexibility).\n- Highlight where uncertainty is highest and could be reduced via experiments or prototypes.\n\n## 7. Synthesize Recommendation and Guardrails\n\n- Select the recommended option (or sequence of options) and justify it:\n  - How it scores against key criteria.\n  - Why its risks are acceptable relative to alternatives.\n- Make trade-offs explicit:\n  - What you are intentionally not optimizing for right now.\n  - What you are postponing or consciously discarding.\n- Propose guardrails:\n  - Leading indicators/metrics to monitor.\n  - Checkpoints to revisit the decision.\n  - Preconditions that would trigger reconsideration.\n\n## 8. Communicate Clearly and Capture the Decision\n\n- Summarize the decision in a concise narrative for stakeholders:\n  - Context and question.\n  - Options considered.\n  - Criteria and key arguments.\n  - Recommendation and next steps.\n- Capture remaining open questions and suggested experiments to further de-risk the path.\n- When appropriate, structure the summary so it can be recorded as an ADR or decision log entry.\n"
    },
    {
      "id": "deep-data",
      "windsurf": {
        "description": "Design trustworthy data models, quality controls, and governance for analytics and AI",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Data Workflow\n\nThis workflow instructs Cascade to treat data as a product: modeled clearly, validated continuously, and governed responsibly.\n\n## 1. Clarify Data Use Cases and Stakeholders\n\n- Identify decisions, models, and reports that depend on the data.\n- List stakeholders:\n  - Producers, consumers, data owners, and downstream teams.\n- Classify data domains (e.g., customer, product, billing, events).\n\n## 2. Map Sources, Lineage, and Ownership\n\n- Inventory upstream systems and pipelines that feed the data.\n- Establish clear ownership for each key dataset or table.\n- Sketch high-level lineage from raw inputs to curated outputs.\n\n## 3. Design Data Models and Contracts\n\n- Choose modeling approaches appropriate to the stack (e.g., star schema, data vault, event models).\n- Define schemas, primary keys, relationships, and naming conventions.\n- Specify **data contracts** between producers and consumers:\n  - Schemas, SLAs for freshness, and expectations around nulls and defaults.\n\n## 4. Define Data Quality Checks and Monitoring\n\n- Identify quality dimensions that matter:\n  - Freshness, completeness, uniqueness, validity, consistency, and accuracy.\n- Implement automated checks where possible:\n  - Threshold-based alerts, anomaly detection, schema change monitoring.\n- Integrate checks into pipelines and CI/CD for data (\"data tests\").\n\n## 5. Address Privacy, Security, and Governance\n\n- Classify data sensitivity (e.g., public, internal, personal, special categories).\n- Align access controls, encryption, and retention with classification.\n- Ensure compliance with relevant regulations and policies by using `/workflow-deep-regulation` and `/workflow-deep-ethics` where appropriate.\n\n## 6. Document Semantics and Usage\n\n- Write clear documentation for datasets:\n  - Definitions of fields, units, caveats, and known issues.\n- Link data docs to dashboards, models, and application code that use them.\n- Encourage feedback loops when consumers find inconsistencies or gaps.\n\n## 7. Evolve Data Models Safely\n\n- Manage schema changes with deprecation plans and migration paths.\n- Use feature flags, dual-write/read strategies, or views to smooth transitions.\n- Periodically review data models and quality metrics to ensure they still fit evolving product and AI needs.\n"
    },
    {
      "id": "deep-debug",
      "windsurf": {
        "description": "Perform deep, systematic debugging to find true root causes and design robust, well-tested fixes",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Debug Workflow\n\nThis workflow instructs Cascade to debug like an experienced tester and engineer combined: methodical reproduction, deep code understanding, hypothesis-driven experiments, and prevention-focused fixes.\n\n## 1. Clarify, Triage, and Reproduce\n\n- Restate the problem in precise, observable terms:\n  - Expected vs actual behavior.\n  - Error messages, stack traces, logs, and user-visible symptoms.\n- Capture environment details:\n  - Versions (app, dependencies, platform, browser/OS).\n  - Config flags, feature toggles, data conditions.\n- Establish a reliable reproduction path:\n  - Document exact steps.\n  - Strive to minimize the repro to the smallest scenario that still fails.\n- Assess impact and urgency (severity, affected users, business risk) to guide depth vs speed.\n\n## 2. Map and Trace the Code Path in Detail\n\n- Start from the **entrypoint** used in your repro:\n  - HTTP route, RPC handler, CLI command, queue consumer, cron job, etc.\n  - Locate the corresponding handler/controller using `code_search` and `grep_search`.\n- Use stack traces and logs to identify the **observed call stack** at the failure point:\n  - Note the top few frames (entry) and the bottom frames (where the failure surfaced).\n  - Distinguish framework/runtime plumbing from your own application code.\n- Build a **mental call graph** for the failing path:\n  - From the entry handler, follow function calls forward into services, repositories, and utilities.\n  - Note branches, loops, callbacks, and asynchronous boundaries (promises, background tasks, events).\n  - Record this as a short textual outline so it can be updated as you learn more.\n- Use targeted instrumentation to **dynamically trace** execution when helpful:\n  - Temporary logs at function entry/exit with key parameters and identifiers.\n  - Correlation IDs to follow a single request across services.\n  - If available, enable distributed tracing or framework-level request tracing around the failing scenario.\n- Identify and annotate **integration and state boundaries** along the path:\n  - Network calls (APIs, message queues, external services).\n  - Database queries, caches, file systems, and other side effects.\n  - Feature flags and configuration reads that may change behaviour.\n- Relate this traced path back to the broader architecture:\n  - Which bounded context and C4 container/component does it live in?\n  - Are there other code paths that share components or data with this one (potential collateral impact)?\n\n## 3. Form Hypotheses (Tester’s Mindset)\n\n- List plausible classes of defects given the symptoms:\n  - Data/contract mismatches, null/undefined handling, off-by-one, race conditions, caching inconsistencies, time-zone or locale issues, precision/rounding, permission checks, performance timeouts, etc.\n- Use a quick **CSD Matrix**:\n  - What do we know for sure about the failure?\n  - What are working assumptions?\n  - What remains unclear?\n- Prioritize hypotheses based on alignment with evidence and historical defect patterns in this codebase.\n\n## 4. Improve Observability and Testability\n\n- Apply testability heuristics:\n  - Can the failing area be exercised through a smaller, more focused unit or integration test?\n  - Is the behavior observable via logs/metrics/traces at the right granularity?\n- Add or refine instrumentation (temporarily if needed):\n  - Structured logs with key inputs, branches, and outputs.\n  - Metrics or counters around suspected hotspots.\n- Create or adjust automated tests to:\n  - Reproduce the failure in a controlled environment (ideally as a failing test).\n  - Cover nearby edge cases that might share the same root cause.\n\n## 5. Run Targeted Experiments and Use the Web Wisely\n\n- Use the debugger, REPL, or print-style debugging selectively to inspect state.\n- Vary inputs, environment variables, and feature flags to see how the behavior changes.\n- Use `search_web` when:\n  - Error messages mention framework/runtime internals.\n  - Behavior is tied to specific versions of libraries or platforms.\n- Verify that online solutions:\n  - Match the actual versions and stack in use.\n  - Are current (2024–2026), or intentionally apply older approaches only when working with legacy code.\n\n## 6. Converge on Root Cause (Not Just the Symptom)\n\n- Once a candidate fix emerges, apply **5 Whys** to ensure you have gone deep enough:\n  - Why did this specific error occur?\n  - Why did the system allow this state or input?\n  - Why was it not caught by tests or monitoring?\n- Optionally use a mini **Fishbone** lens (People, Process, Platform, Code, Data) to see if broader factors contributed.\n- Check for similar vulnerable areas (e.g., same pattern used elsewhere) to avoid “one-off” patches.\n\n## 7. Design a Safe, Sustainable Fix\n\n- Favor the smallest change that fully addresses the root cause and fits the architecture.\n- Consider:\n  - Performance implications (e.g., added checks vs hot paths).\n  - Security implications (e.g., new validation, error messaging).\n  - User experience (e.g., clearer error handling, graceful degradation).\n- Strengthen automated tests:\n  - Turn repro into a regression test.\n  - Add tests for adjacent edge cases to guard against similar issues.\n- Where appropriate, add assertions or invariants at boundaries to prevent reintroduction.\n\n## 8. Validate, Monitor, and Learn\n\n- Re-run the original repro and a broader smoke/regression check around the affected area.\n- If applicable, validate in staging or with feature flags/canary releases before full rollout.\n- After deployment, monitor logs/metrics for:\n  - Recurrence of the error.\n  - New anomalies introduced by the fix.\n- Capture a brief “bug narrative”:\n  - Root cause.\n  - How it was found and fixed.\n  - What changed in tests/monitoring/process to prevent similar bugs in future.\n"
    },
    {
      "id": "deep-decide",
      "windsurf": {
        "description": "Make rigorous, well-documented decisions using structured frameworks and evidence",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Decision Workflow\n\nThis workflow instructs Cascade to move from options to a clear, justified decision and follow-through.\n\n## 1. Confirm Decision Scope and Ownership\n\n- Restate the decision as a precise question, including scope and time horizon.\n- Identify the decision owner and key stakeholders.\n- Clarify decision type:\n  - One-way door (hard to reverse) vs two-way door (easy to revisit).\n\n## 2. Synthesize Context and Options\n\n- Summarize relevant background, constraints, and goals.\n- Enumerate candidate options, including:\n  - Status quo and at least one minimal and one ambitious option.\n- Use `/workflow-deep-consider` if the option space or context is complex.\n\n## 3. Select Decision Frameworks and Criteria\n\n- Choose appropriate tools:\n  - Decision matrix, expected value, regret minimization, OODA loop, or similar.\n- Define evaluation criteria aligned with goals and values:\n  - Impact, cost, risk, reversibility, learning potential, strategic alignment.\n- Weight criteria where necessary to reflect priorities.\n\n## 4. Gather Evidence and Run Targeted Analyses\n\n- Identify critical unknowns and assumptions.\n- Use `/workflow-deep-search`, `/workflow-deep-experiment`, or `/workflow-deep-investigate` to reduce uncertainty.\n- Avoid analysis paralysis:\n  - Focus effort where it meaningfully changes the decision.\n\n## 5. Choose and Record the Decision\n\n- Select the preferred option based on the chosen framework and evidence.\n- Make trade-offs explicit:\n  - What is being optimized, what is being deferred, and what risks are accepted.\n- Record the decision in a durable form (e.g., ADR, decision log) with:\n  - Context, options, criteria, rationale, and expected outcomes.\n\n## 6. Plan Implementation, Guardrails, and Monitoring\n\n- Define concrete next steps, owners, and timelines.\n- Set leading indicators and guardrails to detect if the decision is going badly.\n- Decide on review checkpoints and conditions that would trigger reconsideration.\n\n## 7. Review and Learn from Decisions\n\n- At review time, compare outcomes with expectations:\n  - What worked, what surprised, and what assumptions were wrong.\n- Capture lessons:\n  - How to improve future decision-making processes and heuristics.\n- Update documentation, playbooks, or strategies based on what was learned.\n"
    },
    {
      "id": "deep-design-token",
      "windsurf": {
        "description": "Extract and systematize design tokens from existing interfaces into a reliable design system foundation",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Design Token Workflow\n\nThis workflow instructs Cascade to reverse-engineer and codify a UI’s visual language as robust design tokens, ready for use in design tools and frontend code.\n\n## 1. Clarify Scope, Targets, and Goals\n\n- Define what you are extracting tokens from:\n  - Single component, page, flow, product surface, or full design system.\n- Clarify the goal of tokenization:\n  - Consistency across products, theming/dark mode, cross-platform reuse, or migration from ad-hoc styles.\n- Note constraints:\n  - Platforms in scope (web, iOS, Android, desktop), existing design tools, and frontend stacks.\n\n## 2. Inventory the Existing Visual Language\n\n- Using design files and/or frontend inspection tools, systematically capture:\n  - **Color**: background, foreground, accent, border, states (hover, focus, active, disabled), semantic roles (success, warning, error, info).\n  - **Typography**: font families, sizes, weights, line heights, letter spacing, text transforms.\n  - **Spacing and sizing**: margins, paddings, gaps, layout grid, container widths.\n  - **Radii, borders, and shadows**: corner radii families, border widths/styles, elevation patterns.\n  - **Motion and timing**: durations, easing curves, common animation patterns.\n- Look for implicit scales (e.g., 4/8/12px spacing, typographic steps) rather than one-off values.\n\n## 3. Derive Primitive Token Scales\n\n- Normalize raw values into **primitive tokens**:\n  - E.g., `color.blue.50/100/500`, `space.0/1/2/3`, `radius.sm/md/lg`, `shadow.xs/sm/md/lg`, `duration.fast/normal/slow`.\n- Ensure scales are coherent and minimal:\n  - Prefer a small set of well-chosen steps over many near-duplicates.\n- Capture these primitives in a tool-agnostic format (e.g., JSON/YAML) to support multiple platforms.\n\n## 4. Define Semantic Tokens and Modes\n\n- Map primitives to **semantic tokens** that express UI meaning:\n  - E.g., `color.bg.surface`, `color.text.muted`, `color.border.focus`, `color.action.primary`, `color.feedback.success.bg`.\n- Consider multiple modes (light/dark, brand variants, high-contrast):\n  - Keep semantic names stable while swapping underlying primitive values per mode.\n- Use naming that aligns with how designers and engineers describe the UI, not implementation details.\n\n## 5. Validate Tokens by Rebuilding Key Surfaces\n\n- Choose representative components and screens:\n  - Primary buttons, inputs, alerts, cards, navigation, and at least one complex page.\n- Attempt to rebuild them **only using tokens**:\n  - Adjust primitives or semantic mappings where the reconstruction diverges noticeably from the intended design.\n- Capture gaps revealed during this exercise:\n  - Missing tokens, ambiguous names, or overly broad semantics.\n\n## 6. Plan Frontend Implementation and Tooling\n\n- Decide how tokens will be represented in code:\n  - CSS variables, theme objects, Style Dictionary pipelines, platform-specific exports (iOS, Android), or a combination.\n- Align file structure and naming across:\n  - Design tool libraries, token source of truth, and frontend consumption.\n- Consider build and distribution strategy:\n  - Versioning, package boundaries, and how teams adopt token updates.\n\n## 7. Integrate with Components, UX, and Tests\n\n- Ensure components use tokens rather than hard-coded values:\n  - Map semantic tokens to component props, variants, and states.\n- Coordinate with `/workflow-deep-ux` and `/workflow-deep-polish`:\n  - Use tokens to implement visual hierarchy, states, and brand expression.\n- Incorporate tokens into `/workflow-deep-test` strategy where relevant:\n  - Visual regression tests, snapshot tests, or contract tests for theming.\n\n## 8. Govern Token Evolution\n\n- Define contribution and review processes for token changes:\n  - Who can add/modify tokens, how proposals are evaluated, and how breaking changes are managed.\n- Monitor usage and drift:\n  - Identify unused tokens and hard-coded values that should be migrated.\n- Keep documentation current:\n  - Token catalogs, usage guidelines, and examples for designers and engineers.\n"
    },
    {
      "id": "deep-design",
      "windsurf": {
        "description": "Apply structured product and interaction design thinking (Double Diamond) to shape solutions before implementation",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Design Workflow\n\nThis workflow instructs Cascade to use modern design-thinking patterns to move from problem to solution in a disciplined way.\n\n## 1. Discover: Understand Problem and Context\n\n- Clarify business goals and success metrics.\n- Identify primary users and their tasks.\n- Review existing product behavior, analytics, and support insights.\n- Use `search_web` where needed to explore comparable products and patterns.\n\n## 2. Define: Frame the Right Problem\n\n- Synthesize findings into:\n  - Problem statements and user needs.\n  - Constraints and non-goals.\n- Map user journeys and key moments that matter.\n- Prioritize which part of the journey to address first.\n\n## 3. Develop: Explore Solution Space\n\n- Generate multiple solution ideas, not just one.\n- Use variations that trade off complexity vs value.\n- Sketch information architecture and interaction flows at a coarse level.\n- Consider cross-platform implications (web, mobile, responsive breakpoints) if relevant.\n\n## 4. Deliver: Converge and Specify\n\n- Choose a direction based on value, feasibility, and risk.\n- Elaborate the chosen solution:\n  - Screen/flow narratives, states, and edge cases.\n  - Component hierarchy and layout for implementation.\n- Capture constraints and open questions explicitly.\n\n## 5. Prepare for Implementation\n\n- Translate design decisions into artifacts developers can use:\n  - Annotated mockups, component specs, acceptance criteria, UX success measures.\n- Highlight dependencies and sequencing (what can be built first, what can be stubbed).\n- Ensure that key UX flows are clearly documented and testable.\n\n## 6. Plan for Validation and Iteration\n\n- Define how success will be measured after release:\n  - UX metrics, behavioral analytics, qualitative feedback.\n- Propose small experiments or A/B tests where appropriate.\n- Capture a short design rationale so future changes can understand trade-offs made today.\n"
    },
    {
      "id": "deep-document",
      "windsurf": {
        "description": "Create high-quality, durable technical documentation and specs aligned with modern docs-as-code and architecture practices",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Document Workflow\n\nThis workflow instructs Cascade to write documentation that is clear, useful, and maintainable, not just descriptive prose.\n\n## 1. Clarify Purpose, Audience, and Lifespan\n\n- Identify the primary purpose:\n  - Onboarding, reference, decision record, how-to, conceptual overview, or incident/postmortem.\n- Identify audiences and their needs:\n  - New contributors, core maintainers, stakeholders, SREs, auditors, end users.\n- Estimate lifespan and update frequency to choose appropriate depth and format.\n\n## 2. Choose the Right Document Type and Structure\n\n- Select a doc type aligned with purpose:\n  - README/overview, design doc, ADR (Architecture Decision Record), RFC, API reference, runbook, troubleshooting guide.\n- Use known patterns:\n  - For ADRs: Context, Decision, Alternatives, Rationale, Consequences.\n  - For design docs: Problem, Goals/Non-goals, Context, Proposed Design, Alternatives, Risks, Rollout, Monitoring.\n- Sketch a clear outline before drafting to ensure flow.\n\n## 3. Gather Inputs and Ground in Reality\n\n- Collect relevant artifacts:\n  - Code, diagrams, tickets, discussions, metrics, existing docs.\n- Use `code_search` and `grep_search` to anchor statements in actual behavior and structure.\n- When needed, use `search_web` for current best practices in documenting the specific technology or pattern.\n\n## 4. Draft for Clarity, Then Detail\n\n- Start with a concise summary:\n  - What this is, who it’s for, and what readers will get from it.\n- Write short, active sentences and concrete examples.\n- Prefer diagrams and tables where they convey structure more clearly than prose.\n- Call out assumptions, constraints, and edge cases explicitly.\n\n## 5. Connect Docs to Architecture and Code\n\n- Link to relevant modules, services, ADRs, tickets, and dashboards.\n- Where appropriate, describe the system using a C4-inspired structure (Context, Containers, Components) in text or diagrams.\n- Ensure that documented flows match real call paths and data flows observed in code and telemetry.\n\n## 6. Review for Accuracy, Gaps, and Consumption\n\n- Self-review:\n  - Check for ambiguity, missing pre-requisites, and undocumented edge cases.\n  - Verify all technical claims against code or authoritative sources.\n- Consider readers’ workflows:\n  - Is this doc discoverable from the relevant code, repo root, or command?\n  - Does it answer the questions they are most likely to have?\n\n## 7. Plan for Maintenance and Evolution\n\n- State how and when the document should be updated (triggers such as releases, major refactors, or dependency upgrades).\n- Where possible, keep documentation close to code (docs-as-code) and reference it from tests or CI checks.\n- Encourage lightweight ADRs or changelogs to record future significant decisions that affect this doc.\n"
    },
    {
      "id": "deep-ethics",
      "windsurf": {
        "description": "Evaluate and mitigate ethical risks in AI systems and product decisions",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Ethics Workflow\n\nThis workflow instructs Cascade to surface and address ethical risks, especially in AI-enabled and data-intensive systems.\n\n## 1. Frame the System and Contexts\n\n- Describe the system or feature:\n  - Purpose, capabilities, users, and non-users who may be affected.\n- Identify contexts of use:\n  - Everyday, high-stakes, vulnerable users, or sensitive domains.\n- Note business incentives that might conflict with user or societal interests.\n\n## 2. Identify Stakeholders and Potential Harms\n\n- List stakeholder groups:\n  - Direct users, people represented in data, bystanders, and societal groups.\n- Brainstorm possible harms:\n  - Discrimination, exclusion, misinformation, manipulation, loss of autonomy, safety risks, reputational damage.\n- Pay attention to historically marginalized or vulnerable populations.\n\n## 3. Analyze Fairness, Transparency, and Autonomy\n\n- Ask how decisions are made:\n  - Human, algorithmic, or mixed; where learning models are used.\n- Consider fairness risks:\n  - Biased data, proxy variables, unjustified disparities between groups.\n- Evaluate transparency and control:\n  - Can users understand what the system is doing and why? Can they opt out or contest outcomes?\n\n## 4. Check Legal and Policy Frameworks\n\n- Use `/workflow-deep-search` to identify applicable laws and policies for the jurisdiction and time (e.g., data protection, AI-specific regulations, consumer protection, sector rules).\n- Distinguish between **legal compliance** and **ethical aspiration**:\n  - Note where the system might be legal but still ethically questionable.\n- Align with internal codes of conduct or responsible AI guidelines if available.\n\n## 5. Design Mitigations and Guardrails\n\n- Propose mitigations at multiple levels:\n  - Data collection and labeling, model design, user interface, process and oversight.\n- Consider:\n  - Consent and choice architectures, explanation mechanisms, rate limits, human review, escalation paths.\n- Prefer changes that **reduce systemic risk**, not just surface-level messaging.\n\n## 6. Plan Monitoring, Feedback, and Redress\n\n- Define signals that might indicate ethical problems post-deployment:\n  - Complaints, appeals, error reports, watchdog or regulator feedback.\n- Ensure there are channels for affected users to raise concerns and seek redress.\n- Plan reviews at regular intervals or after significant scope changes.\n\n## 7. Capture the Ethics Assessment\n\n- Summarize ethical issues considered, mitigations chosen, and open questions.\n- Document trade-offs and rationales, including what values were prioritized.\n- Encourage periodic re-evaluation as norms, laws, and usage patterns evolve.\n"
    },
    {
      "id": "deep-experiment",
      "windsurf": {
        "description": "Design and run high-quality experiments to validate ideas with evidence",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Experiment Workflow\n\nThis workflow instructs Cascade to design and run rigorous experiments so decisions are grounded in evidence, not opinion.\n\n## 1. Clarify Hypothesis and Decision\n\n- Restate the experiment as a concrete question:\n  - What do we want to learn and what decision will this inform?\n- Formulate explicit hypotheses:\n  - Null and alternative, or competing models of how the world works.\n- Identify risk level and blast radius to choose an appropriate level of rigor.\n\n## 2. Define Success Metrics and Guardrails\n\n- Select **primary outcome metrics** tied to the decision (e.g., conversion, retention, task completion time).\n- Add **secondary metrics** for richer insight (e.g., engagement, satisfaction).\n- Define **guardrail metrics** to protect against hidden harms:\n  - Error rates, latency, support tickets, churn, fairness indicators.\n- Specify minimum detectable effect, if possible, to calibrate expectations.\n\n## 3. Choose Experiment Design\n\n- Pick an appropriate design based on context:\n  - A/B or A/B/n, feature flag rollout, holdout groups, or quasi-experiments.\n- Decide on unit of randomization:\n  - User, account, session, request, or time-based.\n- Consider contamination and interference:\n  - Cross-device use, shared accounts, social/market effects.\n\n## 4. Plan Sample, Duration, and Segmentation\n\n- Estimate sample size and experiment duration where feasible:\n  - Use rough power calculations or industry heuristics for guidance.\n- Decide on key segments to track:\n  - New vs existing users, geography, device, plan tier, or risk groups.\n- Document stopping rules to avoid p-hacking and mid-stream overreaction.\n\n## 5. Design Implementation and Instrumentation\n\n- Map experiment conditions to concrete implementation:\n  - Feature flags, configuration, routing, or model selection.\n- Ensure instrumentation is in place **before** launch:\n  - Events, properties, and identifiers required for analysis.\n- Validate data quality on a small internal cohort or staging environment.\n\n## 6. Run the Experiment Safely and Ethically\n\n- Monitor guardrail metrics during the run for regressions.\n- Define clear rollback or pause conditions if harm is detected.\n- Respect legal, privacy, and ethical constraints:\n  - Avoid manipulative patterns; treat users fairly across variants.\n\n## 7. Analyze Results and Make the Decision\n\n- Use appropriate statistical methods for the design:\n  - Differences in means/medians, ratios, or regression where needed.\n- Look for heterogeneity across key segments, but avoid fishing expeditions.\n- Interpret results in terms of the original decision:\n  - Ship, iterate, roll back, or run a follow-up experiment.\n\n## 8. Capture Learnings and Feed Back into Strategy\n\n- Summarize:\n  - Hypothesis, design, metrics, results, and final decision.\n- Record non-obvious insights and surprising null results.\n- Link the experiment to specs, roadmaps, and documentation so future work benefits from the learning.\n"
    },
    {
      "id": "deep-explore",
      "windsurf": {
        "description": "Deeply explore the codebase for comprehensive understanding of structure, components, and execution paths",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Explore Workflow\n\nThis workflow instructs Cascade to go beyond surface-level reading and build a deep, structural, and execution-flow understanding of the codebase.\n\n## 1. High-Level Structural Analysis\n\n- Start by listing the root directory and key subdirectories to understand the project layout.\n- Use `list_dir` or `run_command` with `tree` (if available and constrained) to visualize the hierarchy.\n- Identify the project type (monorepo, polyrepo, framework used) and key configuration files (package.json, tsconfig.json, etc.).\n\n## 2. Component & Module Discovery\n\n- Use `code_search` (Fast Context) to map out major components and modules.\n- Identify where business logic, data access, UI components, and utilities reside.\n- Look for architectural patterns (MVC, Clean Architecture, Hexagonal, etc.).\n\n## 3. Execution Path Tracing\n\n- Identify entry points (e.g., `main.ts`, `index.js`, API route handlers, CLI commands).\n- Follow the execution flow from entry points down to core logic.\n- Use `grep_search` to trace function calls and data passing.\n- Understand how data flows through the system (inputs -> processing -> storage/outputs).\n\n## 4. Deep Dive into Key Areas\n\n- Don't just read top-level files; examine nested components and utilities.\n- Understand the relationships between files (imports, exports, dependencies).\n- Look for cross-cutting concerns (logging, error handling, auth).\n\n## 5. Synthesis & Reasoning\n\n- Synthesize the gathered information into a coherent mental model of the system.\n- Explain *why* the code is structured this way, not just *what* it is.\n- Identify potential bottlenecks, complexity hotspots, or areas for refactoring.\n"
    },
    {
      "id": "deep-git",
      "body": "# Deep Git Workflow\n\nThis workflow instructs Cascade to act as an expert **git / GitHub partner** that:\n\n- Designs and maintains **clean history and branches**.\n- Uses git and GitHub **CLI and web flows** safely and proficiently.\n- Crafts **best-practice PRs, reviews, comments, and repo materials** (README, LICENSE, CONTRIBUTING, etc.).\n- Respects **team etiquette, safety, and traceability**.\n\nIt should default to **teaching, planning, and proposing safe commands**, not rewriting history or force-pushing without explicit user consent.\n\n---\n\n## 1. Frame the Git/GitHub mission\n\n- **1.1 Clarify the scenario**\n  - Identify what the user is trying to do:\n    - Start or refactor a repository.\n    - Implement a feature or bugfix branch.\n    - Prepare or review a pull request.\n    - Clean up history or branches.\n    - Improve repo docs and contribution experience.\n  - Ask minimal clarifying questions when needed (team size, release cadence, hosting, branching policy).\n\n- **1.2 Define success and risk level**\n  - Success examples:\n    - \"Small, readable PR ready for review.\"\n    - \"Repository ready for open-source contributors.\"\n    - \"Clean, linear history for this feature before merging.\"\n  - Risk examples:\n    - Rewriting shared history.\n    - Force-pushing to protected branches.\n    - Deleting branches or tags.\n  - If the mission involves **history rewriting, destructive operations, or production branches**, flag the risk explicitly and get explicit confirmation.\n\n---\n\n## 2. Inspect repository state via CLI (read-only first)\n\nBefore suggesting changes, build a clear mental model of the repo using **read-only** git commands.\n\n- **2.1 Baseline status and branches**\n  - Run and interpret (via terminal tools):\n    - `git status -sb`\n    - `git branch --show-current`\n    - `git remote -v`\n  - Determine:\n    - Current branch and whether it tracks a remote.\n    - Whether there are uncommitted changes, untracked files, or merge conflicts.\n\n- **2.2 Recent history and diffs**\n  - For context, use **non-destructive** commands such as:\n    - `git log -n 10 --oneline --graph --decorate`\n    - `git diff --stat`\n    - `git diff` (optionally with a specified range, e.g. `main...HEAD`).\n  - Summarize:\n    - What changed recently.\n    - Whether the branch diverged from the default branch.\n\n- **2.3 Repo configuration snapshot**\n  - When helpful, inspect:\n    - `.gitignore`\n    - Repo root files: `README*`, `LICENSE*`, `CONTRIBUTING*`, `CODE_OF_CONDUCT*`, `SECURITY*`, `CHANGELOG*`.\n  - Note which **governance files are missing** and should be added.\n\nAlways propose commands clearly and avoid running anything destructive (resets, force pushes, branch deletions) unless explicitly requested and confirmed.\n\n---\n\n## 3. Branching strategy and workflow design\n\n- **3.1 Choose an appropriate branching model**\n  - For small teams / modern SaaS, prefer **trunk-based development** with short-lived feature branches.\n  - For more complex or legacy workflows, adapt but avoid unnecessary complexity.\n  - Make a **recommendation** (e.g. `main` + short-lived `feature/...` branches) and explain trade-offs.\n\n- **3.2 Branch naming and hygiene**\n  - Suggest clear, consistent patterns, for example:\n    - `feature/<area>-<short-description>`\n    - `fix/<bug-or-issue-id>`\n    - `chore/<maintenance-task>`\n  - Encourage:\n    - Deleting merged branches on remote and local.\n    - Avoiding long-lived, multi-purpose branches.\n\n- **3.3 Safe branching flows**\n  - Prefer flows like:\n\n```bash\ngit switch main\ngit pull --ff-only\n\ngit switch -c feature/<area>-<short-description>\n# work, commit, push\n\ngit push -u origin feature/<area>-<short-description>\n```\n\n  - When cleaning up history on a **local, unshared** branch, propose interactive rebase patterns:\n\n```bash\ngit rebase -i main\n```\n\n  - Never rewrite history on branches that are already shared without:\n    - Explaining risks.\n    - Providing a recovery plan.\n    - Getting explicit user approval.\n\n---\n\n## 4. Commit design and commit message best practices\n\n- **4.1 Structure of changes**\n  - Encourage **small, focused commits** that each:\n    - Implement one logical change.\n    - Can be understood and reverted independently.\n  - Discourage giant \"kitchen sink\" commits except when performing mechanical changes (then clearly label them).\n\n- **4.2 Commit message style**\n  - Use a consistent convention, e.g. **Conventional Commits** or a lightweight variant:\n\n    - Format: `type(scope): short, imperative description`\n    - Examples:\n      - `feat(auth): add OAuth2 login flow`\n      - `fix(api): handle null user_id in session middleware`\n      - `docs: clarify environment setup`\n\n  - General rules:\n    - Use the **imperative mood**: \"add\", \"fix\", \"update\".\n    - Keep the **subject line concise** (~50 characters ideal, wrap at 72 chars in bodies).\n    - Add a body when needed to explain **why**, not just what.\n\n- **4.3 Linking work items**\n  - Encourage referencing issues or tickets when appropriate:\n    - `Refs #123` or `Closes #123`.\n  - Ensure the message conveys enough context that `git log` is readable without opening every diff.\n\nWhen asked to craft commit messages, generate **several good options** and ensure they align with the repo’s existing style if visible.\n\n---\n\n## 5. Pull request design and etiquette\n\n- **5.1 Shape of a good PR**\n  - Aim for PRs that are:\n    - **Small and focused** around a single feature or fix.\n    - Self-contained with passing tests.\n    - Easy to review in 15–30 minutes.\n  - When the change is inherently large, help the user split into:\n    - A sequence of preparatory refactors.\n    - One or more focused behavior changes.\n\n- **5.2 PR titles and descriptions**\n  - Titles:\n    - Clear, concise summary of the change.\n    - Optionally include type/prefix and issue reference.\n  - Descriptions:\n    - Explain **why** the change is needed.\n    - Summarize **what** changed at a high level.\n    - Note any **breaking changes** or migrations.\n    - Add **testing notes** (how it was verified).\n    - Include **screenshots or clips** for UI changes.\n  - Provide well-structured PR templates when designing repos.\n\n- **5.3 Review etiquette**\n  - For authors:\n    - Be responsive and respectful to reviewer comments.\n    - Avoid force-pushing after review without summarizing what changed.\n    - Clearly mark when the PR is ready for re-review.\n  - For reviewers:\n    - Ask clarifying questions before making strong assertions.\n    - Use **suggested changes** where helpful instead of vague comments.\n    - Focus on correctness, clarity, and maintainability, not personal style.\n    - Distinguish **blocking** vs **non-blocking** feedback.\n\nWhen requested, generate **example PR descriptions or review comments** that model this etiquette.\n\n---\n\n## 6. Repository hygiene and public-facing materials\n\n- **6.1 Core files for a healthy repo**\n  - Recommend and help author:\n    - `README.md` – overview, getting started, usage, development, support.\n    - `LICENSE` – clear license (MIT/Apache-2.0/etc.).\n    - `CONTRIBUTING.md` – how to propose changes, coding standards, review process.\n    - `CODE_OF_CONDUCT.md` – community expectations.\n    - `SECURITY.md` – how to report vulnerabilities.\n    - `CHANGELOG.md` or release notes.\n  - Align recommendations with **GitHub best practices** and surface missing pieces.\n\n- **6.2 README structure**\n  - Default sections when authoring a README:\n    - Project tagline and short description.\n    - Badges (build, coverage, version) where appropriate.\n    - Quickstart (install + minimal usage example).\n    - Configuration / advanced usage.\n    - Development setup.\n    - Contributing and license.\n\n- **6.3 Open-source etiquette**\n  - Encourage:\n    - Clear governance (who maintains what).\n    - Responsive triage of issues and PRs.\n    - Respectful, inclusive communication.\n\nWhen drafting these files, follow **plain, clear English** and prefer actionable, specific instructions over vague statements.\n\n---\n\n## 7. Safe use of powerful git operations\n\n- **7.1 History rewriting**\n  - Prefer history rewriting (e.g. `git rebase -i`, `git commit --amend`) only when:\n    - The branch has **not** been shared yet; or\n    - The team has an explicit, shared agreement on rewriting.\n  - Before suggesting such commands:\n    - Explain the purpose and impact.\n    - Offer a backup/escape plan (e.g. using `git reflog`).\n\n- **7.2 Force pushes and destructive actions**\n  - Treat `git push --force` and branch deletions as **high-risk**.\n  - Only propose them when:\n    - There is no safer alternative.\n    - The user explicitly confirms understanding of the risk.\n  - Clearly label such steps as **dangerous** and suggest verifying remotes and branch protection first.\n\n- **7.3 Recovery and troubleshooting**\n  - Use tools like `git reflog`, `git fsck`, and backup branches (e.g. `backup/<date>-<short-desc>`) to recover from mistakes.\n  - When something goes wrong, prioritize **making a backup branch** before further surgery.\n\n---\n\n## 8. Output style and interaction pattern\n\nWhen acting as `*-deep-git`:\n\n- **8.1 Default behavior**\n  - Start with a **short summary** of the repo state and mission.\n  - Propose a **step-by-step plan** before listing commands.\n  - Present git commands in fenced code blocks and clearly label what each one does.\n\n- **8.2 Respect user environment and policies**\n  - Assume a standard git CLI environment; avoid exotic tools unless present.\n  - Never assume permission to run potentially destructive commands automatically; ask for explicit confirmation.\n\n- **8.3 Teaching and documentation**\n  - Where useful, briefly explain **why** a given git or GitHub practice is recommended, referencing modern best practices.\n  - Prefer concise, high-signal explanations over verbose tutorials.\n\nThis workflow should enable Cascade to act as a **senior git/GitHub collaborator**, elevating both the technical safety of operations and the social etiquette of collaboration.\n"
    },
    {
      "id": "deep-incident",
      "windsurf": {
        "description": "Respond to and learn from incidents in a structured, blameless way",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Incident Workflow\n\nThis workflow instructs Cascade to handle production incidents methodically: triage, stabilize, communicate, and learn.\n\n## 1. Detect, Classify, and Declare\n\n- Confirm that an incident is occurring using alerts, reports, or anomaly signals.\n- Classify severity and impact:\n  - Affected users, functionality, data, and regulatory exposure.\n- Appoint an **incident commander** and establish communication channels.\n\n## 2. Stabilize and Limit Impact\n\n- Prioritize user and data safety over root-cause hunting.\n- Use safe, reversible actions where possible:\n  - Rollbacks, feature flags, traffic shaping, rate limiting, failover.\n- Document actions and timestamps as you go for later analysis.\n\n## 3. Investigate in Real Time\n\n- Use logs, metrics, traces, and dashboards to narrow down failure domains.\n- Form quick hypotheses and test them with minimal-risk experiments.\n- Keep notes on what is ruled out to avoid duplication of effort.\n\n## 4. Communicate Clearly\n\n- Provide regular internal updates:\n  - Current understanding, mitigations in progress, and next checkpoints.\n- When appropriate, communicate externally:\n  - Status pages, customer messages, and regulatory notifications.\n- Be transparent but careful with speculation; focus on facts.\n\n## 5. Resolve and Verify Recovery\n\n- Implement the chosen mitigation or fix.\n- Verify recovery:\n  - Key metrics, logs, and user flows back within normal bounds.\n- Decide when to formally close the incident and hand off to follow-up.\n\n## 6. Transition to Retrospective and Systemic Fixes\n\n- Schedule a follow-up using `/workflow-deep-retrospective`.\n- Ensure action items are captured and prioritized:\n  - Code fixes, infra changes, observability upgrades, process improvements.\n- Update runbooks, playbooks, and training based on what was learned.\n"
    },
    {
      "id": "deep-infrastructure",
      "windsurf": {
        "description": "Design and operate resilient, secure, and automatable infrastructure",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Infrastructure Workflow\n\nThis workflow instructs Cascade to think like an experienced SRE/DevOps/infra engineer.\n\n## 1. Clarify Workloads, SLOs, and Constraints\n\n- Describe the services and workloads to be supported:\n  - Traffic patterns, data volumes, latency expectations, and dependencies.\n- Identify or propose SLOs for availability, latency, and error rates.\n- Capture constraints:\n  - Budget, regions, compliance, team skills, and existing platforms.\n\n## 2. Design Environments and Topology\n\n- Define environment strategy:\n  - Dev, test, staging, production, and any specialized environments.\n- Sketch high-level topology:\n  - Regions, availability zones, network segments, and trust boundaries.\n- Consider multi-region, multi-cloud, or hybrid approaches where justified.\n\n## 3. Embrace Infrastructure as Code and Automation\n\n- Choose appropriate IaC tools (e.g., Terraform, CloudFormation, Pulumi) based on stack.\n- Define patterns for:\n  - Reusable modules, configuration management, and secrets handling.\n- Integrate infra changes into CI/CD with review, validation, and automated testing.\n\n## 4. Plan for Reliability, Scalability, and Resilience\n\n- Select scaling strategies:\n  - Horizontal vs vertical scaling, autoscaling policies, and capacity buffers.\n- Design for failure:\n  - Redundancy, graceful degradation, backpressure, and safe fallbacks.\n- Align with `/workflow-deep-observability` to ensure infra health is visible.\n\n## 5. Address Security and Compliance Baselines\n\n- Apply security best practices:\n  - Least privilege, network segmentation, patching, hardened images, and key rotation.\n- Consider relevant standards and benchmarks (e.g., CIS benchmarks).\n- Integrate security checks into pipelines where feasible.\n\n## 6. Operational Excellence and Runbooks\n\n- Define operational tasks and on-call responsibilities.\n- Create and maintain runbooks for:\n  - Common incidents, deployments, rollbacks, and maintenance.\n- Use post-incident learnings (`/workflow-deep-incident`) to refine infra and operations.\n\n## 7. Iterate with Measurements and Feedback\n\n- Monitor infra-related costs, performance, and reliability trends.\n- Plan incremental improvements rather than large, risky overhauls.\n- Keep documentation and diagrams in sync with reality to avoid configuration drift.\n"
    },
    {
      "id": "deep-investigate",
      "windsurf": {
        "description": "Conduct rigorous, multi-method investigations into issues, claims, ideas, and feasibility using structured problem-solving frameworks",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Investigate Workflow\n\nThis workflow instructs Cascade to investigate systematically, combining scientific-method thinking, root-cause analysis tools, and creative exploration.\n\n## 1. Frame the Investigation Clearly\n\n- Restate what is being investigated as a precise question or hypothesis.\n- Classify the investigation type:\n  - Defect/incident, performance issue, security concern.\n  - Product/feature feasibility, architectural choice, process question.\n- Use a lightweight **CSD Matrix**:\n  - **Certainties** – facts and hard data.\n  - **Suppositions** – assumptions that seem likely.\n  - **Doubts** – unknowns, ambiguities, or contested claims.\n- Define success criteria: what counts as a satisfactory answer or level of confidence.\n\n## 2. Collect Initial Evidence and Context\n\n- Gather available artifacts:\n  - Logs, traces, metrics, error messages.\n  - User reports, tickets, acceptance criteria, existing docs.\n- Map where in the codebase and system this likely lives using `code_search` and `grep_search`.\n- Use `search_web` for quick orientation when relevant (framework behavior, platform quirks, known bugs), confirming versions and dates.\n\n## 3. Generate Hypotheses Broadly\n\n- Brainstorm plausible explanations or outcomes before investigating deeply.\n- Use an **Ishikawa/Fishbone-inspired** lens for software:\n  - People (skills, communication, process).\n  - Process (SDLC, reviews, testing, release practices).\n  - Platform/Environment (infrastructure, config, dependencies).\n  - Code/Logic (algorithms, data structures, branching, edge cases).\n  - Data (schema, migrations, quality, contracts).\n- For product/feasibility questions, include multiple conceptual options, not just the “obvious” one.\n\n## 4. Prioritize Lines of Inquiry\n\n- Rank hypotheses using simple risk and plausibility:\n  - Impact if true (severity, cost, user impact).\n  - Likelihood given the evidence so far.\n- Optionally apply:\n  - A quick **Pareto** mindset (what 20% of causes likely produce 80% of impact).\n  - A small **decision matrix** when multiple investigative directions compete for limited attention.\n\n## 5. Design Targeted Experiments\n\n- For the top hypotheses, design minimal, high-signal experiments:\n  - What observation, test, or probe will distinguish between hypotheses?\n  - What data will be collected (logs, metrics, traces, user behavior, benchmarks)?\n  - What outcome would confirm vs refute each hypothesis?\n- Ensure experiments are safe (non-destructive, minimal production risk) and prioritized by information value.\n\n## 6. Execute, Observe, and Update the Model\n\n- Run experiments iteratively rather than all at once.\n- After each experiment:\n  - Update the CSD Matrix (promote suppositions to certainties or move them to doubts).\n  - Eliminate or refine hypotheses.\n  - Note surprises and anomalies explicitly.\n- Use `search_web` and repository history (git logs, PRs) to relate findings to known changes or external information.\n\n## 7. Deep Root Cause and Risk Analysis\n\n- Once a leading explanation emerges, probe deeper:\n  - Apply **5 Whys** to move from surface symptom to systemic cause.\n  - Optionally sketch a simple **Fault Tree**: start from the observed problem and decompose contributing conditions.\n- For systemic or recurring issues, think in **FMEA** terms:\n  - What are the main failure modes in this area?\n  - For each: Severity, Occurrence likelihood, Detection capability (even if only qualitatively ranked).\n  - Use this to highlight where mitigation or monitoring is most urgent.\n\n## 8. Synthesis, Recommendations, and Open Questions\n\n- Summarize the investigation:\n  - Question asked and why it matters.\n  - Evidence gathered and experiments run.\n  - Hypotheses considered and which were ruled out.\n  - The most likely explanation(s) and residual uncertainty.\n- Provide recommendations at multiple levels where applicable:\n  - Immediate fixes or mitigations.\n  - Structural or process changes to prevent recurrence.\n  - Monitoring/observability improvements to detect similar issues earlier.\n- Explicitly list remaining open questions and suggest next investigative steps if higher confidence is required.\n"
    },
    {
      "id": "deep-iterate",
      "windsurf": {
        "description": "Execute work in small, validated iterations until a clearly defined end goal is reached",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Iterate Workflow\n\nThis workflow instructs Cascade to work iteratively using the **Automated Iterative Development (AID)** methodology: plan phases, execute steps, validate outputs, commit at boundaries, adapt when issues arise, and repeat.\n\n## The AID Process Loop\n\n```text\n┌─────────────────────────────────────────────────────────────────┐\n│                    AUTOMATED ITERATION LOOP                      │\n├─────────────────────────────────────────────────────────────────┤\n│   ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐ │\n│   │  PLAN    │───▶│ EXECUTE  │───▶│ VALIDATE │───▶│  COMMIT  │ │\n│   │  Phase   │    │  Step    │    │  Output  │    │  Stage   │ │\n│   └──────────┘    └──────────┘    └──────────┘    └──────────┘ │\n│        │                                               │        │\n│        │              ┌──────────┐                     │        │\n│        │◀─────────────│  ADAPT   │◀────────────────────┘        │\n│                       │  & Learn │                              │\n│                       └──────────┘                              │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## 1. Clarify Goal, Decompose, and Create Task List\n\n- Restate the ultimate goal in observable terms:\n  - E.g., tests passing, metric thresholds met, UI behavior achieved, docs produced.\n- **Decompose into committable phases** that can be:\n  - Executed independently\n  - Validated with a specific mechanism\n  - Committed as a logical unit\n  - Rolled back if needed\n- **Create explicit task list** with clear status:\n  ```text\n  [x] Completed phase\n  [ ] Pending phase\n  [>] In-progress phase (exactly one at a time)\n  [!] Blocked phase (needs input)\n  ```\n- Identify or negotiate a **validation mechanism** for each phase:\n  - Test command or suite, manual acceptance steps, metrics or logs to inspect.\n- Define a rough limit on iterations or timebox if applicable.\n\n## 2. Plan the Next Minimal Step\n\n- Choose the smallest meaningful change that moves toward the goal.\n- For each phase, define:\n  - Specific changes to make\n  - Validation mechanism (command, expected output)\n  - Commit message template\n  - Dependencies on previous phases\n- Prefer steps that:\n  - Are easy to validate using the chosen mechanism.\n  - Are reversible or low-risk.\n- Make assumptions explicit so they can be confirmed or revised after validation.\n\n## 3. Execute the Step\n\n- Apply the planned change (code, config, docs, or other artifacts).\n- Keep the change focused on the current step; avoid mixing unrelated edits.\n- **Best practices during execution:**\n  - Read before writing: Always read existing code before modifying\n  - Minimal edits: Make the smallest change that achieves the goal\n  - Preserve style: Match existing code conventions and patterns\n  - Handle edge cases: Consider error conditions and boundary cases\n\n## 4. Validate and Observe\n\n- **Apply validation hierarchy** in order of increasing scope:\n  1. **Syntax**: Does the code parse/compile?\n  2. **Unit**: Do individual functions work?\n  3. **Integration**: Do components work together?\n  4. **End-to-end**: Does the full flow work?\n- Run the agreed validation mechanism:\n  - Execute tests, commands, or checks.\n  - Inspect outputs, logs, or metrics as specified.\n- Compare results with expectations:\n  - Did this step move closer to the goal, fully achieve it, or reveal new information?\n- **When validation fails:**\n  1. Do NOT proceed to next phase\n  2. Diagnose root cause (use `/deep-debug` if complex)\n  3. Fix minimally at the root cause, not symptoms\n  4. Re-run validation from beginning\n  5. Only proceed after validation passes\n\n## 5. Commit at Phase Boundaries\n\n- **After successful validation**, commit the completed phase:\n  1. Stage all related changes\n  2. Write descriptive commit message explaining what and why\n  3. Commit the phase as one logical, working change\n  4. Update task list to mark phase complete\n  5. Proceed to next phase\n- **Commit message template:**\n  ```text\n  Phase N: [Brief description]\n\n  - [Specific change 1]\n  - [Specific change 2]\n\n  Part of [larger initiative] (Phase N of M).\n  ```\n- **Never commit broken or partial code.**\n\n## 6. Adapt the Plan\n\n- **Adaptation triggers and responses:**\n  | Trigger                | Response                              |\n  |------------------------|---------------------------------------|\n  | Validation failure     | Debug, fix, re-validate               |\n  | Unexpected complexity  | Break into smaller steps              |\n  | Missing dependency     | Add prerequisite phase                |\n  | Scope change           | Update task list, document change     |\n  | Blocking issue         | Note for user, continue other phases  |\n\n- If validation partially succeeded or failed:\n  - Update the mental model of the system or task.\n  - Adjust assumptions, and refine or change the next step accordingly.\n- If validation failed in a surprising way:\n  - Optionally apply `/deep-debug` or `/deep-investigate` patterns before proceeding.\n\n## 7. Iterate Until Done or Timeboxed\n\n- Repeat steps 2–6, each time:\n  - Planning a new minimal step.\n  - Executing and validating.\n  - Committing at phase boundaries.\n  - Learning from the outcome.\n- **Autonomous completion criteria:**\n  - All phases in task list are marked complete\n  - All commits are made with descriptive messages\n  - Documentation is updated (if applicable)\n  - Final validation passes\n- Stop when:\n  - The goal is clearly achieved and validated, or\n  - You reach the agreed timebox or iteration limit.\n\n## 8. Summarize Outcome and Handoff\n\n- Provide a comprehensive summary:\n  ```text\n  ## Summary\n\n  ### Commits Made\n  1. Phase 1: [description]\n  2. Phase 2: [description]\n  ...\n\n  ### Deliverables\n  - [file/component]: [description]\n  ...\n\n  ### Validation Results\n  - [validation]: [result]\n  ...\n\n  ### Follow-ups (if any)\n  - [item]\n  ...\n  ```\n- If the goal wasn't fully achieved, clearly state:\n  - What remains, and what you recommend as the next actions.\n- Await user review before considering the task fully complete.\n"
    },
    {
      "id": "deep-observability",
      "windsurf": {
        "description": "Design and evolve robust observability for applications, data, and AI systems",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Observability Workflow\n\nThis workflow instructs Cascade to create and refine observability so teams can understand, debug, and improve systems quickly.\n\n## 1. Clarify Goals and Critical Flows\n\n- Identify key user journeys and business capabilities that must be observable.\n- For each, define what “healthy” looks like:\n  - Latency, error rates, throughput, resource usage, data quality, and cost.\n- Note SLOs or reliability targets where they exist.\n\n## 2. Inventory Current Signals and Gaps\n\n- List existing logs, metrics, and traces for the targeted areas.\n- Map them to the **four golden signals** (latency, traffic, errors, saturation) and, for data, to observability pillars (freshness, volume, schema, quality, lineage).\n- Identify blind spots, noisy signals, and missing correlations.\n\n## 3. Design Metrics, Logs, and Traces\n\n- Define a small, meaningful set of **service-level indicators (SLIs)** aligned to SLOs.\n- Standardize logging patterns:\n  - Structured logs with correlation IDs, key context fields, and severity levels.\n- Decide where to add tracing:\n  - Critical request paths, cross-service calls, data pipelines, and LLM/agent workflows.\n\n## 4. Implement Instrumentation Patterns\n\n- Integrate observability libraries and middleware at appropriate layers.\n- Instrument at logical boundaries:\n  - API handlers, message consumers, background jobs, pipeline stages.\n- For LLM/AI systems, capture:\n  - Model/provider, latency, token usage/cost, failure categories, and user feedback.\n\n## 5. Design Dashboards and Alerts\n\n- Create dashboards that:\n  - Follow flows end-to-end rather than listing raw metrics.\n  - Highlight SLOs, error spikes, and unusual patterns.\n- Define alerting rules:\n  - Thresholds, aggregation windows, and on-call rotations.\n- Avoid alert fatigue:\n  - Prioritize high-severity, actionable alerts with clear runbooks.\n\n## 6. Govern Noise, Cost, and Privacy\n\n- Periodically review high-volume logs and metrics:\n  - Remove or sample low-value signals.\n- Ensure observability data respects privacy and compliance constraints:\n  - Avoid sensitive data in logs; use redaction and access controls.\n- Monitor observability tooling costs and optimize storage and retention policies.\n\n## 7. Review and Evolve Observability\n\n- After major incidents or launches, revisit observability:\n  - What signals helped? What was missing?\n- Incorporate lessons into updated instrumentation, dashboards, and runbooks.\n- Keep a lightweight observability roadmap aligned with product and infrastructure evolution.\n"
    },
    {
      "id": "deep-optimize",
      "windsurf": {
        "description": "Analyze and improve performance and scalability using a measurement-driven methodology",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Optimize Workflow\n\nThis workflow instructs Cascade to optimize performance and scalability using evidence, not guesswork.\n\n## 1. Define Performance Goals and Constraints\n\n- Clarify target SLOs or expectations:\n  - Latency, throughput, error rates, resource utilization, cost ceilings.\n- Identify critical user journeys and hot paths to focus on first.\n- Note constraints:\n  - Hardware limits, third-party quotas, architectural immutables.\n\n## 2. Measure Baseline\n\n- Use appropriate tools to capture current behavior:\n  - Profilers, flamegraphs, tracing, query plans, synthetic load tests.\n- Measure under realistic scenarios:\n  - Representative data volumes and traffic patterns.\n- Record baseline metrics so improvements and regressions can be compared.\n\n## 3. Identify Bottlenecks and Root Causes\n\n- Analyze measurements to find:\n  - Hot functions, slow queries, lock contention, chatty network calls.\n- Apply an 80/20 lens:\n  - Focus first on the few hotspots responsible for most of the slowness.\n- Look for systemic patterns:\n  - Inefficient data access patterns, unnecessary work, over‑synchronization.\n\n## 4. Explore Optimization Strategies\n\n- Consider layers of optimization, roughly in this order:\n  - Algorithm and data structure improvements (time/space complexity).\n  - Data access improvements (indexes, batching, denormalization where appropriate).\n  - Architectural changes (caching, async processing, parallelism, offloading to background work).\n  - Infrastructure tuning (connection pools, thread pools, container limits).\n- Evaluate trade‑offs explicitly:\n  - Complexity vs gain, impact on correctness, operational risk.\n\n## 5. Implement and Validate Changes\n\n- Apply optimizations in small, testable increments.\n- After each change:\n  - Re‑run the relevant benchmarks or load tests.\n  - Compare new metrics against the baseline and goals.\n- Ensure functional behavior and correctness remain intact via tests.\n\n## 6. Guard Against Regressions\n\n- Where feasible, add:\n  - Automated performance checks, budgets, or alerts for key endpoints and jobs.\n  - Dashboards tracking latency, throughput, and resource utilization over time.\n- Document optimization decisions and their rationale so future changes respect the same constraints.\n"
    },
    {
      "id": "deep-polish",
      "windsurf": {
        "description": "Refine products and interfaces to a world-class level of craft and cohesion",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Polish Workflow\n\nThis workflow instructs Cascade to focus on the final 10–20% of refinement that separates “good enough” from truly excellent. It assumes core UX, architecture, and functionality are already in place (see `/workflow-deep-ux`, `/workflow-deep-code`, and `/workflow-deep-test`).\n\n## 1. Clarify the Polish Goal and Quality Bar\n\n- Restate what is being polished:\n  - Screen(s), flow, component library, microcopy set, motion system, or overall product surface.\n- Define the target quality bar:\n  - Reference products or design systems that set the benchmark for this context.\n- Make constraints explicit:\n  - Timebox, scope, platform limitations, brand guidelines, and technical debt that cannot be addressed now.\n\n## 2. Diagnose Current Rough Edges\n\n- Perform a structured pass (building on `/workflow-deep-ux` where relevant):\n  - Visual noise, misalignment, inconsistent spacing.\n  - Awkward flows, missing or unclear states.\n  - Jarring or absent motion, loading, and error handling.\n  - Copy that is unclear, wordy, or off-tone.\n- Capture issues in a concise list grouped by area (visual, interaction, copy, behavior, performance).\n- Prioritize by impact vs effort to guide where polish time should go first.\n\n## 3. Polish Layout, Spacing, and Visual Hierarchy\n\n- Revisit grids and spacing scales:\n  - Align elements to a consistent grid and rhythm.\n  - Normalize spacing between related and unrelated elements.\n- Sharpen hierarchy:\n  - Ensure typography, color, and weight clearly indicate what is primary, secondary, and tertiary.\n- Remove visual clutter:\n  - Eliminate redundant lines, borders, and decoration that do not aid comprehension.\n\n## 4. Refine States, Interactions, and Feedback\n\n- Enumerate states for key components and flows:\n  - Default, hover, focus, active/pressed, loading, success, empty, error, disabled.\n- Ensure each state is visually distinct and accessible:\n  - Adequate contrast, clear focus indicators, and consistent behavior patterns.\n- Add or tune microinteractions where appropriate:\n  - Subtle transitions, hover/press feedback, and progress indicators that clarify system status without distracting.\n\n## 5. Polish Microcopy and Communication\n\n- Review microcopy in context:\n  - Labels, helper text, placeholders, validation messages, empty states, and confirmations.\n- Optimize for clarity first, then tone:\n  - Prefer concrete, action-oriented language over cleverness.\n  - Align tone with brand and user context (especially for stressful flows).\n- Ensure consistency:\n  - Terminology, capitalization, and phrasing patterns across the product.\n\n## 6. Behavioral, Performance, and Responsiveness Polish\n\n- Check responsiveness:\n  - Layout and interactions across breakpoints, orientations, and density settings.\n- Address performance polish:\n  - Perceived speed via skeletons, progressive loading, and responsive interactions.\n  - Avoid jank in animations and scrolling.\n- Verify error handling and recovery:\n  - Clear, actionable error messages and recovery paths.\n\n## 7. Ensure Cohesion with Systems and Brand\n\n- Cross-check against design and component systems:\n  - Variants, tokens, and patterns should be used consistently or intentionally diverge with rationale.\n- Align with brand expression:\n  - Color, imagery, motion, and tone should reinforce the same identity across surfaces.\n- Eliminate one-off hacks where reasonable:\n  - Move ad hoc styling or behavior into reusable patterns where it improves maintainability.\n\n## 8. Validate Polish with Users and Team\n\n- Where feasible, perform quick validation:\n  - Heuristic review, design QA, or light user testing on the polished flows.\n- Incorporate feedback selectively:\n  - Focus on issues that undermine clarity, trust, or perceived quality.\n- Coordinate with engineering and QA:\n  - Ensure polish changes are testable, captured in `/workflow-deep-test` strategy, and do not introduce regressions.\n\n## 9. Capture Patterns and Definition of Done\n\n- Document newly established patterns and decisions:\n  - Layout rules, spacing scales, state patterns, microcopy conventions, and motion guidelines.\n- Update the team’s \"definition of done\" for relevant workstreams to include key polish checks.\n- Note follow-up polish opportunities that exceed current constraints, so they are not lost.\n"
    },
    {
      "id": "deep-prune",
      "windsurf": {
        "description": "Systematically identify and remove dead or low-value code, configuration, and dependencies while preserving behavior",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Prune Workflow\n\nThis workflow instructs Cascade to clean a codebase thoughtfully, minimizing risk while reducing complexity and surface area.\n\n## 1. Define Scope and Safety Constraints\n\n- Clarify what kinds of artifacts are in-scope:\n  - Unused functions/classes/modules, obsolete endpoints, legacy feature flags, stale configs, unused assets, dead tests.\n- Establish guardrails:\n  - No behaviour changes outside the agreed scope.\n  - Prefer deprecation and staged removal for anything user-facing or externally integrated.\n\n## 2. Discover Candidates for Removal\n\n- Use static analysis and search to find:\n  - Unreferenced symbols, unreachable branches, unused feature flags, dead routes.\n- Use runtime data where available:\n  - Logs, metrics, tracing, and analytics to identify rarely or never-used endpoints and features.\n- Cross-check with documentation and stakeholders to avoid removing deliberately dormant or emergency-only paths.\n\n## 3. Assess Risk and Prioritize\n\n- Classify candidates by risk:\n  - Internal-only vs external APIs.\n  - Covered by tests vs untested.\n  - Recently touched vs long-stable.\n- Prioritize low-risk, high-clarity removals first to build confidence and reduce noise.\n\n## 4. Plan Staged Pruning\n\n- For higher-risk items:\n  - Introduce deprecation warnings or feature flags.\n  - Add logging around potential removal points to confirm lack of use over a defined window.\n- Define clear cut-over dates and communication needs (e.g., for external consumers).\n\n## 5. Execute Removals Safely\n\n- Remove code, configuration, and assets in coherent slices rather than scattered edits.\n- Keep commits narrowly focused and well-described for easy rollback.\n- Update tests and docs to reflect removed behavior, avoiding references to dead features.\n\n## 6. Validate and Monitor\n\n- Run the full relevant test suite (including integration/e2e where available).\n- Perform targeted smoke tests around the cleaned areas.\n- Monitor logs, error rates, and user feedback after deployment for unexpected regressions.\n\n## 7. Institutionalize Hygiene\n\n- Where possible, add automated checks:\n  - Linters or build steps that flag unused code, imports, and dependencies.\n  - Dashboards or queries that track rarely used features over time.\n- Capture guidelines for when to deprecate vs immediately remove, and how to communicate removals to stakeholders.\n"
    },
    {
      "id": "deep-refactor",
      "windsurf": {
        "description": "Plan and execute safe, incremental refactors to improve design and maintainability without changing behavior",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Refactor Workflow\n\nThis workflow instructs Cascade to refactor codebases safely, evolving design while preserving behavior.\n\n## 1. Understand Context and Constraints\n\n- Clarify the motivation for refactoring:\n  - Pain points (hard to change, bugs clustering, performance issues).\n  - Desired properties (simpler interfaces, clearer boundaries, better testability).\n- Identify constraints:\n  - Timebox, risk tolerance, deployment cadence, availability of tests.\n- Confirm that behavior must remain functionally equivalent except where explicitly stated.\n\n## 2. Identify Smells, Hotspots, and Seams\n\n- Use `code_search` and `grep_search` plus any metrics to locate:\n  - Complex or frequently changed modules.\n  - God classes/modules, long functions, duplicated logic, cyclical dependencies.\n- Look for natural **seams**:\n  - Existing interfaces, adapters, modules, or test boundaries.\n  - Places where dependencies could be inverted or responsibilities split.\n\n## 3. Design an Incremental Refactor Plan\n\n- Define a series of **small, reversible steps** rather than a big‑bang change.\n- For each step, specify:\n  - Target code area and intended structural change (e.g., extract function, introduce interface, split module).\n  - Tests that must continue to pass.\n  - Any new tests needed to characterize current behavior before changing it.\n- Prefer patterns like:\n  - Strangler Fig (new structure coexists with old until migration is complete).\n  - Branch‑by‑abstraction (introduce abstraction, move implementations behind it, then remove old usage).\n\n## 4. Execute with Test and Git Discipline\n\n- Before each step:\n  - Ensure tests are passing and there is a clean working tree.\n- During each step:\n  - Make focused edits aligned with the plan.\n  - Keep commits small and well described (what changed, why, and how behavior is preserved).\n- After each step:\n  - Run relevant tests (unit + integration) before proceeding.\n  - Fix issues or adjust the plan as needed.\n\n## 5. Validate Design Improvements\n\n- After the main refactor steps:\n  - Reassess complexity (smaller functions, fewer responsibilities per module).\n  - Check for improved testability (easier to mock or isolate components).\n  - Review dependency direction and boundaries (reduced coupling, clearer layering).\n- Optionally perform a brief code review pass using agreed coding and architecture standards.\n\n## 6. Institutionalize and Follow Up\n\n- Capture key refactoring patterns and lessons learned:\n  - What worked well, what was risky or noisy.\n- Consider adding:\n  - Lint rules, architecture tests, or CI checks to prevent regression into old patterns.\n- Identify adjacent areas that would benefit from similar incremental refactors and schedule them appropriately.\n"
    },
    {
      "id": "deep-regulation",
      "windsurf": {
        "description": "Identify and align with relevant regulatory obligations for software and AI systems",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Regulation Workflow\n\nThis workflow instructs Cascade to systematically consider regulatory obligations, especially in UK/EU contexts, while recognizing that it does **not** replace qualified legal advice.\n\n## 1. Clarify Jurisdictions, Domain, and Activities\n\n- Identify where the organization operates and where users are located.\n- Describe the product or feature:\n  - Sector (e.g., finance, health, education), data processed, and AI involvement.\n- Note cross-border data flows, third-party processors, and hosting locations.\n\n## 2. Identify Potentially Applicable Regimes\n\n- Use `/workflow-deep-search` to find up-to-date legal and regulatory frameworks relevant to the context and time (e.g., data protection, AI-specific laws, consumer protection, digital services, cybersecurity).\n- Consider, as examples (not an exhaustive or authoritative list):\n  - Data protection and privacy laws.\n  - AI/automated-decision-making regulations.\n  - Consumer protection and advertising rules.\n  - Sector-specific regulations and professional standards.\n\n## 3. Map Data, Roles, and Processing Activities\n\n- Classify data:\n  - Personal vs non-personal, sensitive/special categories, behavioral data, and telemetry.\n- Determine roles:\n  - Controllers, processors, joint controllers, and sub-processors where relevant.\n- List processing purposes and legal bases as far as they can be inferred from documentation.\n\n## 4. Assess Key Obligations and Risks\n\n- For each identified regime, use `/workflow-deep-search` to:\n  - Outline likely obligations (e.g., transparency, consent, data subject rights, security measures, risk assessments).\n  - Identify high-risk processing activities (e.g., profiling, large-scale sensitive data, high-risk AI use cases).\n- Highlight uncertainties or ambiguities that require professional legal interpretation.\n\n## 5. Propose Controls, Documentation, and Processes\n\n- Suggest practical steps aligned with likely obligations:\n  - Data minimization, privacy-by-design measures, security controls, audit logging.\n- Encourage creation or refinement of:\n  - Policies, records of processing, risk/impact assessments, and user-facing notices.\n- Align recommendations with `/workflow-deep-ethics` to go beyond bare compliance where appropriate.\n\n## 6. Monitor Regulatory Change and Seek Expert Advice\n\n- Recommend:\n  - Following updates from relevant regulators and trusted legal/industry sources.\n  - Periodic re-assessment of obligations as laws and guidance evolve.\n- Clearly state limitations:\n  - This analysis is not legal advice and should be complemented by consultation with qualified legal counsel for concrete compliance decisions.\n\n## 7. Capture Regulatory Assumptions and Open Questions\n\n- Document which laws and guidance were considered, and which were out of scope.\n- Record assumptions, known gaps, and questions to be addressed with legal experts.\n- Link this analysis to specs, risk registers, and decision logs for traceability.\n"
    },
    {
      "id": "deep-retrospective",
      "windsurf": {
        "description": "Run effective, blameless retrospectives and postmortems that lead to real improvements",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Retrospective Workflow\n\nThis workflow instructs Cascade to help teams learn from incidents and projects, not just document them.\n\n## 1. Frame Scope and Objectives\n\n- Define what the retrospective covers:\n  - Specific incident, release, sprint, or project.\n- Clarify objectives:\n  - Understanding root causes, improving processes, strengthening collaboration.\n- Emphasize a **blameless** approach focused on systems, not individuals.\n\n## 2. Reconstruct the Timeline\n\n- Collect key events:\n  - When issues were introduced, detected, escalated, mitigated, and resolved.\n- Use logs, tickets, chats, and deployment history to validate the sequence.\n- Note where information was missing, delayed, or misunderstood.\n\n## 3. Analyze Impact\n\n- Describe impacts across dimensions:\n  - Users and customers.\n  - Business metrics and reputation.\n  - Technical health (incurred debt, instability).\n  - Team well-being (stress, burnout, confusion).\n\n## 4. Identify Root Causes and Contributing Factors\n\n- Apply 5 Whys to get beyond surface symptoms.\n- Use a fishbone lens (People, Process, Platform, Code, Data) to explore systemic contributors.\n- Distinguish between proximate causes and deeper systemic issues.\n\n## 5. Capture What Worked and What Didn’t\n\n- List practices or behaviors that helped:\n  - Effective communication, quick detection, robust tooling, clear ownership.\n- List things that hindered:\n  - Slow detection, unclear roles, brittle systems, missing tests.\n- Highlight surprises or invalidated assumptions.\n\n## 6. Define Actionable Improvements\n\n- Propose concrete actions across:\n  - Code/architecture (e.g., refactors, additional tests, resilience patterns).\n  - Tooling and observability (dashboards, alerts, runbooks).\n  - Process and collaboration (on-call rotations, review practices, escalation paths).\n- Assign rough priority and ownership where applicable.\n- Encourage tracking follow-up actions in the regular backlog.\n"
    },
    {
      "id": "deep-research",
      "windsurf": {
        "description": "Conduct extensive, multi-source web research to gather the latest (Dec 2025) context and best practices",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Search Workflow\n\nThis workflow instructs Cascade to perform rigorous, multi-step web research to ensure all solutions are grounded in the most current (December 2025) information.\n\n## 1. Information Strategy\n\n- Before searching, clearly define what needs to be known.\n- Break down complex topics into specific, searchable queries.\n- Target multiple dimensions: syntax, best practices, security, recent changes/deprecations.\n\n## 2. Multi-Source Execution\n\n- Use `search_web` aggressively with multiple distinct queries.\n- Do not stop at the first result. Cross-reference multiple sources.\n- Prioritize authoritative sources:\n  - Official Documentation\n  - GitHub Repositories (issues, discussions, code)\n  - RFCs and Standards\n  - Reputable Technical Blogs (for patterns/tutorials)\n\n## 3. Temporal Filtering (Dec 2025)\n\n- Explicitly look for content relevant to **December 2025**.\n- Check for \"latest\" versions, recent releases, and \"2025\" in search terms.\n- Discard outdated information (e.g., deprecated APIs, old patterns) unless maintaining legacy systems.\n\n## 4. Contextual Deep Dive\n\n- When a promising source is found, read it thoroughly.\n- Look for edge cases, limitations, and \"gotchas\" in the documentation.\n- Verify compatibility with the user's specific stack versions.\n\n## 5. Synthesis & Application\n\n- Summarize findings, highlighting consensus and conflicts.\n- Explicitly state *why* a particular approach is chosen based on the research.\n- Cite sources to provide evidence for decisions.\n"
    },
    {
      "id": "deep-search",
      "windsurf": {
        "description": "Perform structured local and repository search to locate relevant code, docs, and context inside the project",
        "autoExecutionMode": 2
      },
      "body": "\n# Deep Search Workflow (Local and Repository Search)\n\nThis workflow instructs Cascade to search **within the current project and its immediate environment** before reaching for the wider web. It focuses on ripgrep/grep-style search, file navigation, and repository introspection.\n\n## 1. Clarify the Search Target\n\n- Restate what needs to be found in concrete terms (function, type, config value, log message, error string, etc.).\n- Identify likely scopes:\n  - Code modules, packages, or directories.\n  - Config files, docs, ADRs, or runbooks.\n  - Tests or fixtures that mention the target.\n\n## 2. Plan Local Search Strategy\n\n- Choose one or more search dimensions:\n  - **Text search**: use `rg`/`grep_search` across the repo for key identifiers, error messages, or config keys.\n  - **Filename/path search**: locate files or directories whose names suggest relevance.\n  - **Git history search**: search commits for the term when relevant (e.g., `deep-git` patterns).\n- Prefer narrow, high-signal queries first (exact identifiers, error codes) before broad ones.\n\n## 3. Execute Local Searches Iteratively\n\n- Run targeted searches and inspect results in context:\n  - Open surrounding code for matches, not just the single line.\n  - Note clusters of matches that indicate important modules.\n- If a search returns too many results:\n  - Refine the query (add namespace, file extension filters, or directory scoping).\n  - Use additional terms (e.g., function name + argument name).\n\n## 4. Map Findings and Gaps\n\n- From the matches, build a quick map of:\n  - Where the concept is **defined** (types, functions, classes).\n  - Where it is **used** (call sites, configs, tests).\n  - Any **obvious entry points** (CLIs, HTTP handlers, jobs, workflows).\n- Identify gaps that local search did not resolve (e.g., missing design rationale, external API behavior).\n\n## 5. Decide on Next Step\n\n- If local search reveals enough:\n  - Switch to the appropriate deep-* workflow (e.g., `deep-code`, `deep-debug`, `deep-explore`) using the found code as anchors.\n- If local search is insufficient or points to external systems:\n  - Escalate to `deep-research` (web/multi-source research) to gather missing background.\n- Document key search queries and findings briefly so they can be reused later.\n"
    },
    {
      "id": "deep-spec",
      "windsurf": {
        "description": "Write high-quality specs, design docs, and ADRs that align stakeholders and guide implementation",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Spec Workflow\n\nThis workflow instructs Cascade to produce specifications that are precise, actionable, and durable.\n\n## 1. Clarify Problem, Goals, and Non-Goals\n\n- Restate the problem in clear business and technical terms.\n- Identify goals and success metrics (functional and non-functional).\n- Explicitly list non-goals to prevent scope creep.\n\n## 2. Gather Context and Constraints\n\n- Collect relevant background:\n  - Existing behavior, related systems, previous incidents, competing priorities.\n- Identify constraints:\n  - Deadlines, budget, regulatory requirements, tech stack limitations, team capacity.\n- Use `code_search` and existing docs to ensure the spec reflects reality, not wishful thinking.\n\n## 3. Explore Options and Trade-offs\n\n- Generate multiple plausible solution approaches.\n- For each option, outline:\n  - High-level architecture/flow.\n  - Pros, cons, risks, and dependencies.\n- Optionally apply `/workflow-deep-consider` techniques (decision matrix, cost-benefit thinking, pre-mortem) for important choices.\n\n## 4. Choose and Justify the Preferred Approach\n\n- Recommend an approach (or phased sequence) explicitly.\n- Explain why it’s preferred:\n  - How it meets goals and balances trade-offs better than alternatives.\n- Note open questions and assumptions that must be validated.\n\n## 5. Structure the Spec\n\n- Choose an appropriate format:\n  - Design doc, ADR, RFC, or implementation plan.\n- Include sections such as:\n  - Background and problem statement.\n  - Goals and non-goals.\n  - Proposed solution (with diagrams if helpful).\n  - Alternatives considered.\n  - Risks and mitigations.\n  - Rollout, migration, and monitoring.\n- Ensure the spec is scoped to what the team can reasonably implement in the intended timeframe.\n\n## 6. Review, Link, and Maintain\n\n- Review the spec for clarity, completeness, and testability.\n- Link it from relevant code locations, tickets, and documentation indexes.\n- Encourage capturing significant changes as follow-up ADRs or spec revisions rather than informal chat decisions.\n"
    },
    {
      "id": "deep-test",
      "windsurf": {
        "description": "Design, implement, and evolve high-value automated tests for robust software",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Test Workflow\n\nThis workflow instructs Cascade to approach testing as a first-class design activity, not an afterthought.\n\n## 1. Clarify Behavior and Risk\n\n- Restate what must be true for the system to be considered correct.\n- Identify high-risk areas:\n  - Complex logic, critical business flows, security-sensitive paths, and integrations.\n- Decide on acceptable risk and where exhaustive testing is impractical.\n\n## 2. Choose Test Strategy and Levels\n\n- Define the mix of tests appropriate for the context:\n  - Unit, integration, contract, end-to-end, property-based, and exploratory tests.\n- Align with a testing pyramid mindset:\n  - Many fast, focused tests; fewer slow, broad tests.\n- Consider codebase and tooling constraints when selecting frameworks and libraries.\n\n## 3. Design Test Cases Systematically\n\n- Use test design techniques such as:\n  - Equivalence partitioning, boundary value analysis, and state transition testing.\n- Enumerate scenarios:\n  - Happy paths, edge cases, error conditions, concurrency and timing issues.\n- For APIs and services, design contract tests that capture expectations between components.\n\n## 4. Implement Readable, Maintainable Tests\n\n- Structure tests clearly:\n  - Arrange-Act-Assert or Given-When-Then patterns, meaningful names, and minimal duplication.\n- Use appropriate test doubles:\n  - Mocks, stubs, fakes, and spies, without over-mocking.\n- Keep fixtures and setup simple and explicit; avoid magic globals.\n\n## 5. Integrate Tests into CI/CD\n\n- Ensure tests run reliably in automated pipelines:\n  - Fast feedback for unit tests; scheduled or gated runs for heavier suites.\n- Detect and address flaky tests:\n  - Quarantine while investigating, then fix or remove.\n- Use coverage and mutation testing as **signals**, not absolute targets.\n\n## 6. Evolve the Test Suite with the System\n\n- Periodically review the test suite for:\n  - Redundancy, brittleness, gaps, and misalignment with current risk.\n- Refactor tests alongside production code using `/workflow-deep-refactor` guidance.\n- Remove or rewrite low-value tests that hinder change without providing confidence.\n\n## 7. Document Testing Strategy and Responsibilities\n\n- Capture the testing strategy for the project:\n  - What is covered where, and which tools are used.\n- Clarify ownership:\n  - Who maintains which test suites and environments.\n- Link tests and strategy to specs, ADRs, and CI/CD configuration for traceability.\n"
    },
    {
      "id": "deep-think",
      "windsurf": {
        "description": "Perform deep reasoning and analysis on a subject, considering patterns, edge cases, and multiple perspectives",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Think Workflow\n\nThis workflow instructs Cascade to pause and reason deeply before acting, ensuring solutions are robust, well-considered, and comprehensive.\n\n## 1. First-Principles Decomposition\n\n- Break the problem down into its fundamental truths and constraints.\n- Question assumptions. Is the standard approach the best one here?\n- Identify the core \"job to be done\" for the code or feature.\n\n## 2. Multi-Perspective Analysis\n\n- Analyze the problem from different angles:\n  - **Architectural**: How does this fit into the larger system? Is it coupled?\n  - **Security**: What are the attack vectors? (OWASP Top 10 2025)\n  - **Performance**: Time/Space complexity, I/O bottlenecks.\n  - **Maintainability**: Readability, extensibility, testing.\n  - **User Experience**: How does this affect the end user?\n\n## 3. Pattern & Anti-Pattern Recognition\n\n- Identify applicable design patterns (Factory, Observer, Strategy, etc.).\n- Watch out for anti-patterns (God objects, tight coupling, premature optimization).\n- Consider \"best practices\" but evaluate if they apply to *this specific context*.\n\n## 4. Edge Case & Failure Mode Analysis\n\n- The \"Happy Path\" is not enough.\n- What happens if inputs are null/empty/malformed?\n- What if the network fails? What if the database is down?\n- What are the concurrency implications?\n\n## 5. Logical Synthesis\n\n- Combine the analysis into a cohesive plan.\n- Weigh trade-offs explicitly (e.g., \"This approach is faster but uses more memory\").\n- Produce a solution that is not just \"working code\" but \"well-engineered software\".\n"
    },
    {
      "id": "deep-threat-model",
      "windsurf": {
        "description": "Systematically identify and mitigate security and privacy threats using modern threat-modeling practices",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep Threat Model Workflow\n\nThis workflow instructs Cascade to assess security and privacy risks in a structured, repeatable way.\n\n## 1. Define Scope, Assets, and Stakeholders\n\n- Clarify what is being modeled:\n  - System, service, feature, or specific data flow.\n- Identify key assets:\n  - Sensitive data, critical operations, business secrets, availability requirements.\n- Note stakeholders and threat actors:\n  - End users, admins, internal services, external partners, attackers.\n\n## 2. Map Architecture and Data Flows\n\n- Outline components and trust boundaries:\n  - Clients, services, databases, third-party APIs, message queues.\n- Describe major data flows:\n  - What data moves where, over which protocols, and under what authentication.\n- Use a simple textual C4-style description if diagrams are not available.\n\n## 3. Identify Threats (STRIDE and OWASP-Inspired)\n\n- Apply a STRIDE-style lens to each component and data flow:\n  - Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege.\n- Cross-check with known vulnerability classes (e.g., OWASP Top 10) relevant to the stack.\n- Record concrete threat scenarios, not just categories.\n\n## 4. Assess Risk and Prioritize\n\n- For each identified threat, estimate:\n  - Likelihood (how easy is it to exploit?).\n  - Impact (on confidentiality, integrity, availability, compliance, reputation).\n- Use simple qualitative ratings (e.g., High/Medium/Low) if detailed quantification is not practical.\n- Prioritize threats that are both high-impact and likely.\n\n## 5. Define Mitigations and Controls\n\n- For prioritized threats, propose mitigations at appropriate layers:\n  - Design (isolate components, least privilege, defense in depth).\n  - Implementation (input validation, output encoding, secure defaults, safe libraries).\n  - Operations (monitoring, alerts, rate limits, WAF rules, incident response).\n- Consider privacy and data-minimization strategies where applicable.\n\n## 6. Capture, Integrate, and Revisit\n\n- Summarize the threat model:\n  - Scope, assets, key threats, mitigations, and residual risk.\n- Link findings to tickets, backlog items, or security requirements.\n- Plan periodic updates:\n  - Re-run or extend the model when introducing new features, integrations, or data flows.\n"
    },
    {
      "id": "deep-ux",
      "windsurf": {
        "description": "Perform a deep UX review using research insights, user journeys, and established usability heuristics",
        "autoExecutionMode": 3
      },
      "body": "\n# Deep UX Workflow\n\nThis workflow instructs Cascade to evaluate and improve user experience systematically, grounded in modern UX practice.\n\n## 1. Understand Users, Tasks, and Contexts\n\n- Identify key user segments and their primary goals.\n- List critical tasks and scenarios this interface must support.\n- Note environmental factors:\n  - Devices, assistive technologies, network conditions, time pressure.\n\n## 2. Map Journeys and Flows\n\n- Outline end-to-end user journeys for critical goals.\n- For each step, identify:\n  - User intent, system response, and potential friction points.\n- Distinguish between journeys (high-level across touchpoints) and flows (detailed UI steps) and use both where helpful.\n\n## 3. Heuristic Evaluation\n\n- Evaluate the interface against recognized usability heuristics (e.g., Nielsen’s 10):\n  - Visibility of system status.\n  - Match between system and real world.\n  - User control and freedom.\n  - Consistency and standards.\n  - Error prevention and recovery.\n  - Recognition rather than recall.\n  - Flexibility and efficiency of use.\n  - Aesthetic and minimalist design.\n  - Help users recognize, diagnose, and recover from errors.\n  - Help and documentation.\n- Note concrete issues with severity and supporting examples.\n\n## 4. Accessibility and Inclusive Design\n\n- Check for basic accessibility practices:\n  - Semantic HTML/roles, focus order, keyboard navigation, contrast, text sizing.\n- Consider diverse users:\n  - Different abilities, cultural contexts, and levels of expertise.\n- Where appropriate, reference relevant guidelines (e.g., WCAG) when evaluating issues.\n\n## 5. Synthesize Issues and Opportunities\n\n- Group findings by flow or UI area.\n- Prioritize based on:\n  - Impact on task success and satisfaction.\n  - Frequency of occurrence.\n  - Ease of remediation.\n- Propose UX improvements that are concrete and implementation-aware.\n\n## 6. Plan Validation\n\n- Suggest ways to validate proposed UX changes:\n  - Remote or in-person usability tests.\n  - Prototype testing, A/B tests, or feature flags.\n- Recommend instrumentation to capture UX signals in production (e.g., drop-off points, completion rates).\n\n## 7. Detect and Avoid AI Slop in AI-Driven UX\n\n- When AI-generated content or AI-powered features are present, explicitly evaluate them for **signal vs. noise**:\n  - Check that AI output is relevant, concise, and clearly advances the user’s task.\n  - Watch for \"AI slop\" symptoms: generic or repetitive text, incoherent or low-fidelity visuals, unverifiable claims, or engagement-bait responses.\n- Assess **trust, control, and transparency** around AI:\n  - Ensure AI output is clearly labeled as such, with access to explanations, sources, or uncertainty cues where appropriate.\n  - Provide user controls to opt out, refine, or correct AI suggestions instead of flooding the interface with unsolicited content.\n- Protect **content quality and hierarchy**:\n  - Avoid auto-filling interfaces with AI-generated material that competes with or obscures user-authored and verified content.\n  - Prefer focused, on-demand AI assistance over persistent, high-volume generation.\n- If AI features do not materially improve UX in this context, recommend constraining, redesigning, or removing them rather than tolerating low-quality AI output.\n\n## 8. Apply World-Class Visual and Interaction Design\n\n- Ground visual and interaction decisions in established, high-quality design systems and guidelines (e.g., Apple Human Interface Guidelines, Material Design 3, Fluent 2, Carbon, and contemporary design-system practice), while adapting them to the specific product, brand, and context rather than copying them verbatim.\n- Evaluate **layout and spacing** deliberately:\n  - Use clear grids, alignment, and consistent spacing scales to create visual rhythm and hierarchy.\n  - Ensure key actions and information follow natural scanning patterns and reading order for the target platform.\n- Treat **typography** as a primary tool for hierarchy and clarity:\n  - Choose sizes, weights, and line lengths that optimize legibility and emphasize what matters most.\n  - Maintain sufficient contrast and typographic consistency across states and breakpoints.\n- Design **components and affordances** to match user intent:\n  - Prefer bespoke or tailored component compositions over generic, overused patterns when the product’s workflows or brand call for it.\n  - Make interactive elements clearly interactive, with sensible hit areas and feedback.\n- Use **motion and microinteractions** purposefully:\n  - Apply subtle, meaningful motion to communicate state changes, reinforce spatial models, and reduce cognitive load.\n  - Avoid gratuitous animations that distract from tasks or create motion sickness.\n- When proposing designs, consider at least one alternative layout or component strategy and, using `/workflow-deep-consider` where helpful, justify why the chosen approach best fits the users, content, and brand instead of defaulting to commodity patterns.\n\n## 9. Use Whimsy and Delight Responsibly\n\n- Start from **usability, reliability, and accessibility** as non-negotiable foundations. Only add whimsy once core flows are fast, clear, and robust.\n- Treat whimsical elements as **microinteractions and moments**, not permanent decoration:\n  - Favor small, contextual animations, sound cues, or playful visuals that respond directly to user actions.\n  - Ensure every delightful detail has a clear trigger, purpose, and feedback loop (per microinteraction best practices).\n- Align whimsy with **brand voice, context, and emotional state**:\n  - Use playful elements where exploration, creativity, or celebration is appropriate.\n  - Avoid whimsy in high-stress, high-risk, or sensitive contexts (e.g., financial loss, medical data, incidents) where it can feel disrespectful.\n- Safeguard **clarity and focus**:\n  - Never let decorative whimsy compete with primary actions, content, or status indicators.\n  - Test that delightful elements do not introduce motion sickness, distraction, or cognitive overload.\n- Ensure **accessibility and control**:\n  - Respect reduced-motion preferences and provide ways to tone down or disable non-essential animation.\n  - Keep whimsical copy and visuals understandable for diverse audiences; avoid in-jokes that exclude or confuse.\n- When recommending whimsical patterns, propose at least one **no-whimsy baseline** and one **delight-enhanced variant**, and use `/workflow-deep-consider` reasoning to decide whether the added delight is justified for this product, audience, and moment.\n\n## 10. Bridge UX to Frontend Implementation\n\n- When proposing UX changes, think through how they will be realized in the frontend stack:\n  - Component and state architecture, routing, data fetching, and error boundaries.\n  - Integration with design tokens, themes, and the design system’s component library.\n- Favor implementation patterns that keep UI concerns modular and testable:\n  - Clear separation between presentation and data/logic where appropriate.\n  - Reusable components with well-defined props, slots, or composition patterns.\n- Consider responsiveness and platform specifics at implementation time:\n  - Breakpoints, input methods (mouse, touch, keyboard), and density/zoom settings.\n  - Cross-browser and cross-device behaviors, including reduced-motion and high-contrast modes.\n- Anticipate performance characteristics of the implementation:\n  - Bundle size, code-splitting, lazy loading, and caching strategies.\n  - Minimizing unnecessary re-renders and overdraw; efficient list and media handling.\n- Ensure the UX plan is reflected in engineering quality practices:\n  - Map critical flows and edge cases into `/workflow-deep-test` coverage.\n  - Align error states, loading states, and empty states with actual API and data behavior.\n- Where trade-offs between UX ideals and engineering constraints emerge, use `/workflow-deep-consider` to make them explicit and document the chosen compromise.\n\n"
    }
  ]
}
