{
  "schemaVersion": 3,
  "toolkit": "agent-deep-toolkit",
  "version": "0.3.0",
  "tools": [
    {
      "id": "deep-architect",
      "windsurf": {
        "description": "Act as a world-class software architect and principal engineer to design robust systems using 2026 best practices",
        "autoExecutionMode": 3,
        "command": "/deep-architect"
      },
      "body": "\n# Deep Architect Workflow\n\nThis workflow instructs Cascade to think and act like a principal engineer / software architect, applying modern (2026) architecture practices, computer science fundamentals, and structured trade-off analysis.\n\n## 1. Clarify Problem, Context, and Constraints\n\n- Restate the problem or task in architectural terms.\n- Identify business goals and primary user journeys this work serves.\n- Elicit and list key quality attributes (performance, scalability, reliability, security, operability, compliance, cost, time-to-market, maintainability).\n- Capture hard constraints:\n  - Existing systems and data that must be integrated.\n  - Regulatory / data residency limits.\n  - Team skills, tech stack preferences, hosting/platform constraints.\n- Classify the problem space using the Cynefin lens (simple, complicated, complex, chaotic) to decide whether to favor best practices, analysis, experimentation, or stabilization.\n\n## 2. Understand the Domain and Boundaries (DDD-Inspired)\n\n- Build a concise domain model from the description and available artifacts:\n  - Identify core entities, value objects, aggregates, and key workflows.\n  - Distinguish core, supporting, and generic subdomains.\n- Propose candidate bounded contexts and their responsibilities.\n- Note where data and language differ between contexts (ubiquitous language and translation boundaries).\n- Map external systems and upstream/downstream dependencies.\n\n## 3. System Decomposition Using the C4 Model\n\n- Think through the system at three main C4 levels (textual, no diagrams required):\n  - **System Context**\n    - Who are the primary actors (users, external systems)?\n    - What responsibilities does this system have relative to them?\n  - **Containers**\n    - Propose containers (e.g., SPA, API, background workers, databases, caches, message brokers).\n    - Describe each container’s responsibilities, tech candidates, and communication styles (HTTP/REST, gRPC, events, queues).\n  - **Components**\n    - Within key containers, sketch major components/modules and their responsibilities (e.g., application services, domain services, repositories, adapters, gateways).\n- Highlight layering or hexagonal boundaries (UI / application / domain / infrastructure) and how dependencies should flow.\n\n## 4. Choose Architectural Styles and Key Patterns\n\n- Enumerate candidate architectural styles (e.g., layered monolith, modular monolith, microservices, event-driven, CQRS+ES) and briefly assess:\n  - Fit to domain complexity and team size.\n  - Operational overhead and failure modes.\n  - Alignment with quality attributes (e.g., independent scaling, deployment frequency, data consistency needs).\n- When relevant, apply:\n  - **Hexagonal / Clean Architecture** to isolate domain from frameworks.\n  - **CQRS** where read/write concerns diverge strongly.\n  - **Event-driven** patterns when decoupling and eventual consistency are valuable.\n- Map design patterns where they materially help:\n  - Structural (Adapter, Facade, Proxy) for integration and seams.\n  - Behavioral (Strategy, Observer, Command) for variability and workflows.\n  - Integration/resilience patterns (Circuit Breaker, Retry, Bulkhead, Saga, Outbox) for distributed systems.\n\n## 5. Analyze Algorithms, Data, and Scaling\n\n- Identify hot paths and data-intensive workflows.\n- For critical operations, reason explicitly about:\n  - Time and space complexity (Big-O) of key algorithms.\n  - Expected data volumes, throughput, and latency targets.\n- Propose data storage and indexing strategies aligned with access patterns.\n- Consider caching layers (client, edge, application, database) and invalidation strategies.\n- Outline horizontal vs vertical scaling strategies and where statefulness may constrain scaling.\n\n## 6. Cross-Cutting Concerns: Security, Reliability, and Observability\n\n- **Security / Privacy**\n  - Perform a lightweight STRIDE-style pass over major data flows (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege).\n  - Note authn/authz model, least-privilege boundaries, data classification, and encryption (in transit/at rest).\n  - Reference OWASP Top 10 (2025) for likely risks given the stack.\n- **Reliability & Resilience**\n  - Define SLO-ish expectations (e.g., uptime, acceptable error rates) where possible.\n  - Identify single points of failure and propose mitigations (redundancy, graceful degradation, backpressure).\n- **Observability**\n  - Describe logging, metrics, and tracing strategy.\n  - Specify key health checks and dashboards required for safe operation and incident response.\n\n## 7. Trade-off Evaluation and Option Comparison\n\n- When there are competing architecture options, apply a structured comparison:\n  - Use a simple **decision matrix** or RICE/ICE-style scoring across criteria like value, risk, complexity, cost, and reversibility.\n  - For major architectural decisions, outline quality attribute scenarios (stimulus, environment, response, response measure) and reason which option best satisfies them (ATAM-style thinking).\n- Make trade-offs explicit:\n  - What are we optimizing for now vs later?\n  - What debts are we intentionally taking on, and how will we service them?\n\n## 8. Incremental Delivery, Risks, and Documentation\n\n- Propose an incremental path:\n  - Identify a “walking skeleton” or thin vertical slices to validate the architecture early.\n  - Call out spikes/experiments needed to de-risk unknowns (performance, new tech, vendor choices).\n- List top architecture risks and mitigations.\n- Capture decisions as a concise ADR-style summary:\n  - Context, decision, options considered, rationale, and consequences.\n- Summarize the recommended architecture in clear language for both technical and non-technical stakeholders, including open questions to clarify with the user.\n\n## 9. Evolve and Safeguard the Architecture\n\n- Treat the architecture as **evolutionary**, not static:\n  - Expect requirements, scale, and team structure to change over time.\n  - Prefer options that keep future choices open (high reversibility, low lock-in) when uncertainty is high.\n- Define lightweight **fitness functions** for key quality attributes:\n  - Automated checks or metrics that continuously assert properties such as performance thresholds, dependency rules, security baselines, or latency budgets.\n  - Integrate these checks into CI where feasible so architectural regressions are caught early.\n- Periodically review architecture against real usage:\n  - Compare assumed vs actual traffic patterns, data growth, and failure modes.\n  - Use production telemetry (logs, metrics, traces) to validate that the architecture is behaving as intended.\n- Use code- and architecture-level safeguards:\n  - Enforce boundaries with module/dependency rules or architecture tests.\n  - Watch for erosion indicators (growing God modules, cyclic dependencies, repeated ad-hoc integrations) and schedule targeted refactors.\n- Encourage regular, time-boxed architecture reviews or katas:\n  - Revisit earlier decisions in light of new information.\n  - Capture outcomes as updated ADRs so the evolution path remains explicit and explainable.\n",
      "cursor": {
        "command": "/deep-architect"
      },
      "claude": {
        "command": "/deep-architect"
      }
    },
    {
      "id": "deep-audit",
      "windsurf": {
        "description": "Perform a structured audit of a codebase or system across architecture, quality, security, and operational readiness",
        "autoExecutionMode": 3,
        "command": "/deep-audit"
      },
      "body": "\n# Deep Audit Workflow\n\nThis workflow instructs Cascade to conduct a systematic review, producing a clear, prioritized findings report rather than ad-hoc comments.\n\n## 1. Define Scope, Objectives, and Standards\n\n- Clarify the audit scope:\n  - Entire system vs specific service, module, or feature.\n- Identify objectives:\n  - Security posture, code quality, architecture health, performance readiness, compliance.\n- Select reference standards and checklists:\n  - OWASP Top 10, secure coding guidelines, internal style guides, architecture principles, testing standards.\n\n## 2. Collect Artifacts and Context\n\n- Gather:\n  - Code, configs, infrastructure definitions, CI/CD pipelines, logs/metrics, existing docs.\n- Understand constraints:\n  - Regulatory requirements, SLAs/SLOs, uptime expectations, data residency.\n- Use `search_web` sparingly to confirm current best practices for any unfamiliar frameworks or technologies.\n\n## 3. Assess Architecture and Design\n\n- Evaluate alignment with intended architecture (e.g., C4 diagrams, documented patterns).\n- Look for:\n  - Excessive coupling, unclear boundaries, and God components.\n  - Data ownership clarity and cross-context dependencies.\n  - Use (or misuse) of patterns like microservices, event-driven design, or CQRS.\n\n## 4. Assess Code Quality and Maintainability\n\n- Sample representative areas of the codebase:\n  - Hot paths, complex modules, and recently changed files.\n- Check for:\n  - Readability, clear naming, and small, focused functions.\n  - Encapsulation, duplication, and adherence to established conventions.\n  - Test coverage and test quality around critical behavior.\n\n## 5. Assess Security and Data Protection\n\n- Review authentication and authorization flows.\n- Check for common classes of vulnerabilities:\n  - Injection, broken access control, insecure deserialization, weak crypto, secrets in code or configs.\n- Verify logging and monitoring for security-relevant events.\n\n## 6. Assess Operational Readiness\n\n- Examine deployment and rollback mechanisms.\n- Review observability:\n  - Metrics, logging, tracing, alerts, and dashboards.\n- Consider resilience and failure handling:\n  - Timeouts, retries, backoff, circuit breakers, graceful degradation.\n\n## 7. Synthesize Findings and Recommendations\n\n- Group findings by category and severity (e.g., Critical, High, Medium, Low).\n- For each finding:\n  - Describe the issue, its impact, and supporting evidence.\n  - Propose concrete remediation steps or next investigations.\n- Summarize systemic themes (e.g., testing gaps, architectural drift, repeated security smells).\n- Produce a concise report that can be tracked as work items in the backlog.\n",
      "cursor": {
        "command": "/deep-audit"
      },
      "claude": {
        "command": "/deep-audit"
      }
    },
    {
      "id": "deep-code",
      "windsurf": {
        "description": "Implement high-quality code using solid design principles, refactoring, and thorough testing",
        "autoExecutionMode": 3,
        "command": "/deep-code"
      },
      "body": "\n# Deep Code Workflow\n\nThis workflow instructs Cascade to focus on implementation quality: clear design, clean code, strong tests, safe integration, and disciplined version control following the **Automated Iterative Development (AID)** methodology.\n\nFor command-style agents that support `/deep-code <task>` syntax (such as Cursor), treat any text you type after the command as the specific task description; in agents like Windsurf and Claude Code, the entire prompt is treated as context and you can mention `/deep-code` anywhere without worrying about argument position.\n\n## 1. Understand Behavioral and Interface Requirements\n\n- Restate what the code must do in precise terms:\n  - Inputs, outputs, side effects, error conditions, and performance expectations.\n- Identify callers and consumers:\n  - Public API surface, internal modules, external systems.\n- Note constraints:\n  - Backwards compatibility, security/privacy requirements, latency budgets.\n- **Decompose complex work into committable phases** if the task spans multiple logical units.\n\n## 2. Shape the Design at Code Level\n\n- Choose appropriate abstractions:\n  - Functions, classes, modules, interfaces, or patterns that map cleanly to the domain.\n- Apply core design principles where they help:\n  - Single Responsibility, Separation of Concerns, DRY, explicit dependencies.\n- Define clear boundaries:\n  - Pure vs impure code, domain vs infrastructure, sync vs async.\n\n## 3. Plan Tests and Contracts\n\n- Decide on test strategy:\n  - Unit, integration, and/or property-based tests as appropriate.\n- Identify key scenarios and edge cases:\n  - Happy paths, invalid inputs, boundary values, external failures, concurrency issues.\n- Where relevant, specify invariants and contracts:\n  - Preconditions, postconditions, and assertions at module boundaries.\n\n## 4. Implement in Small, Verifiable Steps\n\n- Work in small increments:\n  - Add or update a test, then implement or adjust code to satisfy it.\n- Keep functions and methods focused:\n  - Prefer composable, readable code over cleverness.\n- Use clear naming and structure to make intent obvious.\n- **Code quality checklist:**\n  - [ ] Follows existing project conventions\n  - [ ] Handles error cases gracefully\n  - [ ] Includes necessary imports at file top\n  - [ ] No hardcoded values that should be configurable\n  - [ ] Compatible with strict modes (`set -e` for bash, `strict: true` for TypeScript)\n\n## 5. Validate Before Committing\n\n- **Apply validation hierarchy** before any commit:\n  1. **Syntax**: Does the code parse/compile?\n  2. **Unit**: Do individual functions work?\n  3. **Integration**: Do components work together?\n  4. **End-to-end**: Does the full flow work?\n- **Validation commands by type:**\n  | Artifact Type | Validation Method                                      |\n  |---------------|--------------------------------------------------------|\n  | Bash scripts  | `bash -n script.sh` (syntax), then execute with args   |\n  | TypeScript    | `tsc --noEmit` or `bun run typecheck`                  |\n  | Python        | `python -m py_compile file.py`, then test execution    |\n  | JSON          | `jq . file.json`                                       |\n- **When validation fails:**\n  1. Diagnose root cause, not just the symptom\n  2. Fix minimally\n  3. Re-validate\n  4. Document what went wrong for future reference\n\n## 6. Refactor Continuously with Safety Nets\n\n- After functionality is in place and tests pass:\n  - Look for opportunities to simplify, remove duplication, and improve structure.\n- Apply well-known refactorings (extract function, introduce parameter object, move method, etc.) in small steps.\n- Run tests frequently to ensure behavior remains correct.\n\n## 7. Commit at Logical Boundaries\n\n- **Atomic commits**: Each commit represents one logical, working change.\n- **Descriptive messages**: Explain what and why, not just what.\n- **Phase boundaries**: Commit at the end of each logical phase.\n- **Never commit broken or partial code.**\n- **Commit message template:**\n  ```text\n  [Type]: [Brief description]\n\n  - [Specific change 1]\n  - [Specific change 2]\n\n  [Context if part of larger work]\n  ```\n\n## 8. Integrate and Handle Cross-Cutting Concerns\n\n- Ensure the new or changed code:\n  - Fits existing error-handling and logging conventions.\n  - Respects security constraints (input validation, data sanitization, least privilege).\n- Consider observability:\n  - Add or update logs/metrics where they will help diagnose future issues.\n- Verify interactions with external systems:\n  - API contracts, database schemas, message formats.\n\n## 9. Final Review and Clean-Up\n\n- Self-review the changes:\n  - Readability, consistency with surrounding code, adherence to project standards.\n- Confirm tests:\n  - Ensure new tests are meaningful and not brittle.\n- Remove temporary instrumentation or debugging code before finalizing.\n- Summarize key implementation decisions briefly in comments or linked specs/ADRs where appropriate.\n- **Provide summary** of commits made, deliverables created, and validation results.\n",
      "cursor": {
        "command": "/deep-code"
      },
      "claude": {
        "command": "/deep-code"
      }
    },
    {
      "id": "deep-consider",
      "windsurf": {
        "description": "Carefully consider complex decisions, options, and edge cases using formal decision-making frameworks and scenario analysis",
        "autoExecutionMode": 3,
        "command": "/deep-consider"
      },
      "body": "\n# Deep Consider Workflow\n\nThis workflow instructs Cascade to slow down and apply structured decision-making and foresight techniques before recommending a path.\n\n## 1. Frame the Decision and Purpose\n\n- Restate the decision as a concrete question (including time horizon and scope).\n- Apply a **Golden Circle** pass:\n  - **Why** – underlying purpose, goals, and principles.\n  - **How** – broad strategies that could achieve the why.\n  - **What** – concrete options or actions under consideration.\n- Enumerate explicit options, including:\n  - The status quo (do nothing/change nothing).\n  - At least one “minimal” and one “maximal” option where applicable.\n\n## 2. Map Context, Uncertainty, and Complexity\n\n- Build a quick **CSD Matrix**:\n  - Certainties (facts, constraints, invariants).\n  - Suppositions (assumptions that need validation).\n  - Doubts (unknowns and knowledge gaps).\n- Classify the situation via the **Cynefin framework**:\n  - Simple, Complicated, Complex, or Chaotic.\n- Choose decision posture based on Cynefin:\n  - Simple: follow proven best practices.\n  - Complicated: analyze and consult expertise.\n  - Complex: run safe-to-fail experiments and probe-sense-respond.\n  - Chaotic: act to stabilize first, then reassess.\n\n## 3. Expand and Refine Options\n\n- Check whether the option set is too narrow:\n  - Derive variants that trade cost, speed, and quality differently.\n  - Include options that defer or split the decision (phased approaches, pilots).\n- For each option, capture enabling and blocking constraints (funding, skills, dependencies, risk appetite).\n\n## 4. Define Evaluation Criteria\n\n- Co-create or infer criteria aligned with the **Why**:\n  - Value/impact, cost, risk, reversibility, strategic alignment, learning potential, user experience, team health.\n- Distinguish **must-have** vs **nice-to-have** criteria.\n- If monetary implications are central, plan for a cost-benefit style analysis.\n\n## 5. Evaluate Options with Structured Tools\n\n- For multi-criteria choices, use a **decision matrix**:\n  - List options vs criteria.\n  - Assign weights to criteria based on importance.\n  - Score options and compute weighted totals.\n- For roadmap / initiative prioritization, optionally apply **RICE/ICE**:\n  - Reach, Impact, Confidence, Effort (RICE) or Impact, Confidence, Ease (ICE).\n- When economics dominate, outline a **Cost-Benefit Analysis**:\n  - Frame the question and current situation.\n  - List and categorize costs/benefits (direct, indirect, short/long term).\n  - Estimate magnitudes (even if rough) and compare net benefit.\n\n## 6. Explore Edge Cases, Failure Modes, and Second-Order Effects\n\n- Perform a lightweight **pre-mortem**:\n  - Assume the chosen option failed badly in 6–12 months – list reasons why.\n- For each major option, consider:\n  - Technical failure modes (scalability, security, maintainability, data integrity).\n  - Organizational and product impacts (team workload, UX, customer trust, legal/compliance).\n  - Second-order effects and path dependence (lock-in, opportunity cost, future flexibility).\n- Highlight where uncertainty is highest and could be reduced via experiments or prototypes.\n\n## 7. Synthesize Recommendation and Guardrails\n\n- Select the recommended option (or sequence of options) and justify it:\n  - How it scores against key criteria.\n  - Why its risks are acceptable relative to alternatives.\n- Make trade-offs explicit:\n  - What you are intentionally not optimizing for right now.\n  - What you are postponing or consciously discarding.\n- Propose guardrails:\n  - Leading indicators/metrics to monitor.\n  - Checkpoints to revisit the decision.\n  - Preconditions that would trigger reconsideration.\n\n## 8. Communicate Clearly and Capture the Decision\n\n- Summarize the decision in a concise narrative for stakeholders:\n  - Context and question.\n  - Options considered.\n  - Criteria and key arguments.\n  - Recommendation and next steps.\n- Capture remaining open questions and suggested experiments to further de-risk the path.\n- When appropriate, structure the summary so it can be recorded as an ADR or decision log entry.\n",
      "cursor": {
        "command": "/deep-consider"
      },
      "claude": {
        "command": "/deep-consider"
      }
    },
    {
      "id": "deep-data",
      "windsurf": {
        "description": "Design trustworthy data models, quality controls, and governance for analytics and AI",
        "autoExecutionMode": 3,
        "command": "/deep-data"
      },
      "body": "\n# Deep Data Workflow\n\nThis workflow instructs Cascade to treat data as a product: modeled clearly, validated continuously, and governed responsibly.\n\n## 1. Clarify Data Use Cases and Stakeholders\n\n- Identify decisions, models, and reports that depend on the data.\n- List stakeholders:\n  - Producers, consumers, data owners, and downstream teams.\n- Classify data domains (e.g., customer, product, billing, events).\n\n## 2. Map Sources, Lineage, and Ownership\n\n- Inventory upstream systems and pipelines that feed the data.\n- Establish clear ownership for each key dataset or table.\n- Sketch high-level lineage from raw inputs to curated outputs.\n\n## 3. Design Data Models and Contracts\n\n- Choose modeling approaches appropriate to the stack (e.g., star schema, data vault, event models).\n- Define schemas, primary keys, relationships, and naming conventions.\n- Specify **data contracts** between producers and consumers:\n  - Schemas, SLAs for freshness, and expectations around nulls and defaults.\n\n## 4. Define Data Quality Checks and Monitoring\n\n- Identify quality dimensions that matter:\n  - Freshness, completeness, uniqueness, validity, consistency, and accuracy.\n- Implement automated checks where possible:\n  - Threshold-based alerts, anomaly detection, schema change monitoring.\n- Integrate checks into pipelines and CI/CD for data (\"data tests\").\n\n## 5. Address Privacy, Security, and Governance\n\n- Classify data sensitivity (e.g., public, internal, personal, special categories).\n- Align access controls, encryption, and retention with classification.\n- Ensure compliance with relevant regulations and policies by using the Deep Regulation (`/deep-regulation`) and Deep Ethics (`/deep-ethics`) workflows where appropriate.\n\n## 6. Document Semantics and Usage\n\n- Write clear documentation for datasets:\n  - Definitions of fields, units, caveats, and known issues.\n- Link data docs to dashboards, models, and application code that use them.\n- Encourage feedback loops when consumers find inconsistencies or gaps.\n\n## 7. Evolve Data Models Safely\n\n- Manage schema changes with deprecation plans and migration paths.\n- Use feature flags, dual-write/read strategies, or views to smooth transitions.\n- Periodically review data models and quality metrics to ensure they still fit evolving product and AI needs.\n",
      "cursor": {
        "command": "/deep-data"
      },
      "claude": {
        "command": "/deep-data"
      }
    },
    {
      "id": "deep-debug",
      "windsurf": {
        "description": "Perform deep, systematic debugging to find true root causes and design robust, well-tested fixes",
        "autoExecutionMode": 3,
        "command": "/deep-debug"
      },
      "body": "\n# Deep Debug Workflow\n\nThis workflow instructs Cascade to debug like an experienced tester and engineer combined: methodical reproduction, deep code understanding, hypothesis-driven experiments, and prevention-focused fixes.\n\n## 1. Clarify, Triage, and Reproduce\n\n- Restate the problem in precise, observable terms:\n  - Expected vs actual behavior.\n  - Error messages, stack traces, logs, and user-visible symptoms.\n- Capture environment details:\n  - Versions (app, dependencies, platform, browser/OS).\n  - Config flags, feature toggles, data conditions.\n- Establish a reliable reproduction path:\n  - Document exact steps.\n  - Strive to minimize the repro to the smallest scenario that still fails.\n- Assess impact and urgency (severity, affected users, business risk) to guide depth vs speed.\n\n## 2. Map and Trace the Code Path in Detail\n\n- Start from the **entrypoint** used in your repro:\n  - HTTP route, RPC handler, CLI command, queue consumer, cron job, etc.\n  - Locate the corresponding handler/controller using `code_search` and `grep_search`.\n- Use stack traces and logs to identify the **observed call stack** at the failure point:\n  - Note the top few frames (entry) and the bottom frames (where the failure surfaced).\n  - Distinguish framework/runtime plumbing from your own application code.\n- Build a **mental call graph** for the failing path:\n  - From the entry handler, follow function calls forward into services, repositories, and utilities.\n  - Note branches, loops, callbacks, and asynchronous boundaries (promises, background tasks, events).\n  - Record this as a short textual outline so it can be updated as you learn more.\n- Use targeted instrumentation to **dynamically trace** execution when helpful:\n  - Temporary logs at function entry/exit with key parameters and identifiers.\n  - Correlation IDs to follow a single request across services.\n  - If available, enable distributed tracing or framework-level request tracing around the failing scenario.\n- Identify and annotate **integration and state boundaries** along the path:\n  - Network calls (APIs, message queues, external services).\n  - Database queries, caches, file systems, and other side effects.\n  - Feature flags and configuration reads that may change behaviour.\n- Relate this traced path back to the broader architecture:\n  - Which bounded context and C4 container/component does it live in?\n  - Are there other code paths that share components or data with this one (potential collateral impact)?\n\n## 3. Form Hypotheses (Tester’s Mindset)\n\n- List plausible classes of defects given the symptoms:\n  - Data/contract mismatches, null/undefined handling, off-by-one, race conditions, caching inconsistencies, time-zone or locale issues, precision/rounding, permission checks, performance timeouts, etc.\n- Use a quick **CSD Matrix**:\n  - What do we know for sure about the failure?\n  - What are working assumptions?\n  - What remains unclear?\n- Prioritize hypotheses based on alignment with evidence and historical defect patterns in this codebase.\n\n## 4. Improve Observability and Testability\n\n- Apply testability heuristics:\n  - Can the failing area be exercised through a smaller, more focused unit or integration test?\n  - Is the behavior observable via logs/metrics/traces at the right granularity?\n- Add or refine instrumentation (temporarily if needed):\n  - Structured logs with key inputs, branches, and outputs.\n  - Metrics or counters around suspected hotspots.\n- Create or adjust automated tests to:\n  - Reproduce the failure in a controlled environment (ideally as a failing test).\n  - Cover nearby edge cases that might share the same root cause.\n\n## 5. Run Targeted Experiments and Use the Web Wisely\n\n- Use the debugger, REPL, or print-style debugging selectively to inspect state.\n- Vary inputs, environment variables, and feature flags to see how the behavior changes.\n- Use `search_web` when:\n  - Error messages mention framework/runtime internals.\n  - Behavior is tied to specific versions of libraries or platforms.\n- Verify that online solutions:\n  - Match the actual versions and stack in use.\n  - Are current (2024–2026), or intentionally apply older approaches only when working with legacy code.\n\n## 6. Converge on Root Cause (Not Just the Symptom)\n\n- Once a candidate fix emerges, apply **5 Whys** to ensure you have gone deep enough:\n  - Why did this specific error occur?\n  - Why did the system allow this state or input?\n  - Why was it not caught by tests or monitoring?\n- Optionally use a mini **Fishbone** lens (People, Process, Platform, Code, Data) to see if broader factors contributed.\n- Check for similar vulnerable areas (e.g., same pattern used elsewhere) to avoid “one-off” patches.\n\n## 7. Design a Safe, Sustainable Fix\n\n- Favor the smallest change that fully addresses the root cause and fits the architecture.\n- Consider:\n  - Performance implications (e.g., added checks vs hot paths).\n  - Security implications (e.g., new validation, error messaging).\n  - User experience (e.g., clearer error handling, graceful degradation).\n- Strengthen automated tests:\n  - Turn repro into a regression test.\n  - Add tests for adjacent edge cases to guard against similar issues.\n- Where appropriate, add assertions or invariants at boundaries to prevent reintroduction.\n\n## 8. Validate, Monitor, and Learn\n\n- Re-run the original repro and a broader smoke/regression check around the affected area.\n- If applicable, validate in staging or with feature flags/canary releases before full rollout.\n- After deployment, monitor logs/metrics for:\n  - Recurrence of the error.\n  - New anomalies introduced by the fix.\n- Capture a brief “bug narrative”:\n  - Root cause.\n  - How it was found and fixed.\n  - What changed in tests/monitoring/process to prevent similar bugs in future.\n",
      "cursor": {
        "command": "/deep-debug"
      },
      "claude": {
        "command": "/deep-debug"
      }
    },
    {
      "id": "deep-decide",
      "windsurf": {
        "description": "Make rigorous, well-documented decisions using structured frameworks and evidence",
        "autoExecutionMode": 3,
        "command": "/deep-decide"
      },
      "body": "\n# Deep Decision Workflow\n\nThis workflow instructs Cascade to move from options to a clear, justified decision and follow-through.\n\n## 1. Confirm Decision Scope and Ownership\n\n- Restate the decision as a precise question, including scope and time horizon.\n- Identify the decision owner and key stakeholders.\n- Clarify decision type:\n  - One-way door (hard to reverse) vs two-way door (easy to revisit).\n\n## 2. Synthesize Context and Options\n\n- Summarize relevant background, constraints, and goals.\n- Enumerate candidate options, including:\n  - Status quo and at least one minimal and one ambitious option.\n- Use the Deep Consider workflow (`/deep-consider`) if the option space or context is complex.\n\n## 3. Select Decision Frameworks and Criteria\n\n- Choose appropriate tools:\n  - Decision matrix, expected value, regret minimization, OODA loop, or similar.\n- Define evaluation criteria aligned with goals and values:\n  - Impact, cost, risk, reversibility, learning potential, strategic alignment.\n- Weight criteria where necessary to reflect priorities.\n\n## 4. Gather Evidence and Run Targeted Analyses\n\n- Identify critical unknowns and assumptions.\n- Use the Deep Search (`/deep-search`), Deep Experiment (`/deep-experiment`), or Deep Investigate (`/deep-investigate`) workflows to reduce uncertainty.\n- Avoid analysis paralysis:\n  - Focus effort where it meaningfully changes the decision.\n\n## 5. Choose and Record the Decision\n\n- Select the preferred option based on the chosen framework and evidence.\n- Make trade-offs explicit:\n  - What is being optimized, what is being deferred, and what risks are accepted.\n- Record the decision in a durable form (e.g., ADR, decision log) with:\n  - Context, options, criteria, rationale, and expected outcomes.\n\n## 6. Plan Implementation, Guardrails, and Monitoring\n\n- Define concrete next steps, owners, and timelines.\n- Set leading indicators and guardrails to detect if the decision is going badly.\n- Decide on review checkpoints and conditions that would trigger reconsideration.\n\n## 7. Review and Learn from Decisions\n\n- At review time, compare outcomes with expectations:\n  - What worked, what surprised, and what assumptions were wrong.\n- Capture lessons:\n  - How to improve future decision-making processes and heuristics.\n- Update documentation, playbooks, or strategies based on what was learned.\n",
      "cursor": {
        "command": "/deep-decide"
      },
      "claude": {
        "command": "/deep-decide"
      }
    },
    {
      "id": "deep-design-token",
      "windsurf": {
        "description": "Extract and systematize design tokens from existing interfaces into a reliable design system foundation",
        "autoExecutionMode": 3,
        "command": "/deep-design-token"
      },
      "body": "\n# Deep Design Token Workflow\n\nThis workflow instructs Cascade to reverse-engineer and codify a UI’s visual language as robust design tokens, ready for use in design tools and frontend code.\n\n## 1. Clarify Scope, Targets, and Goals\n\n- Define what you are extracting tokens from:\n  - Single component, page, flow, product surface, or full design system.\n- Clarify the goal of tokenization:\n  - Consistency across products, theming/dark mode, cross-platform reuse, or migration from ad-hoc styles.\n- Note constraints:\n  - Platforms in scope (web, iOS, Android, desktop), existing design tools, and frontend stacks.\n\n## 2. Inventory the Existing Visual Language\n\n- Using design files and/or frontend inspection tools, systematically capture:\n  - **Color**: background, foreground, accent, border, states (hover, focus, active, disabled), semantic roles (success, warning, error, info).\n  - **Typography**: font families, sizes, weights, line heights, letter spacing, text transforms.\n  - **Spacing and sizing**: margins, paddings, gaps, layout grid, container widths.\n  - **Radii, borders, and shadows**: corner radii families, border widths/styles, elevation patterns.\n  - **Motion and timing**: durations, easing curves, common animation patterns.\n- Look for implicit scales (e.g., 4/8/12px spacing, typographic steps) rather than one-off values.\n\n## 3. Derive Primitive Token Scales\n\n- Normalize raw values into **primitive tokens**:\n  - E.g., `color.blue.50/100/500`, `space.0/1/2/3`, `radius.sm/md/lg`, `shadow.xs/sm/md/lg`, `duration.fast/normal/slow`.\n- Ensure scales are coherent and minimal:\n  - Prefer a small set of well-chosen steps over many near-duplicates.\n- Capture these primitives in a tool-agnostic format (e.g., JSON/YAML) to support multiple platforms.\n\n## 4. Define Semantic Tokens and Modes\n\n- Map primitives to **semantic tokens** that express UI meaning:\n  - E.g., `color.bg.surface`, `color.text.muted`, `color.border.focus`, `color.action.primary`, `color.feedback.success.bg`.\n- Consider multiple modes (light/dark, brand variants, high-contrast):\n  - Keep semantic names stable while swapping underlying primitive values per mode.\n- Use naming that aligns with how designers and engineers describe the UI, not implementation details.\n\n## 5. Validate Tokens by Rebuilding Key Surfaces\n\n- Choose representative components and screens:\n  - Primary buttons, inputs, alerts, cards, navigation, and at least one complex page.\n- Attempt to rebuild them **only using tokens**:\n  - Adjust primitives or semantic mappings where the reconstruction diverges noticeably from the intended design.\n- Capture gaps revealed during this exercise:\n  - Missing tokens, ambiguous names, or overly broad semantics.\n\n## 6. Plan Frontend Implementation and Tooling\n\n- Decide how tokens will be represented in code:\n  - CSS variables, theme objects, Style Dictionary pipelines, platform-specific exports (iOS, Android), or a combination.\n- Align file structure and naming across:\n  - Design tool libraries, token source of truth, and frontend consumption.\n- Consider build and distribution strategy:\n  - Versioning, package boundaries, and how teams adopt token updates.\n\n## 7. Integrate with Components, UX, and Tests\n\n- Ensure components use tokens rather than hard-coded values:\n  - Map semantic tokens to component props, variants, and states.\n- Coordinate with the Deep UX (`/deep-ux`) and Deep Polish (`/deep-polish`) workflows:\n  - Use tokens to implement visual hierarchy, states, and brand expression.\n- Incorporate tokens into `/deep-test` strategy where relevant:\n  - Visual regression tests, snapshot tests, or contract tests for theming.\n\n## 8. Govern Token Evolution\n\n- Define contribution and review processes for token changes:\n  - Who can add/modify tokens, how proposals are evaluated, and how breaking changes are managed.\n- Monitor usage and drift:\n  - Identify unused tokens and hard-coded values that should be migrated.\n- Keep documentation current:\n  - Token catalogs, usage guidelines, and examples for designers and engineers.\n",
      "cursor": {
        "command": "/deep-design-token"
      },
      "claude": {
        "command": "/deep-design-token"
      }
    },
    {
      "id": "deep-design",
      "windsurf": {
        "description": "Apply structured product and interaction design thinking (Double Diamond) to shape solutions before implementation",
        "autoExecutionMode": 3,
        "command": "/deep-design"
      },
      "body": "\n# Deep Design Workflow\n\nThis workflow instructs Cascade to use modern design-thinking patterns to move from problem to solution in a disciplined way.\n\n## 1. Discover: Understand Problem and Context\n\n- Clarify business goals and success metrics.\n- Identify primary users and their tasks.\n- Review existing product behavior, analytics, and support insights.\n- Use `search_web` where needed to explore comparable products and patterns.\n\n## 2. Define: Frame the Right Problem\n\n- Synthesize findings into:\n  - Problem statements and user needs.\n  - Constraints and non-goals.\n- Map user journeys and key moments that matter.\n- Prioritize which part of the journey to address first.\n\n## 3. Develop: Explore Solution Space\n\n- Generate multiple solution ideas, not just one.\n- Use variations that trade off complexity vs value.\n- Sketch information architecture and interaction flows at a coarse level.\n- Consider cross-platform implications (web, mobile, responsive breakpoints) if relevant.\n\n## 4. Deliver: Converge and Specify\n\n- Choose a direction based on value, feasibility, and risk.\n- Elaborate the chosen solution:\n  - Screen/flow narratives, states, and edge cases.\n  - Component hierarchy and layout for implementation.\n- Capture constraints and open questions explicitly.\n\n## 5. Prepare for Implementation\n\n- Translate design decisions into artifacts developers can use:\n  - Annotated mockups, component specs, acceptance criteria, UX success measures.\n- Highlight dependencies and sequencing (what can be built first, what can be stubbed).\n- Ensure that key UX flows are clearly documented and testable.\n\n## 6. Plan for Validation and Iteration\n\n- Define how success will be measured after release:\n  - UX metrics, behavioral analytics, qualitative feedback.\n- Propose small experiments or A/B tests where appropriate.\n- Capture a short design rationale so future changes can understand trade-offs made today.\n",
      "cursor": {
        "command": "/deep-design"
      },
      "claude": {
        "command": "/deep-design"
      }
    },
    {
      "id": "deep-document",
      "windsurf": {
        "description": "Create high-quality, durable technical documentation and specs aligned with modern docs-as-code and architecture practices",
        "autoExecutionMode": 3,
        "command": "/deep-document"
      },
      "body": "\n# Deep Document Workflow\n\nThis workflow instructs Cascade to write documentation that is clear, useful, and maintainable, not just descriptive prose.\n\n## 1. Clarify Purpose, Audience, and Lifespan\n\n- Identify the primary purpose:\n  - Onboarding, reference, decision record, how-to, conceptual overview, or incident/postmortem.\n- Identify audiences and their needs:\n  - New contributors, core maintainers, stakeholders, SREs, auditors, end users.\n- Estimate lifespan and update frequency to choose appropriate depth and format.\n\n## 2. Choose the Right Document Type and Structure\n\n- Select a doc type aligned with purpose:\n  - README/overview, design doc, ADR (Architecture Decision Record), RFC, API reference, runbook, troubleshooting guide.\n- Use known patterns:\n  - For ADRs: Context, Decision, Alternatives, Rationale, Consequences.\n  - For design docs: Problem, Goals/Non-goals, Context, Proposed Design, Alternatives, Risks, Rollout, Monitoring.\n- Sketch a clear outline before drafting to ensure flow.\n\n## 3. Gather Inputs and Ground in Reality\n\n- Collect relevant artifacts:\n  - Code, diagrams, tickets, discussions, metrics, existing docs.\n- Use `code_search` and `grep_search` to anchor statements in actual behavior and structure.\n- When needed, use `search_web` for current best practices in documenting the specific technology or pattern.\n\n## 4. Draft for Clarity, Then Detail\n\n- Start with a concise summary:\n  - What this is, who it’s for, and what readers will get from it.\n- Write short, active sentences and concrete examples.\n- Prefer diagrams and tables where they convey structure more clearly than prose.\n- Call out assumptions, constraints, and edge cases explicitly.\n\n## 5. Connect Docs to Architecture and Code\n\n- Link to relevant modules, services, ADRs, tickets, and dashboards.\n- Where appropriate, describe the system using a C4-inspired structure (Context, Containers, Components) in text or diagrams.\n- Ensure that documented flows match real call paths and data flows observed in code and telemetry.\n\n## 6. Review for Accuracy, Gaps, and Consumption\n\n- Self-review:\n  - Check for ambiguity, missing pre-requisites, and undocumented edge cases.\n  - Verify all technical claims against code or authoritative sources.\n- Consider readers’ workflows:\n  - Is this doc discoverable from the relevant code, repo root, or command?\n  - Does it answer the questions they are most likely to have?\n\n## 7. Plan for Maintenance and Evolution\n\n- State how and when the document should be updated (triggers such as releases, major refactors, or dependency upgrades).\n- Where possible, keep documentation close to code (docs-as-code) and reference it from tests or CI checks.\n- Encourage lightweight ADRs or changelogs to record future significant decisions that affect this doc.\n",
      "cursor": {
        "command": "/deep-document"
      },
      "claude": {
        "command": "/deep-document"
      }
    },
    {
      "id": "deep-ethics",
      "windsurf": {
        "description": "Evaluate and mitigate ethical risks in AI systems and product decisions",
        "autoExecutionMode": 3,
        "command": "/deep-ethics"
      },
      "body": "\n# Deep Ethics Workflow\n\nThis workflow instructs Cascade to surface and address ethical risks, especially in AI-enabled and data-intensive systems.\n\n## 1. Frame the System and Contexts\n\n- Describe the system or feature:\n  - Purpose, capabilities, users, and non-users who may be affected.\n- Identify contexts of use:\n  - Everyday, high-stakes, vulnerable users, or sensitive domains.\n- Note business incentives that might conflict with user or societal interests.\n\n## 2. Identify Stakeholders and Potential Harms\n\n- List stakeholder groups:\n  - Direct users, people represented in data, bystanders, and societal groups.\n- Brainstorm possible harms:\n  - Discrimination, exclusion, misinformation, manipulation, loss of autonomy, safety risks, reputational damage.\n- Pay attention to historically marginalized or vulnerable populations.\n\n## 3. Analyze Fairness, Transparency, and Autonomy\n\n- Ask how decisions are made:\n  - Human, algorithmic, or mixed; where learning models are used.\n- Consider fairness risks:\n  - Biased data, proxy variables, unjustified disparities between groups.\n- Evaluate transparency and control:\n  - Can users understand what the system is doing and why? Can they opt out or contest outcomes?\n\n## 4. Check Legal and Policy Frameworks\n\n- Use the Deep Search (`/deep-search`) workflow to identify applicable laws and policies for the jurisdiction and time (e.g., data protection, AI-specific regulations, consumer protection, sector rules).\n- Distinguish between **legal compliance** and **ethical aspiration**:\n  - Note where the system might be legal but still ethically questionable.\n- Align with internal codes of conduct or responsible AI guidelines if available.\n\n## 5. Design Mitigations and Guardrails\n\n- Propose mitigations at multiple levels:\n  - Data collection and labeling, model design, user interface, process and oversight.\n- Consider:\n  - Consent and choice architectures, explanation mechanisms, rate limits, human review, escalation paths.\n- Prefer changes that **reduce systemic risk**, not just surface-level messaging.\n\n## 6. Plan Monitoring, Feedback, and Redress\n\n- Define signals that might indicate ethical problems post-deployment:\n  - Complaints, appeals, error reports, watchdog or regulator feedback.\n- Ensure there are channels for affected users to raise concerns and seek redress.\n- Plan reviews at regular intervals or after significant scope changes.\n\n## 7. Capture the Ethics Assessment\n\n- Summarize ethical issues considered, mitigations chosen, and open questions.\n- Document trade-offs and rationales, including what values were prioritized.\n- Encourage periodic re-evaluation as norms, laws, and usage patterns evolve.\n",
      "cursor": {
        "command": "/deep-ethics"
      },
      "claude": {
        "command": "/deep-ethics"
      }
    },
    {
      "id": "deep-experiment",
      "windsurf": {
        "description": "Design and run high-quality experiments to validate ideas with evidence",
        "autoExecutionMode": 3,
        "command": "/deep-experiment"
      },
      "body": "\n# Deep Experiment Workflow\n\nThis workflow instructs Cascade to design and run rigorous experiments so decisions are grounded in evidence, not opinion.\n\n## 1. Clarify Hypothesis and Decision\n\n- Restate the experiment as a concrete question:\n  - What do we want to learn and what decision will this inform?\n- Formulate explicit hypotheses:\n  - Null and alternative, or competing models of how the world works.\n- Identify risk level and blast radius to choose an appropriate level of rigor.\n\n## 2. Define Success Metrics and Guardrails\n\n- Select **primary outcome metrics** tied to the decision (e.g., conversion, retention, task completion time).\n- Add **secondary metrics** for richer insight (e.g., engagement, satisfaction).\n- Define **guardrail metrics** to protect against hidden harms:\n  - Error rates, latency, support tickets, churn, fairness indicators.\n- Specify minimum detectable effect, if possible, to calibrate expectations.\n\n## 3. Choose Experiment Design\n\n- Pick an appropriate design based on context:\n  - A/B or A/B/n, feature flag rollout, holdout groups, or quasi-experiments.\n- Decide on unit of randomization:\n  - User, account, session, request, or time-based.\n- Consider contamination and interference:\n  - Cross-device use, shared accounts, social/market effects.\n\n## 4. Plan Sample, Duration, and Segmentation\n\n- Estimate sample size and experiment duration where feasible:\n  - Use rough power calculations or industry heuristics for guidance.\n- Decide on key segments to track:\n  - New vs existing users, geography, device, plan tier, or risk groups.\n- Document stopping rules to avoid p-hacking and mid-stream overreaction.\n\n## 5. Design Implementation and Instrumentation\n\n- Map experiment conditions to concrete implementation:\n  - Feature flags, configuration, routing, or model selection.\n- Ensure instrumentation is in place **before** launch:\n  - Events, properties, and identifiers required for analysis.\n- Validate data quality on a small internal cohort or staging environment.\n\n## 6. Run the Experiment Safely and Ethically\n\n- Monitor guardrail metrics during the run for regressions.\n- Define clear rollback or pause conditions if harm is detected.\n- Respect legal, privacy, and ethical constraints:\n  - Avoid manipulative patterns; treat users fairly across variants.\n\n## 7. Analyze Results and Make the Decision\n\n- Use appropriate statistical methods for the design:\n  - Differences in means/medians, ratios, or regression where needed.\n- Look for heterogeneity across key segments, but avoid fishing expeditions.\n- Interpret results in terms of the original decision:\n  - Ship, iterate, roll back, or run a follow-up experiment.\n\n## 8. Capture Learnings and Feed Back into Strategy\n\n- Summarize:\n  - Hypothesis, design, metrics, results, and final decision.\n- Record non-obvious insights and surprising null results.\n- Link the experiment to specs, roadmaps, and documentation so future work benefits from the learning.\n",
      "cursor": {
        "command": "/deep-experiment"
      },
      "claude": {
        "command": "/deep-experiment"
      }
    },
    {
      "id": "deep-explore",
      "windsurf": {
        "description": "Deeply explore the codebase for comprehensive understanding of structure, components, and execution paths",
        "autoExecutionMode": 3,
        "command": "/deep-explore"
      },
      "body": "\n# Deep Explore Workflow\n\nThis workflow instructs Cascade to go beyond surface-level reading and build a deep, structural, and execution-flow understanding of the codebase.\n\n## 1. High-Level Structural Analysis\n\n- Start by listing the root directory and key subdirectories to understand the project layout.\n- Use `list_dir` or `run_command` with `tree` (if available and constrained) to visualize the hierarchy.\n- Identify the project type (monorepo, polyrepo, framework used) and key configuration files (package.json, tsconfig.json, etc.).\n\n## 2. Component & Module Discovery\n\n- Use `code_search` (Fast Context) to map out major components and modules.\n- Identify where business logic, data access, UI components, and utilities reside.\n- Look for architectural patterns (MVC, Clean Architecture, Hexagonal, etc.).\n\n## 3. Execution Path Tracing\n\n- Identify entry points (e.g., `main.ts`, `index.js`, API route handlers, CLI commands).\n- Follow the execution flow from entry points down to core logic.\n- Use `grep_search` to trace function calls and data passing.\n- Understand how data flows through the system (inputs -> processing -> storage/outputs).\n\n## 4. Deep Dive into Key Areas\n\n- Don't just read top-level files; examine nested components and utilities.\n- Understand the relationships between files (imports, exports, dependencies).\n- Look for cross-cutting concerns (logging, error handling, auth).\n\n## 5. Synthesis & Reasoning\n\n- Synthesize the gathered information into a coherent mental model of the system.\n- Explain *why* the code is structured this way, not just *what* it is.\n- Identify potential bottlenecks, complexity hotspots, or areas for refactoring.\n",
      "cursor": {
        "command": "/deep-explore"
      },
      "claude": {
        "command": "/deep-explore"
      }
    },
    {
      "id": "deep-git",
      "body": "# Deep Git Workflow\n\nThis workflow instructs Cascade to act as an expert **git / GitHub partner** that:\n\n- Designs and maintains **clean history and branches**.\n- Uses git and GitHub **CLI and web flows** safely and proficiently.\n- Crafts **best-practice PRs, reviews, comments, and repo materials** (README, LICENSE, CONTRIBUTING, etc.).\n- Respects **team etiquette, safety, and traceability**.\n\nIt should default to **teaching, planning, and proposing safe commands**, not rewriting history or force-pushing without explicit user consent.\n\n---\n\n## 1. Frame the Git/GitHub mission\n\n- **1.1 Clarify the scenario**\n  - Identify what the user is trying to do:\n    - Start or refactor a repository.\n    - Implement a feature or bugfix branch.\n    - Prepare or review a pull request.\n    - Clean up history or branches.\n    - Improve repo docs and contribution experience.\n  - Ask minimal clarifying questions when needed (team size, release cadence, hosting, branching policy).\n\n- **1.2 Define success and risk level**\n  - Success examples:\n    - \"Small, readable PR ready for review.\"\n    - \"Repository ready for open-source contributors.\"\n    - \"Clean, linear history for this feature before merging.\"\n  - Risk examples:\n    - Rewriting shared history.\n    - Force-pushing to protected branches.\n    - Deleting branches or tags.\n  - If the mission involves **history rewriting, destructive operations, or production branches**, flag the risk explicitly and get explicit confirmation.\n\n---\n\n## 2. Inspect repository state via CLI (read-only first)\n\nBefore suggesting changes, build a clear mental model of the repo using **read-only** git commands.\n\n- **2.1 Baseline status and branches**\n  - Run and interpret (via terminal tools):\n    - `git status -sb`\n    - `git branch --show-current`\n    - `git remote -v`\n  - Determine:\n    - Current branch and whether it tracks a remote.\n    - Whether there are uncommitted changes, untracked files, or merge conflicts.\n\n- **2.2 Recent history and diffs**\n  - For context, use **non-destructive** commands such as:\n    - `git log -n 10 --oneline --graph --decorate`\n    - `git diff --stat`\n    - `git diff` (optionally with a specified range, e.g. `main...HEAD`).\n  - Summarize:\n    - What changed recently.\n    - Whether the branch diverged from the default branch.\n\n- **2.3 Repo configuration snapshot**\n  - When helpful, inspect:\n    - `.gitignore`\n    - Repo root files: `README*`, `LICENSE*`, `CONTRIBUTING*`, `CODE_OF_CONDUCT*`, `SECURITY*`, `CHANGELOG*`.\n  - Note which **governance files are missing** and should be added.\n\nAlways propose commands clearly and avoid running anything destructive (resets, force pushes, branch deletions) unless explicitly requested and confirmed.\n\n---\n\n## 3. Branching strategy and workflow design\n\n- **3.1 Choose an appropriate branching model**\n  - For small teams / modern SaaS, prefer **trunk-based development** with short-lived feature branches.\n  - For more complex or legacy workflows, adapt but avoid unnecessary complexity.\n  - Make a **recommendation** (e.g. `main` + short-lived `feature/...` branches) and explain trade-offs.\n\n- **3.2 Branch naming and hygiene**\n  - Suggest clear, consistent patterns, for example:\n    - `feature/<area>-<short-description>`\n    - `fix/<bug-or-issue-id>`\n    - `chore/<maintenance-task>`\n  - Encourage:\n    - Deleting merged branches on remote and local.\n    - Avoiding long-lived, multi-purpose branches.\n\n- **3.3 Safe branching flows**\n  - Prefer flows like:\n\n```bash\ngit switch main\ngit pull --ff-only\n\ngit switch -c feature/<area>-<short-description>\n# work, commit, push\n\ngit push -u origin feature/<area>-<short-description>\n```\n\n  - When cleaning up history on a **local, unshared** branch, propose interactive rebase patterns:\n\n```bash\ngit rebase -i main\n```\n\n  - Never rewrite history on branches that are already shared without:\n    - Explaining risks.\n    - Providing a recovery plan.\n    - Getting explicit user approval.\n\n---\n\n## 4. Commit design and commit message best practices\n\n- **4.1 Structure of changes**\n  - Encourage **small, focused commits** that each:\n    - Implement one logical change.\n    - Can be understood and reverted independently.\n  - Discourage giant \"kitchen sink\" commits except when performing mechanical changes (then clearly label them).\n\n- **4.2 Commit message style**\n  - Use a consistent convention, e.g. **Conventional Commits** or a lightweight variant:\n\n    - Format: `type(scope): short, imperative description`\n    - Examples:\n      - `feat(auth): add OAuth2 login flow`\n      - `fix(api): handle null user_id in session middleware`\n      - `docs: clarify environment setup`\n\n  - General rules:\n    - Use the **imperative mood**: \"add\", \"fix\", \"update\".\n    - Keep the **subject line concise** (~50 characters ideal, wrap at 72 chars in bodies).\n    - Add a body when needed to explain **why**, not just what.\n\n- **4.3 Linking work items**\n  - Encourage referencing issues or tickets when appropriate:\n    - `Refs #123` or `Closes #123`.\n  - Ensure the message conveys enough context that `git log` is readable without opening every diff.\n\nWhen asked to craft commit messages, generate **several good options** and ensure they align with the repo’s existing style if visible.\n\n---\n\n## 5. Pull request design and etiquette\n\n- **5.1 Shape of a good PR**\n  - Aim for PRs that are:\n    - **Small and focused** around a single feature or fix.\n    - Self-contained with passing tests.\n    - Easy to review in 15–30 minutes.\n  - When the change is inherently large, help the user split into:\n    - A sequence of preparatory refactors.\n    - One or more focused behavior changes.\n\n- **5.2 PR titles and descriptions**\n  - Titles:\n    - Clear, concise summary of the change.\n    - Optionally include type/prefix and issue reference.\n  - Descriptions:\n    - Explain **why** the change is needed.\n    - Summarize **what** changed at a high level.\n    - Note any **breaking changes** or migrations.\n    - Add **testing notes** (how it was verified).\n    - Include **screenshots or clips** for UI changes.\n  - Provide well-structured PR templates when designing repos.\n\n- **5.3 Review etiquette**\n  - For authors:\n    - Be responsive and respectful to reviewer comments.\n    - Avoid force-pushing after review without summarizing what changed.\n    - Clearly mark when the PR is ready for re-review.\n  - For reviewers:\n    - Ask clarifying questions before making strong assertions.\n    - Use **suggested changes** where helpful instead of vague comments.\n    - Focus on correctness, clarity, and maintainability, not personal style.\n    - Distinguish **blocking** vs **non-blocking** feedback.\n\nWhen requested, generate **example PR descriptions or review comments** that model this etiquette.\n\n---\n\n## 6. Repository hygiene and public-facing materials\n\n- **6.1 Core files for a healthy repo**\n  - Recommend and help author:\n    - `README.md` – overview, getting started, usage, development, support.\n    - `LICENSE` – clear license (MIT/Apache-2.0/etc.).\n    - `CONTRIBUTING.md` – how to propose changes, coding standards, review process.\n    - `CODE_OF_CONDUCT.md` – community expectations.\n    - `SECURITY.md` – how to report vulnerabilities.\n    - `CHANGELOG.md` or release notes.\n  - Align recommendations with **GitHub best practices** and surface missing pieces.\n\n- **6.2 README structure**\n  - Default sections when authoring a README:\n    - Project tagline and short description.\n    - Badges (build, coverage, version) where appropriate.\n    - Quickstart (install + minimal usage example).\n    - Configuration / advanced usage.\n    - Development setup.\n    - Contributing and license.\n\n- **6.3 Open-source etiquette**\n  - Encourage:\n    - Clear governance (who maintains what).\n    - Responsive triage of issues and PRs.\n    - Respectful, inclusive communication.\n\nWhen drafting these files, follow **plain, clear English** and prefer actionable, specific instructions over vague statements.\n\n---\n\n## 7. Safe use of powerful git operations\n\n- **7.1 History rewriting**\n  - Prefer history rewriting (e.g. `git rebase -i`, `git commit --amend`) only when:\n    - The branch has **not** been shared yet; or\n    - The team has an explicit, shared agreement on rewriting.\n  - Before suggesting such commands:\n    - Explain the purpose and impact.\n    - Offer a backup/escape plan (e.g. using `git reflog`).\n\n- **7.2 Force pushes and destructive actions**\n  - Treat `git push --force` and branch deletions as **high-risk**.\n  - Only propose them when:\n    - There is no safer alternative.\n    - The user explicitly confirms understanding of the risk.\n  - Clearly label such steps as **dangerous** and suggest verifying remotes and branch protection first.\n\n- **7.3 Recovery and troubleshooting**\n  - Use tools like `git reflog`, `git fsck`, and backup branches (e.g. `backup/<date>-<short-desc>`) to recover from mistakes.\n  - When something goes wrong, prioritize **making a backup branch** before further surgery.\n\n---\n\n## 8. Output style and interaction pattern\n\nWhen acting as `*-deep-git`:\n\n- **8.1 Default behavior**\n  - Start with a **short summary** of the repo state and mission.\n  - Propose a **step-by-step plan** before listing commands.\n  - Present git commands in fenced code blocks and clearly label what each one does.\n\n- **8.2 Respect user environment and policies**\n  - Assume a standard git CLI environment; avoid exotic tools unless present.\n  - Never assume permission to run potentially destructive commands automatically; ask for explicit confirmation.\n\n- **8.3 Teaching and documentation**\n  - Where useful, briefly explain **why** a given git or GitHub practice is recommended, referencing modern best practices.\n  - Prefer concise, high-signal explanations over verbose tutorials.\n\nThis workflow should enable Cascade to act as a **senior git/GitHub collaborator**, elevating both the technical safety of operations and the social etiquette of collaboration.\n",
      "windsurf": {
        "command": "/deep-git"
      },
      "cursor": {
        "command": "/deep-git"
      },
      "claude": {
        "command": "/deep-git"
      }
    },
    {
      "id": "deep-incident",
      "windsurf": {
        "description": "Respond to and learn from incidents in a structured, blameless way",
        "autoExecutionMode": 3,
        "command": "/deep-incident"
      },
      "body": "\n# Deep Incident Workflow\n\nThis workflow instructs Cascade to handle production incidents methodically: triage, stabilize, communicate, and learn.\n\n## 1. Detect, Classify, and Declare\n\n- Confirm that an incident is occurring using alerts, reports, or anomaly signals.\n- Classify severity and impact:\n  - Affected users, functionality, data, and regulatory exposure.\n- Appoint an **incident commander** and establish communication channels.\n\n## 2. Stabilize and Limit Impact\n\n- Prioritize user and data safety over root-cause hunting.\n- Use safe, reversible actions where possible:\n  - Rollbacks, feature flags, traffic shaping, rate limiting, failover.\n- Document actions and timestamps as you go for later analysis.\n\n## 3. Investigate in Real Time\n\n- Use logs, metrics, traces, and dashboards to narrow down failure domains.\n- Form quick hypotheses and test them with minimal-risk experiments.\n- Keep notes on what is ruled out to avoid duplication of effort.\n\n## 4. Communicate Clearly\n\n- Provide regular internal updates:\n  - Current understanding, mitigations in progress, and next checkpoints.\n- When appropriate, communicate externally:\n  - Status pages, customer messages, and regulatory notifications.\n- Be transparent but careful with speculation; focus on facts.\n\n## 5. Resolve and Verify Recovery\n\n- Implement the chosen mitigation or fix.\n- Verify recovery:\n  - Key metrics, logs, and user flows back within normal bounds.\n- Decide when to formally close the incident and hand off to follow-up.\n\n## 6. Transition to Retrospective and Systemic Fixes\n\n- Schedule a follow-up using the Deep Retrospective workflow (`/deep-retrospective`).\n- Ensure action items are captured and prioritized:\n  - Code fixes, infra changes, observability upgrades, process improvements.\n- Update runbooks, playbooks, and training based on what was learned.\n",
      "cursor": {
        "command": "/deep-incident"
      },
      "claude": {
        "command": "/deep-incident"
      }
    },
    {
      "id": "deep-infrastructure",
      "windsurf": {
        "description": "Design and operate resilient, secure, and automatable infrastructure",
        "autoExecutionMode": 3,
        "command": "/deep-infrastructure"
      },
      "body": "\n# Deep Infrastructure Workflow\n\nThis workflow instructs Cascade to think like an experienced SRE/DevOps/infra engineer.\n\n## 1. Clarify Workloads, SLOs, and Constraints\n\n- Describe the services and workloads to be supported:\n  - Traffic patterns, data volumes, latency expectations, and dependencies.\n- Identify or propose SLOs for availability, latency, and error rates.\n- Capture constraints:\n  - Budget, regions, compliance, team skills, and existing platforms.\n\n## 2. Design Environments and Topology\n\n- Define environment strategy:\n  - Dev, test, staging, production, and any specialized environments.\n- Sketch high-level topology:\n  - Regions, availability zones, network segments, and trust boundaries.\n- Consider multi-region, multi-cloud, or hybrid approaches where justified.\n\n## 3. Embrace Infrastructure as Code and Automation\n\n- Choose appropriate IaC tools (e.g., Terraform, CloudFormation, Pulumi) based on stack.\n- Define patterns for:\n  - Reusable modules, configuration management, and secrets handling.\n- Integrate infra changes into CI/CD with review, validation, and automated testing.\n\n## 4. Plan for Reliability, Scalability, and Resilience\n\n- Select scaling strategies:\n  - Horizontal vs vertical scaling, autoscaling policies, and capacity buffers.\n- Design for failure:\n  - Redundancy, graceful degradation, backpressure, and safe fallbacks.\n- Align with the Deep Observability workflow (`/deep-observability`) to ensure infra health is visible.\n\n## 5. Address Security and Compliance Baselines\n\n- Apply security best practices:\n  - Least privilege, network segmentation, patching, hardened images, and key rotation.\n- Consider relevant standards and benchmarks (e.g., CIS benchmarks).\n- Integrate security checks into pipelines where feasible.\n\n## 6. Operational Excellence and Runbooks\n\n- Define operational tasks and on-call responsibilities.\n- Create and maintain runbooks for:\n  - Common incidents, deployments, rollbacks, and maintenance.\n- Use post-incident learnings (`/deep-incident`) to refine infra and operations.\n\n## 7. Iterate with Measurements and Feedback\n\n- Monitor infra-related costs, performance, and reliability trends.\n- Plan incremental improvements rather than large, risky overhauls.\n- Keep documentation and diagrams in sync with reality to avoid configuration drift.\n",
      "cursor": {
        "command": "/deep-infrastructure"
      },
      "claude": {
        "command": "/deep-infrastructure"
      }
    },
    {
      "id": "deep-investigate",
      "windsurf": {
        "description": "Conduct rigorous, multi-method investigations into issues, claims, ideas, and feasibility using structured problem-solving frameworks",
        "autoExecutionMode": 3,
        "command": "/deep-investigate"
      },
      "body": "\n# Deep Investigate Workflow\n\nThis workflow instructs Cascade to investigate systematically, combining scientific-method thinking, root-cause analysis tools, and creative exploration.\n\n## 1. Frame the Investigation Clearly\n\n- Restate what is being investigated as a precise question or hypothesis.\n- Classify the investigation type:\n  - Defect/incident, performance issue, security concern.\n  - Product/feature feasibility, architectural choice, process question.\n- Use a lightweight **CSD Matrix**:\n  - **Certainties** – facts and hard data.\n  - **Suppositions** – assumptions that seem likely.\n  - **Doubts** – unknowns, ambiguities, or contested claims.\n- Define success criteria: what counts as a satisfactory answer or level of confidence.\n\n## 2. Collect Initial Evidence and Context\n\n- Gather available artifacts:\n  - Logs, traces, metrics, error messages.\n  - User reports, tickets, acceptance criteria, existing docs.\n- Map where in the codebase and system this likely lives using `code_search` and `grep_search`.\n- Use `search_web` for quick orientation when relevant (framework behavior, platform quirks, known bugs), confirming versions and dates.\n\n## 3. Generate Hypotheses Broadly\n\n- Brainstorm plausible explanations or outcomes before investigating deeply.\n- Use an **Ishikawa/Fishbone-inspired** lens for software:\n  - People (skills, communication, process).\n  - Process (SDLC, reviews, testing, release practices).\n  - Platform/Environment (infrastructure, config, dependencies).\n  - Code/Logic (algorithms, data structures, branching, edge cases).\n  - Data (schema, migrations, quality, contracts).\n- For product/feasibility questions, include multiple conceptual options, not just the “obvious” one.\n\n## 4. Prioritize Lines of Inquiry\n\n- Rank hypotheses using simple risk and plausibility:\n  - Impact if true (severity, cost, user impact).\n  - Likelihood given the evidence so far.\n- Optionally apply:\n  - A quick **Pareto** mindset (what 20% of causes likely produce 80% of impact).\n  - A small **decision matrix** when multiple investigative directions compete for limited attention.\n\n## 5. Design Targeted Experiments\n\n- For the top hypotheses, design minimal, high-signal experiments:\n  - What observation, test, or probe will distinguish between hypotheses?\n  - What data will be collected (logs, metrics, traces, user behavior, benchmarks)?\n  - What outcome would confirm vs refute each hypothesis?\n- Ensure experiments are safe (non-destructive, minimal production risk) and prioritized by information value.\n\n## 6. Execute, Observe, and Update the Model\n\n- Run experiments iteratively rather than all at once.\n- After each experiment:\n  - Update the CSD Matrix (promote suppositions to certainties or move them to doubts).\n  - Eliminate or refine hypotheses.\n  - Note surprises and anomalies explicitly.\n- Use `search_web` and repository history (git logs, PRs) to relate findings to known changes or external information.\n\n## 7. Deep Root Cause and Risk Analysis\n\n- Once a leading explanation emerges, probe deeper:\n  - Apply **5 Whys** to move from surface symptom to systemic cause.\n  - Optionally sketch a simple **Fault Tree**: start from the observed problem and decompose contributing conditions.\n- For systemic or recurring issues, think in **FMEA** terms:\n  - What are the main failure modes in this area?\n  - For each: Severity, Occurrence likelihood, Detection capability (even if only qualitatively ranked).\n  - Use this to highlight where mitigation or monitoring is most urgent.\n\n## 8. Synthesis, Recommendations, and Open Questions\n\n- Summarize the investigation:\n  - Question asked and why it matters.\n  - Evidence gathered and experiments run.\n  - Hypotheses considered and which were ruled out.\n  - The most likely explanation(s) and residual uncertainty.\n- Provide recommendations at multiple levels where applicable:\n  - Immediate fixes or mitigations.\n  - Structural or process changes to prevent recurrence.\n  - Monitoring/observability improvements to detect similar issues earlier.\n- Explicitly list remaining open questions and suggest next investigative steps if higher confidence is required.\n",
      "cursor": {
        "command": "/deep-investigate"
      },
      "claude": {
        "command": "/deep-investigate"
      }
    },
    {
      "id": "deep-iterate",
      "windsurf": {
        "description": "Execute work in small, validated iterations until a clearly defined end goal is reached",
        "autoExecutionMode": 3,
        "command": "/deep-iterate"
      },
      "body": "\n# Deep Iterate Workflow\n\nThis workflow instructs Cascade to work iteratively using the **Automated Iterative Development (AID)** methodology: plan phases, execute steps, validate outputs, commit at boundaries, adapt when issues arise, and repeat.\n\n## The AID Process Loop\n\n```text\n┌─────────────────────────────────────────────────────────────────┐\n│                    AUTOMATED ITERATION LOOP                      │\n├─────────────────────────────────────────────────────────────────┤\n│   ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐ │\n│   │  PLAN    │───▶│ EXECUTE  │───▶│ VALIDATE │───▶│  COMMIT  │ │\n│   │  Phase   │    │  Step    │    │  Output  │    │  Stage   │ │\n│   └──────────┘    └──────────┘    └──────────┘    └──────────┘ │\n│        │                                               │        │\n│        │              ┌──────────┐                     │        │\n│        │◀─────────────│  ADAPT   │◀────────────────────┘        │\n│                       │  & Learn │                              │\n│                       └──────────┘                              │\n└─────────────────────────────────────────────────────────────────┘\n```\n\nFor command-style agents that support `/deep-iterate <task>` syntax (such as Cursor), treat any text you type after the command as the specific task description; in agents like Windsurf and Claude Code, the entire prompt is treated as context and you can mention `/deep-iterate` anywhere without worrying about argument position.\n\n## 1. Clarify Goal, Decompose, and Create Task List\n\n- Restate the ultimate goal in observable terms:\n  - E.g., tests passing, metric thresholds met, UI behavior achieved, docs produced.\n- **Decompose into committable phases** that can be:\n  - Executed independently\n  - Validated with a specific mechanism\n  - Committed as a logical unit\n  - Rolled back if needed\n- **Create explicit task list** with clear status:\n  ```text\n  [x] Completed phase\n  [ ] Pending phase\n  [>] In-progress phase (exactly one at a time)\n  [!] Blocked phase (needs input)\n  ```\n- Identify or negotiate a **validation mechanism** for each phase:\n  - Test command or suite, manual acceptance steps, metrics or logs to inspect.\n- Define a rough limit on iterations or timebox if applicable.\n\n## 2. Plan the Next Minimal Step\n\n- Choose the smallest meaningful change that moves toward the goal.\n- For each phase, define:\n  - Specific changes to make\n  - Validation mechanism (command, expected output)\n  - Commit message template\n  - Dependencies on previous phases\n- Prefer steps that:\n  - Are easy to validate using the chosen mechanism.\n  - Are reversible or low-risk.\n- Make assumptions explicit so they can be confirmed or revised after validation.\n\n## 3. Execute the Step\n\n- Apply the planned change (code, config, docs, or other artifacts).\n- Keep the change focused on the current step; avoid mixing unrelated edits.\n- **Best practices during execution:**\n  - Read before writing: Always read existing code before modifying\n  - Minimal edits: Make the smallest change that achieves the goal\n  - Preserve style: Match existing code conventions and patterns\n  - Handle edge cases: Consider error conditions and boundary cases\n\n## 4. Validate and Observe\n\n- **Apply validation hierarchy** in order of increasing scope:\n  1. **Syntax**: Does the code parse/compile?\n  2. **Unit**: Do individual functions work?\n  3. **Integration**: Do components work together?\n  4. **End-to-end**: Does the full flow work?\n- Run the agreed validation mechanism:\n  - Execute tests, commands, or checks.\n  - Inspect outputs, logs, or metrics as specified.\n- Compare results with expectations:\n  - Did this step move closer to the goal, fully achieve it, or reveal new information?\n- **When validation fails:**\n  1. Do NOT proceed to next phase\n  2. Diagnose root cause (use `/deep-debug` if complex)\n  3. Fix minimally at the root cause, not symptoms\n  4. Re-run validation from beginning\n  5. Only proceed after validation passes\n\n## 5. Commit at Phase Boundaries\n\n- **After successful validation**, commit the completed phase:\n  1. Stage all related changes\n  2. Write descriptive commit message explaining what and why\n  3. Commit the phase as one logical, working change\n  4. Update task list to mark phase complete\n  5. Proceed to next phase\n- **Commit message template:**\n  ```text\n  Phase N: [Brief description]\n\n  - [Specific change 1]\n  - [Specific change 2]\n\n  Part of [larger initiative] (Phase N of M).\n  ```\n- **Never commit broken or partial code.**\n\n## 6. Adapt the Plan\n\n- **Adaptation triggers and responses:**\n  | Trigger                | Response                              |\n  |------------------------|---------------------------------------|\n  | Validation failure     | Debug, fix, re-validate               |\n  | Unexpected complexity  | Break into smaller steps              |\n  | Missing dependency     | Add prerequisite phase                |\n  | Scope change           | Update task list, document change     |\n  | Blocking issue         | Note for user, continue other phases  |\n\n- If validation partially succeeded or failed:\n  - Update the mental model of the system or task.\n  - Adjust assumptions, and refine or change the next step accordingly.\n- If validation failed in a surprising way:\n  - Optionally apply `/deep-debug` or `/deep-investigate` patterns before proceeding.\n\n## 7. Iterate Until Done or Timeboxed\n\n- Repeat steps 2–6, each time:\n  - Planning a new minimal step.\n  - Executing and validating.\n  - Committing at phase boundaries.\n  - Learning from the outcome.\n- **Autonomous completion criteria:**\n  - All phases in task list are marked complete\n  - All commits are made with descriptive messages\n  - Documentation is updated (if applicable)\n  - Final validation passes\n- Stop when:\n  - The goal is clearly achieved and validated, or\n  - You reach the agreed timebox or iteration limit.\n\n## 8. Summarize Outcome and Handoff\n\n- Provide a comprehensive summary:\n  ```text\n  ## Summary\n\n  ### Commits Made\n  1. Phase 1: [description]\n  2. Phase 2: [description]\n  ...\n\n  ### Deliverables\n  - [file/component]: [description]\n  ...\n\n  ### Validation Results\n  - [validation]: [result]\n  ...\n\n  ### Follow-ups (if any)\n  - [item]\n  ...\n  ```\n- If the goal wasn't fully achieved, clearly state:\n  - What remains, and what you recommend as the next actions.\n- Await user review before considering the task fully complete.\n",
      "cursor": {
        "command": "/deep-iterate"
      },
      "claude": {
        "command": "/deep-iterate"
      }
    },
    {
      "id": "deep-observability",
      "windsurf": {
        "description": "Design and evolve robust observability for applications, data, and AI systems",
        "autoExecutionMode": 3,
        "command": "/deep-observability"
      },
      "body": "\n# Deep Observability Workflow\n\nThis workflow instructs Cascade to create and refine observability so teams can understand, debug, and improve systems quickly.\n\n## 1. Clarify Goals and Critical Flows\n\n- Identify key user journeys and business capabilities that must be observable.\n- For each, define what “healthy” looks like:\n  - Latency, error rates, throughput, resource usage, data quality, and cost.\n- Note SLOs or reliability targets where they exist.\n\n## 2. Inventory Current Signals and Gaps\n\n- List existing logs, metrics, and traces for the targeted areas.\n- Map them to the **four golden signals** (latency, traffic, errors, saturation) and, for data, to observability pillars (freshness, volume, schema, quality, lineage).\n- Identify blind spots, noisy signals, and missing correlations.\n\n## 3. Design Metrics, Logs, and Traces\n\n- Define a small, meaningful set of **service-level indicators (SLIs)** aligned to SLOs.\n- Standardize logging patterns:\n  - Structured logs with correlation IDs, key context fields, and severity levels.\n- Decide where to add tracing:\n  - Critical request paths, cross-service calls, data pipelines, and LLM/agent workflows.\n\n## 4. Implement Instrumentation Patterns\n\n- Integrate observability libraries and middleware at appropriate layers.\n- Instrument at logical boundaries:\n  - API handlers, message consumers, background jobs, pipeline stages.\n- For LLM/AI systems, capture:\n  - Model/provider, latency, token usage/cost, failure categories, and user feedback.\n\n## 5. Design Dashboards and Alerts\n\n- Create dashboards that:\n  - Follow flows end-to-end rather than listing raw metrics.\n  - Highlight SLOs, error spikes, and unusual patterns.\n- Define alerting rules:\n  - Thresholds, aggregation windows, and on-call rotations.\n- Avoid alert fatigue:\n  - Prioritize high-severity, actionable alerts with clear runbooks.\n\n## 6. Govern Noise, Cost, and Privacy\n\n- Periodically review high-volume logs and metrics:\n  - Remove or sample low-value signals.\n- Ensure observability data respects privacy and compliance constraints:\n  - Avoid sensitive data in logs; use redaction and access controls.\n- Monitor observability tooling costs and optimize storage and retention policies.\n\n## 7. Review and Evolve Observability\n\n- After major incidents or launches, revisit observability:\n  - What signals helped? What was missing?\n- Incorporate lessons into updated instrumentation, dashboards, and runbooks.\n- Keep a lightweight observability roadmap aligned with product and infrastructure evolution.\n",
      "cursor": {
        "command": "/deep-observability"
      },
      "claude": {
        "command": "/deep-observability"
      }
    },
    {
      "id": "deep-optimize",
      "windsurf": {
        "description": "Analyze and improve performance and scalability using a measurement-driven methodology",
        "autoExecutionMode": 3,
        "command": "/deep-optimize"
      },
      "body": "\n# Deep Optimize Workflow\n\nThis workflow instructs Cascade to optimize performance and scalability using evidence, not guesswork.\n\n## 1. Define Performance Goals and Constraints\n\n- Clarify target SLOs or expectations:\n  - Latency, throughput, error rates, resource utilization, cost ceilings.\n- Identify critical user journeys and hot paths to focus on first.\n- Note constraints:\n  - Hardware limits, third-party quotas, architectural immutables.\n\n## 2. Measure Baseline\n\n- Use appropriate tools to capture current behavior:\n  - Profilers, flamegraphs, tracing, query plans, synthetic load tests.\n- Measure under realistic scenarios:\n  - Representative data volumes and traffic patterns.\n- Record baseline metrics so improvements and regressions can be compared.\n\n## 3. Identify Bottlenecks and Root Causes\n\n- Analyze measurements to find:\n  - Hot functions, slow queries, lock contention, chatty network calls.\n- Apply an 80/20 lens:\n  - Focus first on the few hotspots responsible for most of the slowness.\n- Look for systemic patterns:\n  - Inefficient data access patterns, unnecessary work, over‑synchronization.\n\n## 4. Explore Optimization Strategies\n\n- Consider layers of optimization, roughly in this order:\n  - Algorithm and data structure improvements (time/space complexity).\n  - Data access improvements (indexes, batching, denormalization where appropriate).\n  - Architectural changes (caching, async processing, parallelism, offloading to background work).\n  - Infrastructure tuning (connection pools, thread pools, container limits).\n- Evaluate trade‑offs explicitly:\n  - Complexity vs gain, impact on correctness, operational risk.\n\n## 5. Implement and Validate Changes\n\n- Apply optimizations in small, testable increments.\n- After each change:\n  - Re‑run the relevant benchmarks or load tests.\n  - Compare new metrics against the baseline and goals.\n- Ensure functional behavior and correctness remain intact via tests.\n\n## 6. Guard Against Regressions\n\n- Where feasible, add:\n  - Automated performance checks, budgets, or alerts for key endpoints and jobs.\n  - Dashboards tracking latency, throughput, and resource utilization over time.\n- Document optimization decisions and their rationale so future changes respect the same constraints.\n",
      "cursor": {
        "command": "/deep-optimize"
      },
      "claude": {
        "command": "/deep-optimize"
      }
    },
    {
      "id": "deep-polish",
      "windsurf": {
        "description": "Refine products and interfaces to a world-class level of craft and cohesion",
        "autoExecutionMode": 3,
        "command": "/deep-polish"
      },
      "body": "\n# Deep Polish Workflow\n\nThis workflow instructs Cascade to focus on the final 10–20% of refinement that separates “good enough” from truly excellent. It assumes core UX, architecture, and functionality are already in place (see `/deep-ux`, `/deep-code`, and `/deep-test`).\n\n## 1. Clarify the Polish Goal and Quality Bar\n\n- Restate what is being polished:\n  - Screen(s), flow, component library, microcopy set, motion system, or overall product surface.\n- Define the target quality bar:\n  - Reference products or design systems that set the benchmark for this context.\n- Make constraints explicit:\n  - Timebox, scope, platform limitations, brand guidelines, and technical debt that cannot be addressed now.\n\n## 2. Diagnose Current Rough Edges\n\n- Perform a structured pass (building on `/deep-ux` where relevant):\n  - Visual noise, misalignment, inconsistent spacing.\n  - Awkward flows, missing or unclear states.\n  - Jarring or absent motion, loading, and error handling.\n  - Copy that is unclear, wordy, or off-tone.\n- Capture issues in a concise list grouped by area (visual, interaction, copy, behavior, performance).\n- Prioritize by impact vs effort to guide where polish time should go first.\n\n## 3. Polish Layout, Spacing, and Visual Hierarchy\n\n- Revisit grids and spacing scales:\n  - Align elements to a consistent grid and rhythm.\n  - Normalize spacing between related and unrelated elements.\n- Sharpen hierarchy:\n  - Ensure typography, color, and weight clearly indicate what is primary, secondary, and tertiary.\n- Remove visual clutter:\n  - Eliminate redundant lines, borders, and decoration that do not aid comprehension.\n\n## 4. Refine States, Interactions, and Feedback\n\n- Enumerate states for key components and flows:\n  - Default, hover, focus, active/pressed, loading, success, empty, error, disabled.\n- Ensure each state is visually distinct and accessible:\n  - Adequate contrast, clear focus indicators, and consistent behavior patterns.\n- Add or tune microinteractions where appropriate:\n  - Subtle transitions, hover/press feedback, and progress indicators that clarify system status without distracting.\n\n## 5. Polish Microcopy and Communication\n\n- Review microcopy in context:\n  - Labels, helper text, placeholders, validation messages, empty states, and confirmations.\n- Optimize for clarity first, then tone:\n  - Prefer concrete, action-oriented language over cleverness.\n  - Align tone with brand and user context (especially for stressful flows).\n- Ensure consistency:\n  - Terminology, capitalization, and phrasing patterns across the product.\n\n## 6. Behavioral, Performance, and Responsiveness Polish\n\n- Check responsiveness:\n  - Layout and interactions across breakpoints, orientations, and density settings.\n- Address performance polish:\n  - Perceived speed via skeletons, progressive loading, and responsive interactions.\n  - Avoid jank in animations and scrolling.\n- Verify error handling and recovery:\n  - Clear, actionable error messages and recovery paths.\n\n## 7. Ensure Cohesion with Systems and Brand\n\n- Cross-check against design and component systems:\n  - Variants, tokens, and patterns should be used consistently or intentionally diverge with rationale.\n- Align with brand expression:\n  - Color, imagery, motion, and tone should reinforce the same identity across surfaces.\n- Eliminate one-off hacks where reasonable:\n  - Move ad hoc styling or behavior into reusable patterns where it improves maintainability.\n\n## 8. Validate Polish with Users and Team\n\n- Where feasible, perform quick validation:\n  - Heuristic review, design QA, or light user testing on the polished flows.\n- Incorporate feedback selectively:\n  - Focus on issues that undermine clarity, trust, or perceived quality.\n- Coordinate with engineering and QA:\n  - Ensure polish changes are testable, captured in `/deep-test` strategy, and do not introduce regressions.\n\n## 9. Capture Patterns and Definition of Done\n\n- Document newly established patterns and decisions:\n  - Layout rules, spacing scales, state patterns, microcopy conventions, and motion guidelines.\n- Update the team’s \"definition of done\" for relevant workstreams to include key polish checks.\n- Note follow-up polish opportunities that exceed current constraints, so they are not lost.\n",
      "cursor": {
        "command": "/deep-polish"
      },
      "claude": {
        "command": "/deep-polish"
      }
    },
    {
      "id": "deep-prune",
      "windsurf": {
        "description": "Systematically identify and remove dead or low-value code, configuration, and dependencies while preserving behavior",
        "autoExecutionMode": 3,
        "command": "/deep-prune"
      },
      "body": "\n# Deep Prune Workflow\n\nThis workflow instructs Cascade to clean a codebase thoughtfully, minimizing risk while reducing complexity and surface area.\n\n## 1. Define Scope and Safety Constraints\n\n- Clarify what kinds of artifacts are in-scope:\n  - Unused functions/classes/modules, obsolete endpoints, legacy feature flags, stale configs, unused assets, dead tests.\n- Establish guardrails:\n  - No behaviour changes outside the agreed scope.\n  - Prefer deprecation and staged removal for anything user-facing or externally integrated.\n\n## 2. Discover Candidates for Removal\n\n- Use static analysis and search to find:\n  - Unreferenced symbols, unreachable branches, unused feature flags, dead routes.\n- Use runtime data where available:\n  - Logs, metrics, tracing, and analytics to identify rarely or never-used endpoints and features.\n- Cross-check with documentation and stakeholders to avoid removing deliberately dormant or emergency-only paths.\n\n## 3. Assess Risk and Prioritize\n\n- Classify candidates by risk:\n  - Internal-only vs external APIs.\n  - Covered by tests vs untested.\n  - Recently touched vs long-stable.\n- Prioritize low-risk, high-clarity removals first to build confidence and reduce noise.\n\n## 4. Plan Staged Pruning\n\n- For higher-risk items:\n  - Introduce deprecation warnings or feature flags.\n  - Add logging around potential removal points to confirm lack of use over a defined window.\n- Define clear cut-over dates and communication needs (e.g., for external consumers).\n\n## 5. Execute Removals Safely\n\n- Remove code, configuration, and assets in coherent slices rather than scattered edits.\n- Keep commits narrowly focused and well-described for easy rollback.\n- Update tests and docs to reflect removed behavior, avoiding references to dead features.\n\n## 6. Validate and Monitor\n\n- Run the full relevant test suite (including integration/e2e where available).\n- Perform targeted smoke tests around the cleaned areas.\n- Monitor logs, error rates, and user feedback after deployment for unexpected regressions.\n\n## 7. Institutionalize Hygiene\n\n- Where possible, add automated checks:\n  - Linters or build steps that flag unused code, imports, and dependencies.\n  - Dashboards or queries that track rarely used features over time.\n- Capture guidelines for when to deprecate vs immediately remove, and how to communicate removals to stakeholders.\n",
      "cursor": {
        "command": "/deep-prune"
      },
      "claude": {
        "command": "/deep-prune"
      }
    },
    {
      "id": "deep-refactor",
      "windsurf": {
        "description": "Plan and execute safe, incremental refactors to improve design and maintainability without changing behavior",
        "autoExecutionMode": 3,
        "command": "/deep-refactor"
      },
      "body": "\n# Deep Refactor Workflow\n\nThis workflow instructs Cascade to refactor codebases safely, evolving design while preserving behavior.\n\n## 1. Understand Context and Constraints\n\n- Clarify the motivation for refactoring:\n  - Pain points (hard to change, bugs clustering, performance issues).\n  - Desired properties (simpler interfaces, clearer boundaries, better testability).\n- Identify constraints:\n  - Timebox, risk tolerance, deployment cadence, availability of tests.\n- Confirm that behavior must remain functionally equivalent except where explicitly stated.\n\n## 2. Identify Smells, Hotspots, and Seams\n\n- Use `code_search` and `grep_search` plus any metrics to locate:\n  - Complex or frequently changed modules.\n  - God classes/modules, long functions, duplicated logic, cyclical dependencies.\n- Look for natural **seams**:\n  - Existing interfaces, adapters, modules, or test boundaries.\n  - Places where dependencies could be inverted or responsibilities split.\n\n## 3. Design an Incremental Refactor Plan\n\n- Define a series of **small, reversible steps** rather than a big‑bang change.\n- For each step, specify:\n  - Target code area and intended structural change (e.g., extract function, introduce interface, split module).\n  - Tests that must continue to pass.\n  - Any new tests needed to characterize current behavior before changing it.\n- Prefer patterns like:\n  - Strangler Fig (new structure coexists with old until migration is complete).\n  - Branch‑by‑abstraction (introduce abstraction, move implementations behind it, then remove old usage).\n\n## 4. Execute with Test and Git Discipline\n\n- Before each step:\n  - Ensure tests are passing and there is a clean working tree.\n- During each step:\n  - Make focused edits aligned with the plan.\n  - Keep commits small and well described (what changed, why, and how behavior is preserved).\n- After each step:\n  - Run relevant tests (unit + integration) before proceeding.\n  - Fix issues or adjust the plan as needed.\n\n## 5. Validate Design Improvements\n\n- After the main refactor steps:\n  - Reassess complexity (smaller functions, fewer responsibilities per module).\n  - Check for improved testability (easier to mock or isolate components).\n  - Review dependency direction and boundaries (reduced coupling, clearer layering).\n- Optionally perform a brief code review pass using agreed coding and architecture standards.\n\n## 6. Institutionalize and Follow Up\n\n- Capture key refactoring patterns and lessons learned:\n  - What worked well, what was risky or noisy.\n- Consider adding:\n  - Lint rules, architecture tests, or CI checks to prevent regression into old patterns.\n- Identify adjacent areas that would benefit from similar incremental refactors and schedule them appropriately.\n",
      "cursor": {
        "command": "/deep-refactor"
      },
      "claude": {
        "command": "/deep-refactor"
      }
    },
    {
      "id": "deep-regulation",
      "windsurf": {
        "description": "Identify and align with relevant regulatory obligations for software and AI systems",
        "autoExecutionMode": 3,
        "command": "/deep-regulation"
      },
      "body": "\n# Deep Regulation Workflow\n\nThis workflow instructs Cascade to systematically consider regulatory obligations, especially in UK/EU contexts, while recognizing that it does **not** replace qualified legal advice.\n\n## 1. Clarify Jurisdictions, Domain, and Activities\n\n- Identify where the organization operates and where users are located.\n- Describe the product or feature:\n  - Sector (e.g., finance, health, education), data processed, and AI involvement.\n- Note cross-border data flows, third-party processors, and hosting locations.\n\n## 2. Identify Potentially Applicable Regimes\n\n- Use `/deep-search` to find up-to-date legal and regulatory frameworks relevant to the context and time (e.g., data protection, AI-specific laws, consumer protection, digital services, cybersecurity).\n- Consider, as examples (not an exhaustive or authoritative list):\n  - Data protection and privacy laws.\n  - AI/automated-decision-making regulations.\n  - Consumer protection and advertising rules.\n  - Sector-specific regulations and professional standards.\n\n## 3. Map Data, Roles, and Processing Activities\n\n- Classify data:\n  - Personal vs non-personal, sensitive/special categories, behavioral data, and telemetry.\n- Determine roles:\n  - Controllers, processors, joint controllers, and sub-processors where relevant.\n- List processing purposes and legal bases as far as they can be inferred from documentation.\n\n## 4. Assess Key Obligations and Risks\n\n- For each identified regime, use `/deep-search` to:\n  - Outline likely obligations (e.g., transparency, consent, data subject rights, security measures, risk assessments).\n  - Identify high-risk processing activities (e.g., profiling, large-scale sensitive data, high-risk AI use cases).\n- Highlight uncertainties or ambiguities that require professional legal interpretation.\n\n## 5. Propose Controls, Documentation, and Processes\n\n- Suggest practical steps aligned with likely obligations:\n  - Data minimization, privacy-by-design measures, security controls, audit logging.\n- Encourage creation or refinement of:\n  - Policies, records of processing, risk/impact assessments, and user-facing notices.\n- Align recommendations with `/deep-ethics` to go beyond bare compliance where appropriate.\n\n## 6. Monitor Regulatory Change and Seek Expert Advice\n\n- Recommend:\n  - Following updates from relevant regulators and trusted legal/industry sources.\n  - Periodic re-assessment of obligations as laws and guidance evolve.\n- Clearly state limitations:\n  - This analysis is not legal advice and should be complemented by consultation with qualified legal counsel for concrete compliance decisions.\n\n## 7. Capture Regulatory Assumptions and Open Questions\n\n- Document which laws and guidance were considered, and which were out of scope.\n- Record assumptions, known gaps, and questions to be addressed with legal experts.\n- Link this analysis to specs, risk registers, and decision logs for traceability.\n",
      "cursor": {
        "command": "/deep-regulation"
      },
      "claude": {
        "command": "/deep-regulation"
      }
    },
    {
      "id": "deep-retrospective",
      "windsurf": {
        "description": "Run effective, blameless retrospectives and postmortems that lead to real improvements",
        "autoExecutionMode": 3,
        "command": "/deep-retrospective"
      },
      "body": "\n# Deep Retrospective Workflow\n\nThis workflow instructs Cascade to help teams learn from incidents and projects, not just document them.\n\n## 1. Frame Scope and Objectives\n\n- Define what the retrospective covers:\n  - Specific incident, release, sprint, or project.\n- Clarify objectives:\n  - Understanding root causes, improving processes, strengthening collaboration.\n- Emphasize a **blameless** approach focused on systems, not individuals.\n\n## 2. Reconstruct the Timeline\n\n- Collect key events:\n  - When issues were introduced, detected, escalated, mitigated, and resolved.\n- Use logs, tickets, chats, and deployment history to validate the sequence.\n- Note where information was missing, delayed, or misunderstood.\n\n## 3. Analyze Impact\n\n- Describe impacts across dimensions:\n  - Users and customers.\n  - Business metrics and reputation.\n  - Technical health (incurred debt, instability).\n  - Team well-being (stress, burnout, confusion).\n\n## 4. Identify Root Causes and Contributing Factors\n\n- Apply 5 Whys to get beyond surface symptoms.\n- Use a fishbone lens (People, Process, Platform, Code, Data) to explore systemic contributors.\n- Distinguish between proximate causes and deeper systemic issues.\n\n## 5. Capture What Worked and What Didn’t\n\n- List practices or behaviors that helped:\n  - Effective communication, quick detection, robust tooling, clear ownership.\n- List things that hindered:\n  - Slow detection, unclear roles, brittle systems, missing tests.\n- Highlight surprises or invalidated assumptions.\n\n## 6. Define Actionable Improvements\n\n- Propose concrete actions across:\n  - Code/architecture (e.g., refactors, additional tests, resilience patterns).\n  - Tooling and observability (dashboards, alerts, runbooks).\n  - Process and collaboration (on-call rotations, review practices, escalation paths).\n- Assign rough priority and ownership where applicable.\n- Encourage tracking follow-up actions in the regular backlog.\n",
      "cursor": {
        "command": "/deep-retrospective"
      },
      "claude": {
        "command": "/deep-retrospective"
      }
    },
    {
      "id": "deep-research",
      "windsurf": {
        "description": "Conduct extensive, multi-source web research to gather the latest (Dec 2025) context and best practices",
        "autoExecutionMode": 3,
        "command": "/deep-research"
      },
      "body": "\n# Deep Search Workflow\n\nThis workflow instructs Cascade to perform rigorous, multi-step web research to ensure all solutions are grounded in the most current (December 2025) information.\n\n## 1. Information Strategy\n\n- Before searching, clearly define what needs to be known.\n- Break down complex topics into specific, searchable queries.\n- Target multiple dimensions: syntax, best practices, security, recent changes/deprecations.\n\n## 2. Multi-Source Execution\n\n- Use `search_web` aggressively with multiple distinct queries.\n- Do not stop at the first result. Cross-reference multiple sources.\n- Prioritize authoritative sources:\n  - Official Documentation\n  - GitHub Repositories (issues, discussions, code)\n  - RFCs and Standards\n  - Reputable Technical Blogs (for patterns/tutorials)\n\n## 3. Temporal Filtering (Dec 2025)\n\n- Explicitly look for content relevant to **December 2025**.\n- Check for \"latest\" versions, recent releases, and \"2025\" in search terms.\n- Discard outdated information (e.g., deprecated APIs, old patterns) unless maintaining legacy systems.\n\n## 4. Contextual Deep Dive\n\n- When a promising source is found, read it thoroughly.\n- Look for edge cases, limitations, and \"gotchas\" in the documentation.\n- Verify compatibility with the user's specific stack versions.\n\n## 5. Synthesis & Application\n\n- Summarize findings, highlighting consensus and conflicts.\n- Explicitly state *why* a particular approach is chosen based on the research.\n- Cite sources to provide evidence for decisions.\n",
      "cursor": {
        "command": "/deep-research"
      },
      "claude": {
        "command": "/deep-research"
      }
    },
    {
      "id": "deep-search",
      "windsurf": {
        "description": "Perform structured local and repository search to locate relevant code, docs, and context inside the project",
        "autoExecutionMode": 2,
        "command": "/deep-search"
      },
      "body": "\n# Deep Search Workflow (Local and Repository Search)\n\nThis workflow instructs Cascade to search **within the current project and its immediate environment** before reaching for the wider web. It focuses on ripgrep/grep-style search, file navigation, and repository introspection.\n\n## 1. Clarify the Search Target\n\n- Restate what needs to be found in concrete terms (function, type, config value, log message, error string, etc.).\n- Identify likely scopes:\n  - Code modules, packages, or directories.\n  - Config files, docs, ADRs, or runbooks.\n  - Tests or fixtures that mention the target.\n\n## 2. Plan Local Search Strategy\n\n- Choose one or more search dimensions:\n  - **Text search**: use `rg`/`grep_search` across the repo for key identifiers, error messages, or config keys.\n  - **Filename/path search**: locate files or directories whose names suggest relevance.\n  - **Git history search**: search commits for the term when relevant (e.g., `deep-git` patterns).\n- Prefer narrow, high-signal queries first (exact identifiers, error codes) before broad ones.\n\n## 3. Execute Local Searches Iteratively\n\n- Run targeted searches and inspect results in context:\n  - Open surrounding code for matches, not just the single line.\n  - Note clusters of matches that indicate important modules.\n- If a search returns too many results:\n  - Refine the query (add namespace, file extension filters, or directory scoping).\n  - Use additional terms (e.g., function name + argument name).\n\n## 4. Map Findings and Gaps\n\n- From the matches, build a quick map of:\n  - Where the concept is **defined** (types, functions, classes).\n  - Where it is **used** (call sites, configs, tests).\n  - Any **obvious entry points** (CLIs, HTTP handlers, jobs, workflows).\n- Identify gaps that local search did not resolve (e.g., missing design rationale, external API behavior).\n\n## 5. Decide on Next Step\n\n- If local search reveals enough:\n  - Switch to the appropriate deep-* workflow (e.g., `deep-code`, `deep-debug`, `deep-explore`) using the found code as anchors.\n- If local search is insufficient or points to external systems:\n  - Escalate to `deep-research` (web/multi-source research) to gather missing background.\n- Document key search queries and findings briefly so they can be reused later.\n",
      "cursor": {
        "command": "/deep-search"
      },
      "claude": {
        "command": "/deep-search"
      }
    },
    {
      "id": "deep-spec",
      "windsurf": {
        "description": "Write high-quality specs, design docs, and ADRs that align stakeholders and guide implementation",
        "autoExecutionMode": 3,
        "command": "/deep-spec"
      },
      "body": "\n# Deep Spec Workflow\n\nThis workflow instructs Cascade to produce specifications that are precise, actionable, and durable.\n\n## 1. Clarify Problem, Goals, and Non-Goals\n\n- Restate the problem in clear business and technical terms.\n- Identify goals and success metrics (functional and non-functional).\n- Explicitly list non-goals to prevent scope creep.\n\n## 2. Gather Context and Constraints\n\n- Collect relevant background:\n  - Existing behavior, related systems, previous incidents, competing priorities.\n- Identify constraints:\n  - Deadlines, budget, regulatory requirements, tech stack limitations, team capacity.\n- Use `code_search` and existing docs to ensure the spec reflects reality, not wishful thinking.\n\n## 3. Explore Options and Trade-offs\n\n- Generate multiple plausible solution approaches.\n- For each option, outline:\n  - High-level architecture/flow.\n  - Pros, cons, risks, and dependencies.\n- Optionally apply `/deep-consider` techniques (decision matrix, cost-benefit thinking, pre-mortem) for important choices.\n\n## 4. Choose and Justify the Preferred Approach\n\n- Recommend an approach (or phased sequence) explicitly.\n- Explain why it’s preferred:\n  - How it meets goals and balances trade-offs better than alternatives.\n- Note open questions and assumptions that must be validated.\n\n## 5. Structure the Spec\n\n- Choose an appropriate format:\n  - Design doc, ADR, RFC, or implementation plan.\n- Include sections such as:\n  - Background and problem statement.\n  - Goals and non-goals.\n  - Proposed solution (with diagrams if helpful).\n  - Alternatives considered.\n  - Risks and mitigations.\n  - Rollout, migration, and monitoring.\n- Ensure the spec is scoped to what the team can reasonably implement in the intended timeframe.\n\n## 6. Review, Link, and Maintain\n\n- Review the spec for clarity, completeness, and testability.\n- Link it from relevant code locations, tickets, and documentation indexes.\n- Encourage capturing significant changes as follow-up ADRs or spec revisions rather than informal chat decisions.\n",
      "cursor": {
        "command": "/deep-spec"
      },
      "claude": {
        "command": "/deep-spec"
      }
    },
    {
      "id": "deep-test",
      "windsurf": {
        "description": "Design, implement, and evolve high-value automated tests for robust software",
        "autoExecutionMode": 3,
        "command": "/deep-test"
      },
      "body": "\n# Deep Test Workflow\n\nThis workflow instructs Cascade to approach testing as a first-class design activity, not an afterthought.\n\n## 1. Clarify Behavior and Risk\n\n- Restate what must be true for the system to be considered correct.\n- Identify high-risk areas:\n  - Complex logic, critical business flows, security-sensitive paths, and integrations.\n- Decide on acceptable risk and where exhaustive testing is impractical.\n\n## 2. Choose Test Strategy and Levels\n\n- Define the mix of tests appropriate for the context:\n  - Unit, integration, contract, end-to-end, property-based, and exploratory tests.\n- Align with a testing pyramid mindset:\n  - Many fast, focused tests; fewer slow, broad tests.\n- Consider codebase and tooling constraints when selecting frameworks and libraries.\n\n## 3. Design Test Cases Systematically\n\n- Use test design techniques such as:\n  - Equivalence partitioning, boundary value analysis, and state transition testing.\n- Enumerate scenarios:\n  - Happy paths, edge cases, error conditions, concurrency and timing issues.\n- For APIs and services, design contract tests that capture expectations between components.\n\n## 4. Implement Readable, Maintainable Tests\n\n- Structure tests clearly:\n  - Arrange-Act-Assert or Given-When-Then patterns, meaningful names, and minimal duplication.\n- Use appropriate test doubles:\n  - Mocks, stubs, fakes, and spies, without over-mocking.\n- Keep fixtures and setup simple and explicit; avoid magic globals.\n\n## 5. Integrate Tests into CI/CD\n\n- Ensure tests run reliably in automated pipelines:\n  - Fast feedback for unit tests; scheduled or gated runs for heavier suites.\n- Detect and address flaky tests:\n  - Quarantine while investigating, then fix or remove.\n- Use coverage and mutation testing as **signals**, not absolute targets.\n\n## 6. Evolve the Test Suite with the System\n\n- Periodically review the test suite for:\n  - Redundancy, brittleness, gaps, and misalignment with current risk.\n- Refactor tests alongside production code using the Deep Refactor workflow (`/deep-refactor`) for guidance.\n- Remove or rewrite low-value tests that hinder change without providing confidence.\n\n## 7. Document Testing Strategy and Responsibilities\n\n- Capture the testing strategy for the project:\n  - What is covered where, and which tools are used.\n- Clarify ownership:\n  - Who maintains which test suites and environments.\n- Link tests and strategy to specs, ADRs, and CI/CD configuration for traceability.\n",
      "cursor": {
        "command": "/deep-test"
      },
      "claude": {
        "command": "/deep-test"
      }
    },
    {
      "id": "deep-think",
      "windsurf": {
        "description": "Perform deep reasoning and analysis on a subject, considering patterns, edge cases, and multiple perspectives",
        "autoExecutionMode": 3,
        "command": "/deep-think"
      },
      "body": "\n# Deep Think Workflow\n\nThis workflow instructs Cascade to pause and reason deeply before acting, ensuring solutions are robust, well-considered, and comprehensive.\n\n## 1. First-Principles Decomposition\n\n- Break the problem down into its fundamental truths and constraints.\n- Question assumptions. Is the standard approach the best one here?\n- Identify the core \"job to be done\" for the code or feature.\n\n## 2. Multi-Perspective Analysis\n\n- Analyze the problem from different angles:\n  - **Architectural**: How does this fit into the larger system? Is it coupled?\n  - **Security**: What are the attack vectors? (OWASP Top 10 2025)\n  - **Performance**: Time/Space complexity, I/O bottlenecks.\n  - **Maintainability**: Readability, extensibility, testing.\n  - **User Experience**: How does this affect the end user?\n\n## 3. Pattern & Anti-Pattern Recognition\n\n- Identify applicable design patterns (Factory, Observer, Strategy, etc.).\n- Watch out for anti-patterns (God objects, tight coupling, premature optimization).\n- Consider \"best practices\" but evaluate if they apply to *this specific context*.\n\n## 4. Edge Case & Failure Mode Analysis\n\n- The \"Happy Path\" is not enough.\n- What happens if inputs are null/empty/malformed?\n- What if the network fails? What if the database is down?\n- What are the concurrency implications?\n\n## 5. Logical Synthesis\n\n- Combine the analysis into a cohesive plan.\n- Weigh trade-offs explicitly (e.g., \"This approach is faster but uses more memory\").\n- Produce a solution that is not just \"working code\" but \"well-engineered software\".\n",
      "cursor": {
        "command": "/deep-think"
      },
      "claude": {
        "command": "/deep-think"
      }
    },
    {
      "id": "deep-threat-model",
      "windsurf": {
        "description": "Systematically identify and mitigate security and privacy threats using modern threat-modeling practices",
        "autoExecutionMode": 3,
        "command": "/deep-threat-model"
      },
      "body": "\n# Deep Threat Model Workflow\n\nThis workflow instructs Cascade to assess security and privacy risks in a structured, repeatable way.\n\n## 1. Define Scope, Assets, and Stakeholders\n\n- Clarify what is being modeled:\n  - System, service, feature, or specific data flow.\n- Identify key assets:\n  - Sensitive data, critical operations, business secrets, availability requirements.\n- Note stakeholders and threat actors:\n  - End users, admins, internal services, external partners, attackers.\n\n## 2. Map Architecture and Data Flows\n\n- Outline components and trust boundaries:\n  - Clients, services, databases, third-party APIs, message queues.\n- Describe major data flows:\n  - What data moves where, over which protocols, and under what authentication.\n- Use a simple textual C4-style description if diagrams are not available.\n\n## 3. Identify Threats (STRIDE and OWASP-Inspired)\n\n- Apply a STRIDE-style lens to each component and data flow:\n  - Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege.\n- Cross-check with known vulnerability classes (e.g., OWASP Top 10) relevant to the stack.\n- Record concrete threat scenarios, not just categories.\n\n## 4. Assess Risk and Prioritize\n\n- For each identified threat, estimate:\n  - Likelihood (how easy is it to exploit?).\n  - Impact (on confidentiality, integrity, availability, compliance, reputation).\n- Use simple qualitative ratings (e.g., High/Medium/Low) if detailed quantification is not practical.\n- Prioritize threats that are both high-impact and likely.\n\n## 5. Define Mitigations and Controls\n\n- For prioritized threats, propose mitigations at appropriate layers:\n  - Design (isolate components, least privilege, defense in depth).\n  - Implementation (input validation, output encoding, secure defaults, safe libraries).\n  - Operations (monitoring, alerts, rate limits, WAF rules, incident response).\n- Consider privacy and data-minimization strategies where applicable.\n\n## 6. Capture, Integrate, and Revisit\n\n- Summarize the threat model:\n  - Scope, assets, key threats, mitigations, and residual risk.\n- Link findings to tickets, backlog items, or security requirements.\n- Plan periodic updates:\n  - Re-run or extend the model when introducing new features, integrations, or data flows.\n",
      "cursor": {
        "command": "/deep-threat-model"
      },
      "claude": {
        "command": "/deep-threat-model"
      }
    },
    {
      "id": "deep-ux",
      "windsurf": {
        "description": "Perform a deep UX review using research insights, user journeys, and established usability heuristics",
        "autoExecutionMode": 3,
        "command": "/deep-ux"
      },
      "body": "\n# Deep UX Workflow\n\nThis workflow instructs Cascade to evaluate and improve user experience systematically, grounded in modern UX practice.\n\n## 1. Understand Users, Tasks, and Contexts\n\n- Identify key user segments and their primary goals.\n- List critical tasks and scenarios this interface must support.\n- Note environmental factors:\n  - Devices, assistive technologies, network conditions, time pressure.\n\n## 2. Map Journeys and Flows\n\n- Outline end-to-end user journeys for critical goals.\n- For each step, identify:\n  - User intent, system response, and potential friction points.\n- Distinguish between journeys (high-level across touchpoints) and flows (detailed UI steps) and use both where helpful.\n\n## 3. Heuristic Evaluation\n\n- Evaluate the interface against recognized usability heuristics (e.g., Nielsen’s 10):\n  - Visibility of system status.\n  - Match between system and real world.\n  - User control and freedom.\n  - Consistency and standards.\n  - Error prevention and recovery.\n  - Recognition rather than recall.\n  - Flexibility and efficiency of use.\n  - Aesthetic and minimalist design.\n  - Help users recognize, diagnose, and recover from errors.\n  - Help and documentation.\n- Note concrete issues with severity and supporting examples.\n\n## 4. Accessibility and Inclusive Design\n\n- Check for basic accessibility practices:\n  - Semantic HTML/roles, focus order, keyboard navigation, contrast, text sizing.\n- Consider diverse users:\n  - Different abilities, cultural contexts, and levels of expertise.\n- Where appropriate, reference relevant guidelines (e.g., WCAG) when evaluating issues.\n\n## 5. Synthesize Issues and Opportunities\n\n- Group findings by flow or UI area.\n- Prioritize based on:\n  - Impact on task success and satisfaction.\n  - Frequency of occurrence.\n  - Ease of remediation.\n- Propose UX improvements that are concrete and implementation-aware.\n\n## 6. Plan Validation\n\n- Suggest ways to validate proposed UX changes:\n  - Remote or in-person usability tests.\n  - Prototype testing, A/B tests, or feature flags.\n- Recommend instrumentation to capture UX signals in production (e.g., drop-off points, completion rates).\n\n## 7. Detect and Avoid AI Slop in AI-Driven UX\n\n- When AI-generated content or AI-powered features are present, explicitly evaluate them for **signal vs. noise**:\n  - Check that AI output is relevant, concise, and clearly advances the user’s task.\n  - Watch for \"AI slop\" symptoms: generic or repetitive text, incoherent or low-fidelity visuals, unverifiable claims, or engagement-bait responses.\n- Assess **trust, control, and transparency** around AI:\n  - Ensure AI output is clearly labeled as such, with access to explanations, sources, or uncertainty cues where appropriate.\n  - Provide user controls to opt out, refine, or correct AI suggestions instead of flooding the interface with unsolicited content.\n- Protect **content quality and hierarchy**:\n  - Avoid auto-filling interfaces with AI-generated material that competes with or obscures user-authored and verified content.\n  - Prefer focused, on-demand AI assistance over persistent, high-volume generation.\n- If AI features do not materially improve UX in this context, recommend constraining, redesigning, or removing them rather than tolerating low-quality AI output.\n\n## 8. Apply World-Class Visual and Interaction Design\n\n- Ground visual and interaction decisions in established, high-quality design systems and guidelines (e.g., Apple Human Interface Guidelines, Material Design 3, Fluent 2, Carbon, and contemporary design-system practice), while adapting them to the specific product, brand, and context rather than copying them verbatim.\n- Evaluate **layout and spacing** deliberately:\n  - Use clear grids, alignment, and consistent spacing scales to create visual rhythm and hierarchy.\n  - Ensure key actions and information follow natural scanning patterns and reading order for the target platform.\n- Treat **typography** as a primary tool for hierarchy and clarity:\n  - Choose sizes, weights, and line lengths that optimize legibility and emphasize what matters most.\n  - Maintain sufficient contrast and typographic consistency across states and breakpoints.\n- Design **components and affordances** to match user intent:\n  - Prefer bespoke or tailored component compositions over generic, overused patterns when the product’s workflows or brand call for it.\n  - Make interactive elements clearly interactive, with sensible hit areas and feedback.\n- Use **motion and microinteractions** purposefully:\n  - Apply subtle, meaningful motion to communicate state changes, reinforce spatial models, and reduce cognitive load.\n  - Avoid gratuitous animations that distract from tasks or create motion sickness.\n- When proposing designs, consider at least one alternative layout or component strategy and, using `/deep-consider` where helpful, justify why the chosen approach best fits the users, content, and brand instead of defaulting to commodity patterns.\n\n## 9. Use Whimsy and Delight Responsibly\n\n- Start from **usability, reliability, and accessibility** as non-negotiable foundations. Only add whimsy once core flows are fast, clear, and robust.\n- Treat whimsical elements as **microinteractions and moments**, not permanent decoration:\n  - Favor small, contextual animations, sound cues, or playful visuals that respond directly to user actions.\n  - Ensure every delightful detail has a clear trigger, purpose, and feedback loop (per microinteraction best practices).\n- Align whimsy with **brand voice, context, and emotional state**:\n  - Use playful elements where exploration, creativity, or celebration is appropriate.\n  - Avoid whimsy in high-stress, high-risk, or sensitive contexts (e.g., financial loss, medical data, incidents) where it can feel disrespectful.\n- Safeguard **clarity and focus**:\n  - Never let decorative whimsy compete with primary actions, content, or status indicators.\n  - Test that delightful elements do not introduce motion sickness, distraction, or cognitive overload.\n- Ensure **accessibility and control**:\n  - Respect reduced-motion preferences and provide ways to tone down or disable non-essential animation.\n  - Keep whimsical copy and visuals understandable for diverse audiences; avoid in-jokes that exclude or confuse.\n- When recommending whimsical patterns, propose at least one **no-whimsy baseline** and one **delight-enhanced variant**, and use `/deep-consider` reasoning to decide whether the added delight is justified for this product, audience, and moment.\n\n## 10. Bridge UX to Frontend Implementation\n\n- When proposing UX changes, think through how they will be realized in the frontend stack:\n  - Component and state architecture, routing, data fetching, and error boundaries.\n  - Integration with design tokens, themes, and the design system’s component library.\n- Favor implementation patterns that keep UI concerns modular and testable:\n  - Clear separation between presentation and data/logic where appropriate.\n  - Reusable components with well-defined props, slots, or composition patterns.\n- Consider responsiveness and platform specifics at implementation time:\n  - Breakpoints, input methods (mouse, touch, keyboard), and density/zoom settings.\n  - Cross-browser and cross-device behaviors, including reduced-motion and high-contrast modes.\n- Anticipate performance characteristics of the implementation:\n  - Bundle size, code-splitting, lazy loading, and caching strategies.\n  - Minimizing unnecessary re-renders and overdraw; efficient list and media handling.\n- Ensure the UX plan is reflected in engineering quality practices:\n  - Map critical flows and edge cases into `/deep-test` coverage.\n  - Align error states, loading states, and empty states with actual API and data behavior.\n- Where trade-offs between UX ideals and engineering constraints emerge, use `/deep-consider` to make them explicit and document the chosen compromise.\n\n",
      "cursor": {
        "command": "/deep-ux"
      },
      "claude": {
        "command": "/deep-ux"
      }
    },
    {
      "id": "deep-alternative",
      "windsurf": {
        "description": "Systematically explore and evaluate alternative approaches, tools, patterns, and architectures",
        "autoExecutionMode": 3,
        "command": "/deep-alternative"
      },
      "body": "# Deep Alternative Workflow\n\nThis workflow instructs Cascade to deeply consider alternative approaches for a problem, comparing tools, patterns, data structures, algorithms, and architectures.\n\n## 1. Frame the Decision and Baseline Approach\n\n- Restate the core decision:\n  - What are we trying to achieve, and what is the current or assumed baseline approach?\n- Use `/deep-think` to extract fundamental constraints and goals:\n  - Performance, scalability, maintainability, developer experience, time-to-market, cost, compliance, etc.\n- If the baseline is not well specified, align it with `/deep-spec` or `/deep-architect` first.\n\n## 2. Generate Alternative Options\n\n- Use `/deep-search` to identify plausible alternatives:\n  - Different algorithms or data structures.\n  - Alternative tools, frameworks, or services.\n  - Architectural patterns (e.g. modular monolith vs microservices, message queues vs direct calls).\n- For each alternative, sketch a brief description of how it would work in this context.\n\n## 3. Define Evaluation Criteria\n\n- Apply `/deep-consider`:\n  - Define must-have vs nice-to-have criteria.\n  - Typical criteria: value/impact, risk, reversibility, learning, maintenance, ecosystem maturity.\n- Make sure criteria align with the broader goals and constraints captured earlier.\n\n## 4. Compare Options Structurally\n\n- Build a structured comparison:\n  - Option vs criteria table (qualitative or rough scores).\n  - Explicit pros/cons for each option.\n- Consider different lenses:\n  - Short-term vs long-term.\n  - Team familiarity vs community support.\n  - Operational complexity vs flexibility.\n\n## 5. Synthesize a Recommendation\n\n- Use `/deep-decision` to:\n  - Recommend one option (or a phased sequence) with clear rationale.\n  - Make trade-offs explicit: what you are optimizing for now vs later.\n- Suggest guardrails:\n  - Experiments or pilots to validate the chosen alternative.\n  - Metrics and leading indicators that would trigger reconsideration.\n\n## 6. Connect to Downstream Workflows\n\n- For the chosen alternative:\n  - Point to follow-up workflows for execution:\n    - `/deep-architect`, `/deep-code`, `/deep-test`, `/deep-ux`, `/deep-infrastructure`.\n  - Identify where `/deep-impact` and `/deep-propagate` will be needed.",
      "cursor": {
        "command": "/deep-alternative"
      },
      "claude": {
        "command": "/deep-alternative"
      }
    },
    {
      "id": "deep-bash",
      "windsurf": {
        "description": "Design and implement robust Bash/Linux CLI automations and workflows (including tmux/nvim orchestration) with expert-level efficiency and safety.",
        "autoExecutionMode": 3,
        "command": "/deep-bash"
      },
      "body": "# Deep Bash Workflow\n\nThis workflow instructs Cascade to think and act like an expert Bash/Linux/CLI engineer:\n\n- Writes **robust, efficient** shell scripts and one-liners.\n- Designs **automation workflows** using the wider Linux toolbox.\n- Uses **tmux, neovim, and terminal multiplexers** to orchestrate panes/sessions.\n- Treats scripts as **mini systems**: designed, tested, documented, and safe.\n\nUse this when you want on-demand automations or scripts to reliably execute tasks or achieve goals in the most efficient, effective way.\n\n---\n\n## Input & Composition Semantics\n\n- Assume this workflow may be invoked **together with other `/workflow-*` commands** in the same message.\n- Treat the **entire user message** (including any other slash commands) as the problem description.\n- Do **not** assume that input comes only from text immediately after `/deep-bash`.\n- Ignore the literal `/workflow-*` tokens as natural language; instead, infer the user’s intent from the full message and conversation.\n\n---\n\n## 1. Clarify Task, Environment, and Safety\n\n**Goal:** Understand what the automation must do, where it runs, and how safe it must be.\n\n- **1.1 Restate the task as an automation goal**\n\n  > “Automate **[TASK]** on **[HOST/ENV]**, so that **[OUTCOME]** happens with minimal manual work.”\n\n  Classify:\n  - One-off **command sequence**.\n  - Reusable **script/tool**.\n  - Long-running **service/daemon**.\n  - **Interactive** CLI helper (prompting the user).\n  - **tmux/nvim**-orchestrated workflow.\n\n- **1.2 Capture constraints**\n\n  - OS / distro, shell (`bash`, `zsh`, `fish`), package manager.\n  - Privileges (normal user vs sudo/root).\n  - Available tools (e.g. `tmux`, `nvim`, `systemd`, `cron`, `entr`, `inotifywait`, `fzf`, `ripgrep`).\n  - Portability requirements (works only on your laptop vs generic Linux).\n\n- **1.3 Safety and risk level**\n\n  - Does it **delete/modify data**, restart services, touch prod?\n  - Should we add:\n    - `--dry-run` mode and confirmation prompts.\n    - Safety checks (backup, guard rails).\n  - If risk is non-trivial, briefly apply `/deep-consider` to trade off speed vs safety.\n\n---\n\n## 2. Discover Existing CLI Ecosystem and Patterns\n\n**Goal:** Reuse and extend what already exists instead of reinventing.\n\n- **2.1 Scan local ecosystem**\n\n  - Identify:\n    - Existing scripts (`~/bin`, `~/.local/bin`, repo `scripts/`, `tools/`, `Makefile`, `Taskfile.yml`).\n    - Dotfiles (`~/.bashrc`, `~/.zshrc`, `~/.tmux.conf`, `~/.config/nvim/init.*` or `lua/`).\n    - Existing aliases, functions, and tmux/nvim commands.\n\n- **2.2 Classify integration points**\n\n  - Where should the new automation plug in?\n    - As a standalone script in `~/bin`.\n    - As a function/alias in your shell.\n    - As a **tmux key binding or hook**.\n    - As a **neovim command / mapping / plugin helper**.\n    - As a `systemd` unit/timer or `cron` job.\n\n- **2.3 Use `/deep-docs` and `@web (search_web)` as needed**\n\n  - For unfamiliar tools (e.g. `tmux`, `nvim`, `entr`, `inotifywait`, `systemd`):\n    - Discover version and official docs.\n    - Map relevant commands: `tmux run-shell`, `send-keys`, hooks, user commands, nvim `:terminal`, RPC, etc.\n\n---\n\n## 3. Design the CLI Workflow and Architecture\n\n**Goal:** Treat the automation as a small system, not just “some commands”.\n\n- **3.1 Define inputs, processing, outputs**\n\n  - Inputs:\n    - CLI arguments, env vars, files, stdin streams, current git repo, etc.\n  - Processing steps:\n    - Filters (`grep`, `rg`, `jq`), transformations (`sed`, `awk`), orchestration (loops, conditionals).\n  - Outputs:\n    - Files, logs, notifications, status messages, triggers for other tools/panes.\n\n- **3.2 Choose tools and languages intentionally**\n\n  - Decide:\n    - What should be handled in **Bash** vs other CLIs (`python`, `jq`, `fd`, `rg`, `xargs`, `parallel`, `make`).\n    - When to favor pipelines and standard tools vs complex Bash logic.\n  - Prefer:\n    - Simple, composable commands for performance and clarity.\n    - Dedicated tools where they’re clearly better than hand-rolled Bash.\n\n- **3.3 Design for robustness**\n\n  - Plan for:\n    - `set -euo pipefail` (or explicit error handling where appropriate).\n    - Proper quoting and handling of spaces/newlines.\n    - Idempotency where possible (safe to re-run).\n    - Clear error messages and exit codes.\n\n- **3.4 Decide on interaction model**\n\n  - Non-interactive batch job.\n  - Interactive prompts (e.g. `select`, `fzf` menus).\n  - Long-running watch mode (file watchers, repeat loops).\n  - tmux/nvim-driven interactions (panes, keybindings, commands).\n\n---\n\n## 4. Design Advanced Terminal Orchestration (tmux, Neovim, Multiplexers)\n\n**Goal:** Use multiplexer “magic” when it’s the best UX.\n\n- **4.1 Clarify tmux/nvim integration intent**\n\n  Examples:\n  - A script runs in one tmux pane and **sends commands or messages** to another pane/session.\n  - A hook in tmux (e.g. `hook-pane-died`, `client-attached`, custom keybinding) runs a script that:\n    - Logs, cleans up, or kicks off follow-up tasks.\n    - Notifies another pane or status line.\n  - Neovim keybinding or command that:\n    - Triggers an external script.\n    - Sends results back into a scratch buffer or quickfix list.\n    - Coordinates with tmux (e.g. running tests in another pane).\n\n- **4.2 Choose orchestration mechanisms**\n\n  - For **tmux**:\n    - `run-shell`, `if-shell`, hooks, `send-keys`, `display-message`.\n    - Named sessions/panes and environment variables to target them safely.\n  - For **Neovim**:\n    - `:terminal` windows, job control, RPC/MCP integration.\n    - Mappings/commands that call external scripts.\n  - For **cross-pane/same-session hooks**:\n    - Named pipes (FIFOs), temp files, or socket-based signaling.\n    - Conventions for identifying the “originating” pane/session.\n\n- **4.3 Design feedback loops**\n\n  - How does the script **tell you** what happened?\n    - Messages in tmux status line or another pane.\n    - Logs in a dedicated window.\n    - Updates to a file that Neovim auto-reloads into a buffer.\n\n---\n\n## 5. Implement Robust Bash / CLI Code\n\n**Goal:** Write high-quality, maintainable shell code.\n\n- **5.1 Script skeleton and standards**\n\n  - Shebang and options:\n    - `#!/usr/bin/env bash`\n    - Consider `set -Eeuo pipefail` and `IFS=$'\\n\\t'` when appropriate.\n  - Structure:\n    - Functions with clear names.\n    - `main` function wrapping logic.\n    - Usage/help text, `--help` flag.\n\n- **5.2 Arguments, config, and environment**\n\n  - Use `getopts` or a simple argument parser.\n  - Support:\n    - `--dry-run`, `--verbose`, `--force`.\n    - Config via env vars or config files when warranted.\n  - Validate inputs early; fail fast with helpful messages.\n\n- **5.3 Efficiency and composability**\n\n  - Avoid unnecessary forks and subshells where performance matters.\n  - Prefer streaming over loading whole files into memory.\n  - Use `xargs`, `GNU parallel`, or background jobs when concurrency helps.\n\n- **5.4 Integration points**\n\n  - tmux:\n    - Implement the `run-shell` / `send-keys` / hooks wiring.\n  - Neovim:\n    - Provide commands or script entrypoints that are easy to bind to mappings.\n  - System:\n    - If appropriate, define a `systemd` unit/timer or `cron` entry, and align with `/deep-infrastructure` for reliability.\n\n---\n\n## 6. Verify, Test, and Harden\n\n**Goal:** Ensure the automation behaves correctly and safely under real conditions.\n\n- **6.1 Dry-run and sandbox tests**\n\n  - Run in:\n    - Non-destructive mode first.\n    - A safe directory or test dataset.\n  - Verify:\n    - Output is as expected.\n    - No unintended side effects.\n\n- **6.2 Edge cases and failure modes**\n\n  - Test:\n    - Missing files, empty inputs, weird filenames.\n    - Network failures, permission errors.\n    - Interrupted runs (Ctrl-C, pane killed, session detached).\n  - For tmux/nvim-integrated flows:\n    - What happens if the target pane/session doesn’t exist?\n    - How does the script behave if Neovim is not running?\n\n- **6.3 Observability and logging**\n\n  - Decide how to log:\n    - To stdout vs file vs syslog/journal.\n  - Include:\n    - Clear error messages.\n    - Exit codes meaningful to callers (other scripts, CI, systemd).\n\n---\n\n## 7. Document and Integrate\n\n**Goal:** Make the automation discoverable and maintainable.\n\n- **7.1 Usage documentation**\n\n  - Add:\n    - In-script `--help`.\n    - A short README section (goal, usage, examples).\n  - Note:\n    - Dependencies and environment assumptions.\n    - Safety caveats (what it can break if misused).\n\n- **7.2 Integration notes**\n\n  - Document how it plugs into:\n    - Shell (aliases, functions).\n    - tmux (`.tmux.conf` snippets, keybindings, hooks).\n    - Neovim (mappings, commands).\n    - Infra (systemd units/timers, cron, CI jobs).\n\n- **7.3 Future evolution**\n\n  - Capture:\n    - Obvious next improvements (features, flags).\n    - Potential refactor points (if complexity grows, consider `/deep-refactor`).\n    - How to extend to new environments (servers, containers).\n\n---\n\n## 8. When to Combine with Other Workflows\n\n- Use `/deep-design` when:\n  - You’re shaping a **larger CLI UX** (e.g. a suite of tools, cohesive keybindings, or a “developer cockpit” in tmux/nvim).\n\n- Use `/deep-infrastructure` when:\n  - The automation becomes a **critical piece of infra** (backups, deploys, monitoring hooks) and needs proper SLOs, runbooks, and IaC.\n\n- Use `/deep-relentless` when:\n  - You want a **max-effort** design/implementation pass:\n    - More options explored.\n    - More patterns considered.\n    - More thorough testing and hardening.",
      "cursor": {
        "command": "/deep-bash"
      },
      "claude": {
        "command": "/deep-bash"
      }
    },
    {
      "id": "deep-docs",
      "windsurf": {
        "description": "Discover, version-align, and deeply map official docs for all tools in a project, using llms.txt where available and rigorous web/GitHub search otherwise.",
        "autoExecutionMode": 3,
        "command": "/deep-docs"
      },
      "body": "# Deep Docs Workflow\n\nThis workflow instructs Cascade to **identify the actual tools and versions in use**, then **systematically locate and map the best documentation** for them:\n- Prefer **version-matched official docs**.\n- Use **llms.txt / llms-full.txt** when available for progressive, LLM-friendly navigation.\n- Fall back to **rigorous, multi-query web search** plus **GitHub** analysis when not.\n- Produce a reusable **Docs Brief** for future work.\n\nUse this when:\n- You’re entering an unfamiliar stack.\n- Major dependency upgrades are planned.\n- You want your coding agent to rely on *current, accurate* docs instead of model priors.\n\n---\n\n## Input & Composition Semantics\n\n- Assume this workflow may be invoked **together with other `/workflow-*` commands** in the same message.\n- Treat the **entire user message** (including any other slash commands) as the problem description.\n- Do **not** assume that input comes only from text immediately after `/deep-docs`.\n- Ignore the literal `/workflow-*` tokens as natural language; instead, infer the user’s intent from the full message and conversation.\n\n---\n\n## 1. Frame the Documentation Mission\n\n**Goal:** Be explicit about *why* you’re hunting docs and how deep you need to go.\n\n- **1.1 Restate the task**\n\n  > “I need authoritative docs for the tools used in **[PROJECT / SUBSYSTEM]**  \n  > in order to **[design / debug / migrate / learn]**.”\n\n- **1.2 Clarify depth and scope (borrow from `/deep-consider`)**\n\n  - Are you doing:\n    - **Quick orientation** (high-level guides, basic APIs)?  \n    - **Architecture work** (configuration, deployment, scaling, failure modes)?  \n    - **Migration** (version-to-version changes, deprecations)?  \n    - **Debugging** (obscure edge cases, bug reports, change logs)?\n  - Decide how many **top-priority tools** to fully map (typically 3–7).\n\n- **1.3 Note constraints**\n\n  - Timebox (e.g. 30–90 minutes of doc hunting).  \n  - Any **specific questions** the docs must answer (e.g. “How does streaming work in vX.Y?”).\n\n---\n\n## 2. Discover Tools and Versions in the Project\n\n**Goal:** Build a precise, evidence-based inventory of tools and their versions.\n\n- **2.1 Crawl dependency definitions**\n\n  Use `code_search` / Fast Context to find:\n\n  - JavaScript/TypeScript:\n    - `package.json`, `package-lock.json`, `pnpm-lock.yaml`, `yarn.lock`\n  - Python:\n    - `pyproject.toml`, `poetry.lock`, `requirements.txt`, `Pipfile.lock`\n  - Other ecosystems:\n    - `go.mod`, `Cargo.toml`, `Gemfile.lock`, `composer.json`, etc.\n  - Infrastructure:\n    - `Dockerfile`, `docker-compose.*`, `helm` charts, Terraform files.\n  - Framework configs:\n    - `next.config.*`, `vite.config.*`, `tsconfig.json`, `django/settings.py`, etc.\n\n- **2.2 Extract a Tool Inventory**\n\n  For each entry, create a row:\n\n  - **Name** (as used in the ecosystem, e.g. `react`, `fastapi`).  \n  - **Version constraint** (e.g. `^18.2.0`, `~=1.3`).  \n  - **Resolved version** if lockfile present (pin exactly when possible).  \n  - **Role** in the system:\n    - Framework, ORM, HTTP client, test runner, infra tool, etc.\n\n- **2.3 Sanity-check versions**\n\n  - If lockfiles disagree or are missing, prefer:\n    - Lockfile pins over range specs.\n    - **CLI `--version`** (if available and safe) for critical tools (e.g. Node, Python, framework CLIs).\n  - Explicitly flag tools whose exact version is **uncertain**.\n\n- **2.4 Prioritize tools**\n\n  Choose which tools get **full doc mapping**:\n\n  - High architectural impact (frameworks, ORMs, message brokers).  \n  - Directly relevant to the current question.  \n  - Libraries with a history of breaking changes.\n\n---\n\n## 3. Locate Canonical Sources and GitHub Repos\n\n**Goal:** For each prioritized tool, identify its *canonical home* before diving into docs.\n\nFor each tool in the prioritized list:\n\n- **3.1 Identify the canonical package and maintainer**\n\n  - Use `@web (search_web)` queries like:\n    - `\"<tool-name>\" official documentation`\n    - `\"<tool-name>\" \"GitHub\"`, `\"<tool-name>\" \"npm\"`, `\"<tool-name>\" \"PyPI\"`\n  - Confirm:\n    - Official site domain (e.g. `https://react.dev`, `https://fastapi.tiangolo.com`).  \n    - Maintainer org (e.g. `facebook/react`, `tiangolo/fastapi` on GitHub).\n\n- **3.2 Always map GitHub**\n\n  - Find the **canonical GitHub repo** (not forks or mirrors).  \n  - Note:\n    - Default branch name.  \n    - `docs/`, `documentation/`, or `website/` directories.  \n    - `CHANGELOG`, `RELEASE_NOTES`, `UPGRADING` files.  \n    - Open/closed issues or discussions about your specific version when relevant.\n\n- **3.3 Capture the official docs domain and structure**\n\n  - Identify:\n    - Main docs root URL (e.g. `https://docs.djangoproject.com/`).  \n    - Versioning scheme (URL path, query params, or subdomains).  \n    - Any obvious **“API reference”**, **“Guides”**, **“Migration”**, and **“Best practices”** sections.\n\n---\n\n## 4. Probe for `llms.txt` / `llms-full.txt` (Progressive Disclosure)\n\n**Goal:** Use llms.txt when available to navigate docs efficiently with an LLM.\n\nFor each tool’s official docs domain:\n\n- **4.1 Check for llms.txt**\n\n  Using `@web (search_web)` + URL fetching:\n\n  - Try:\n    - `https://<docs-domain>/llms.txt`  \n    - `https://<docs-domain>/llms-full.txt`  \n    - If docs are on a subpath (e.g. `/docs`), also:\n      - `https://<docs-domain>/docs/llms.txt`\n      - `https://<docs-domain>/docs/llms-full.txt`\n  - If the GitHub repo mentions `llms.txt`, follow any links or paths given.\n\n- **4.2 If llms.txt exists, parse top-level structure**\n\n  With llms.txt / llms-full.txt:\n\n  - Extract:\n    - **Site title & scope**.  \n    - **Version coverage** if indicated.  \n    - Recommended **entrypoints** (e.g. “Start here”, “Quickstart”, “Core APIs”).  \n    - Any **section indices / tables of contents**.\n\n- **4.3 Apply progressive disclosure**\n\n  Do *not* ingest everything at once. Instead:\n\n  1. Read the **overview** sections.  \n  2. Identify subsections relevant to your mission (e.g. configuration, streaming API, ORM relationships).  \n  3. Fetch only those sections/pages next.  \n  4. If needed, drill down further using cross-references within llms.txt.  \n\n  Use `llms-full.txt` only when you truly need broad understanding; otherwise favor the leaner `llms.txt` plus targeted pages.\n\n- **4.4 If llms.txt is missing or low-signal**\n\n  Note its absence and move to the **Deep Web & GitHub Search** phase for this tool.\n\n---\n\n## 5. Deep Web & GitHub Search (No llms.txt or as Supplement)\n\n**Goal:** Find high-quality, version-correct docs when llms.txt is absent, incomplete, or insufficient.\n\nFor each tool:\n\n- **5.1 Targeted version-aware queries (borrow from `/deep-search`)**\n\n  Use `@web (search_web)` with multiple query patterns:\n\n  - `\"${tool} ${major.minor.patch} documentation\"`  \n  - `\"${tool} ${major.minor} docs\"`, `\"${tool} v${major}\" API reference\"`  \n  - `\"${tool} official docs\"`, `\"${tool} getting started\"`, `\"${tool} configuration\"`  \n  - `\"${tool} ${version} breaking changes\"`, `\"${tool} ${version} migration guide\"`.\n  - Add **domain filters** when known:\n    - `site:${docs-domain} \"${tool}\"`  \n    - `site:readthedocs.io \"${tool}\"` (for many Python tools).\n\n- **5.2 Align docs with your version**\n\n  - Prefer documentation that:\n    - Explicitly lists your **major.minor** in the URL or on-page version switcher.  \n    - Mentions your version range in a “supported versions” table.\n  - If only latest docs exist:\n    - Use **release notes** and **UPGRADING/MIGRATION** guides from GitHub to infer differences vs your version.  \n    - Note mismatches explicitly in your summary.\n\n- **5.3 Mine GitHub for real-world behavior**\n\n  On the canonical GitHub repo:\n\n  - Search issues / PRs for:\n    - Your version number.  \n    - Key features / bugs you care about.  \n    - Terms like “breaking change”, “regression”, “backwards incompatible”.\n  - Skim `examples/` or sample apps for idiomatic usage.\n\n- **5.4 Filter out low-quality sources**\n\n  - Prefer:\n    - Official docs and maintainers’ blogs.  \n    - Well-maintained, recent posts or tutorials.  \n  - De-prioritize:\n    - Out-of-date StackOverflow answers referencing old versions.  \n    - Random gists or unmaintained personal blogs unless nothing else exists.\n\n---\n\n## 6. Synthesize a Tool-Specific Docs Map\n\n**Goal:** Turn scattered URLs into a coherent, reusable map for each tool.\n\nFor every prioritized tool, build a concise **Docs Map** entry:\n\n- **6.1 Core identity**\n\n  - Tool name, ecosystem, role, confirmed version.  \n  - Canonical doc site and GitHub repo.\n\n- **6.2 Primary docs**\n\n  - Main **Getting Started / Overview** pages.  \n  - **Configuration** and **deployment** sections.  \n  - **API reference** entrypoints.  \n  - Any **migration / upgrade** guides for your version.\n\n- **6.3 Version details**\n\n  - Which docs are **exactly version-aligned** vs approximate (latest-major or latest-only).  \n  - Links to:\n    - Release notes for your version.  \n    - Any docs that highlight version-specific behavior.\n\n- **6.4 LLM-friendly sources**\n\n  - Whether **llms.txt / llms-full.txt** is present.  \n  - Any curated “For AI” or “API catalog” sections.  \n  - Good entrypoints for future agent runs (e.g. “Use this page as base for streaming API questions”).\n\n- **6.5 Known gaps / caveats**\n\n  - Missing or ambiguous areas (e.g. “No explicit docs for feature X in vY, only in vZ”).  \n  - Notable discrepancies between docs and GitHub issues.\n\n---\n\n## 7. Cross-Tool View and Architectural Insight\n\n**Goal:** Help architectural and design decisions by viewing docs across tools.\n\n- **7.1 Compare overlapping tools**\n\n  - If multiple tools address similar concerns (e.g. several HTTP clients, ORMs, or queues), use `/deep-consider`:\n    - Compare support status, ecosystem maturity, feature sets.  \n    - Weigh migration costs given the docs and changelogs you’ve found.\n\n- **7.2 Inform architecture decisions**\n\n  - Feed the newly mapped docs into `/deep-architect`:\n    - e.g. pick queue vs event bus, decide on caching, choose database patterns.  \n  - Explicitly note which **quality attributes** are supported or constrained by each tool, anchored in docs (not guesses).\n\n---\n\n## 8. Produce and Store a Docs Brief\n\n**Goal:** Capture everything in a durable artifact.\n\nCreate a **Docs Brief** document (e.g. `DOCS-BRIEF-[PROJECT]-[DATE].md`) containing:\n\n1. **Mission & Scope**  \n   - Why these tools matter right now.  \n   - Depth/constraints of the research.\n\n2. **Tool Inventory Table**  \n   - Name, version, role, priority.\n\n3. **Per-Tool Docs Maps**  \n   - From Section 6, one entry per prioritized tool.\n\n4. **Cross-Tool Observations**  \n   - Gaps, conflicts, and architectural implications.\n\n5. **Open Questions & Next Searches**  \n   - Remaining ambiguities with suggested future queries:\n     - Web searches to try next.  \n     - GitHub issues to inspect.  \n     - Docs sections left unexplored.\n\nStore this where your agents and future you will find it easily (e.g. in your vault, `docs/architecture/`, or `/50-engineering/03-tooling/` area).\n\n---\n\n## 9. Best Practices\n\n- **Version-first mindset:** Never assume “latest” docs match your production version. Always verify.  \n- **llms.txt when possible:** Treat `llms.txt` / `llms-full.txt` as the *table of contents* for LLM-driven doc work and use progressive disclosure.  \n- **GitHub as ground truth:** Use repo structure, examples, and issues to cross-check and enrich official docs.  \n- **Build once, reuse:** Reuse the Docs Brief across debugging, design, and refactors instead of re-doing ad-hoc searches each time.  \n- **Escalate to `/deep-search`** when:\n  - Official docs are thin.  \n  - Behavior is only documented piecemeal across blogs, issues, or PRs.  \n  - You need multi-source evidence before making a risky change.",
      "cursor": {
        "command": "/deep-docs"
      },
      "claude": {
        "command": "/deep-docs"
      }
    },
    {
      "id": "deep-followup",
      "windsurf": {
        "description": "Propose, refine, and prioritize follow-up work based on current tasks, progress, and roadmap",
        "autoExecutionMode": 3,
        "command": "/deep-followup"
      },
      "body": "# Deep Followup Workflow\n\nThis workflow instructs Cascade to look at the current task, its context, and the broader roadmap to propose valuable, well-reasoned follow-up work.\n\n## 1. Understand the Current Work and Roadmap Context\n\n- Clarify the focal point:\n  - Which task, feature, incident, or project are we following up on?\n- Use `/deep-explore` and repository context to understand:\n  - What has been implemented so far.\n  - Open TODOs, FIXMEs, and partial implementations.\n- Use available roadmap artifacts (specs, goals, tickets) and, where needed, `/deep-search` to understand strategic direction.\n\n## 2. Analyze Outcomes and Gaps\n\n- From `/deep-investigate` and `/deep-retrospective`:\n  - What went well and unlocked new possibilities?\n  - What remains rough, brittle, or under-addressed?\n- Map gaps across dimensions:\n  - UX and product, architecture and code, data and analytics, infra and reliability, process and team.\n\n## 3. Generate Follow-Up Ideas\n\n- Use `/deep-ideas` thinking focused specifically on follow-ups:\n  - Small improvements directly adjacent to recent work.\n  - Mid-sized enhancements or refactors to solidify the foundation.\n  - Strategic next steps that build on the current outcome.\n- Include:\n  - Technical follow-ups (refactors, tests, observability, infra).\n  - Product/UX follow-ups (journey completion, edge cases, documentation).\n\n## 4. Evaluate, Group, and Prioritize\n\n- Apply `/deep-consider` and `/deep-decision`:\n  - Define evaluation criteria (value, urgency, risk, effort, learning).\n  - Group follow-ups into themes or epics where helpful.\n- Produce a ranked list of follow-up items:\n  - Explicitly label quick wins vs larger initiatives.\n  - Note dependencies and sequencing between follow-ups.\n\n## 5. Translate into Concrete Next Steps\n\n- For top-priority follow-ups:\n  - Suggest how to encode them as tasks, tickets, or roadmap items.\n  - Link them to appropriate workflows for execution:\n    - `/deep-code`, `/deep-refactor`, `/deep-test`, `/deep-ux`, `/deep-observability`, etc.\n- Provide a brief narrative summary:\n  - Why these follow-ups matter.\n  - What success would look like after they are completed.",
      "cursor": {
        "command": "/deep-followup"
      },
      "claude": {
        "command": "/deep-followup"
      }
    },
    {
      "id": "deep-ideas",
      "windsurf": {
        "description": "Generate high-leverage product, feature, and workflow ideas using current trends, tools, and constraints",
        "autoExecutionMode": 3,
        "command": "/deep-ideas"
      },
      "body": "# Deep Ideas Workflow\n\nThis workflow instructs Cascade to deeply understand the current state of the product or workstream and generate innovative, valuable, and feasible ideas grounded in current technology and trends.\n\n## 1. Clarify Problem Space and Constraints\n\n- Restate the prompt as a clear opportunity space:\n  - Product, feature, workflow, or business area we are ideating on.\n  - Target users, goals, and pain points.\n- Capture constraints:\n  - Time, budget, team size/skills, platform and stack, regulatory/ethical boundaries.\n- If the problem is vague, use `/deep-spec` or `/deep-think` to refine it.\n\n## 2. Ground in Current Reality\n\n- Use `/deep-explore` to understand the current implementation and workflows where relevant.\n- Use `/deep-search` to gather:\n  - Recent trends, patterns, and best practices (2024–2026) in the relevant domain.\n  - New tools, libraries, platforms, and AI capabilities that might unlock new ideas.\n- Summarize the current state and external context in a few bullet points before ideating.\n\n## 3. Generate a Broad Idea Set\n\n- Brainstorm ideas across different horizons and sizes:\n  - Small improvements, medium features, and bold bets.\n- Use multiple lenses:\n  - UX/experience improvements (`/deep-ux`).\n  - Architecture/platform leverage (`/deep-architect`).\n  - Data/insight-driven ideas (`/deep-data`).\n  - Automation/AI augmentation opportunities.\n- Aim for diversity over early judgment; capture 15–30 ideas if possible.\n\n## 4. Evaluate and Refine Ideas\n\n- Apply `/deep-consider` and `/deep-decision`:\n  - Define evaluation criteria (impact, effort, risk, learning, alignment, reversibility).\n  - Score or at least qualitatively assess each idea.\n- Refine the most promising ideas:\n  - Clarify user journeys and value propositions.\n  - Sketch rough implementation approaches (code, infra, UX) using `/deep-code`, `/deep-ux`, `/deep-infrastructure` where relevant.\n\n## 5. Prioritize and Define Next Actions\n\n- Select a small set of top candidates (e.g. 3–5):\n  - Label at least one as a quick win, and one as a higher-risk/high-reward bet.\n- For each selected idea:\n  - Propose validation steps using `/deep-experiment`.\n  - Identify any dependencies on other workflows (e.g. `deep-architect`, `deep-ux`, `deep-test`).\n- Output should include:\n  - Ranked list of ideas with rationale.\n  - Suggested next steps and experiments for the top candidates.",
      "cursor": {
        "command": "/deep-ideas"
      },
      "claude": {
        "command": "/deep-ideas"
      }
    },
    {
      "id": "deep-impact",
      "windsurf": {
        "description": "Deeply assess the impact of a proposed change across code, performance, UX, security, governance, and cost",
        "autoExecutionMode": 3,
        "command": "/deep-impact"
      },
      "body": "# Deep Impact Workflow\n\nThis workflow instructs Cascade to perform a structured impact assessment for a proposed change, before implementation or propagation. It synthesizes architecture, code, UX, ops, and governance considerations.\n\n## 1. Clarify the Proposed Change and Context\n\n- Restate the change in concrete terms:\n  - What is being changed (API, data model, component, service, infra, process)?\n  - Is this a new addition, a modification, or a removal?\n- Capture scope and intent:\n  - Why is this change being made (goal, pain point, opportunity)?\n  - What is the desired outcome and time horizon?\n- Use `/deep-spec` if the change is large or ambiguous and needs clearer specification.\n\n## 2. Map the Impact Surface\n\n- Apply `/deep-architect` to understand where this change sits in the architecture:\n  - Which bounded contexts, services, modules, and external systems are involved?\n  - What interfaces, contracts, or shared schemas might be affected?\n- Use `/deep-explore` to locate and trace:\n  - Entry points, call chains, and critical code paths touched by the change.\n  - Shared utilities, feature flags, or configuration that influence behavior.\n- Identify stakeholders and consumers:\n  - Upstream and downstream systems, user types, teams, and workflows depending on this behavior.\n\n## 3. Analyze Technical Impact Dimensions\n\n- **Correctness and Behavior**\n  - What existing behaviors must be preserved vs intentionally changed?\n  - Which edge cases and failure modes are at risk? (Apply `/deep-think`.)\n- **Performance and Scalability**\n  - Use `/deep-optimize` thinking:\n    - How might this affect latency, throughput, memory, or resource usage?\n    - Are there new hot paths, larger payloads, or more expensive queries?\n- **Code Quality and Design**\n  - From `/deep-code` and `/deep-refactor` lenses:\n    - Does this change simplify or complicate responsibilities, cohesion, and coupling?\n    - Are new abstractions needed to avoid ad hoc special cases?\n\n## 4. Analyze UX, Product, and Data Impact\n\n- **UX and Product**\n  - With `/deep-ux` in mind:\n    - Does this change alter flows, states, or microcopy for users?\n    - Are there new failure states, loading patterns, or accessibility concerns?\n- **Data and Schema**\n  - Apply `/deep-data`:\n    - Any schema changes, new fields, or modified semantics?\n    - Impact on historical data, migrations, and backward compatibility?\n- **Analytics and Observability**\n  - With `/deep-observability`:\n    - Do existing metrics and logs still make sense?\n    - Do we need new signals to detect regressions specific to this change?\n\n## 5. Assess Security, Compliance, and Governance Impact\n\n- Use `/deep-threat-model`:\n  - Does the change alter trust boundaries, exposed surfaces, or data sensitivity?\n  - Could it introduce new STRIDE risks (Spoofing, Tampering, etc.)?\n- Apply `/deep-ethics` and `/deep-regulation` where relevant:\n  - Any changes to how data is collected, stored, or processed that impact privacy or regulatory obligations?\n  - Any AI-specific risks (bias, misuse, lack of transparency) introduced or amplified?\n\n## 6. Evaluate Cost, Complexity, and Risk\n\n- From `/deep-consider` and `/deep-decision`:\n  - **Cost:** implementation time, infra/runtime cost, maintenance overhead.\n  - **Complexity:** cognitive load for future maintainers, conceptual burden added.\n  - **Risk:** likelihood and severity of regressions, blast radius, reversibility.\n- Consider alternative options (or doing nothing) using `/deep-alternative` once available.\n\n## 7. Synthesize Findings and Recommend a Path\n\n- Summarize impact across dimensions:\n  - Technical (correctness, performance, design), UX/product, data, security/compliance, cost.\n- Propose options:\n  - Proceed as proposed, proceed with modifications, run experiment first, or defer.\n- Use `/deep-decision` to select a recommendation and define:\n  - Guardrails (metrics, thresholds, feature flags).\n  - Preconditions and explicit \"stop/rollback\" conditions.\n\n## 8. Prepare for Safe Propagation\n\n- If the change is approved in principle, prepare inputs for `/deep-propagate`:\n  - List of code modules, services, configs, and infra components to update.\n  - Affected tests, docs, and observability assets.\n  - Rollout strategy (flags, canary, staged deployment).\n- Ensure that `/deep-test` has a clear mandate:\n  - Which tests to add or strengthen to cover the areas of highest impact.",
      "cursor": {
        "command": "/deep-impact"
      },
      "claude": {
        "command": "/deep-impact"
      }
    },
    {
      "id": "deep-index",
      "windsurf": {
        "description": "Index and navigator for all deep-* workflows, their categories, and dependencies",
        "autoExecutionMode": 3,
        "command": "/deep-index"
      },
      "body": "# Deep Index Workflow\n\nThis workflow provides an index over all deep-* workflows, showing when to use each one, how they relate, and which core workflows they depend on.\n\n## 1. How to Use /deep-index\n\n- Invoke `/deep-index` manually when you:\n  - Aren't sure which deep-* workflow to run next.\n  - Want to understand how the deep-* suite fits together.\n  - Need to see dependencies and avoid conceptual loops.\n- This workflow is **not auto-invoked** by others; it is a navigational aid and shared mental model.\n- Other workflows may reference it conceptually (\"see `/deep-index`\"), but they do not depend on it for execution.\n\n## 2. Core Reasoning and Decision Workflows\n\nThese are foundational workflows that many others conceptually depend on.\n\n- **/deep-search**  \n  - **Purpose:** Multi-source, up-to-date web/docs research.  \n  - **Depends on:** —  \n  - **Often used with:** `deep-think`, `deep-consider`, `deep-architect`, `deep-ux`.\n\n- **/deep-think**  \n  - **Purpose:** First-principles, multi-lens reasoning and edge-case analysis.  \n  - **Depends on:** —  \n  - **Often used with:** `deep-search`, `deep-decision`, `deep-architect`, `deep-code`.\n\n- **/deep-consider**  \n  - **Purpose:** Structured decision framing, criteria, trade-offs, and premortems.  \n  - **Depends on:** `deep-think`, `deep-search`.  \n  - **Often used with:** `deep-decision`, `deep-architect`, `deep-alternative`.\n\n- **/deep-decision**  \n  - **Purpose:** Synthesize options into a recommendation with guardrails and metrics.  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-consider`.  \n  - **Often used with:** `deep-experiment`, `deep-impact`, `deep-regulation`.\n\n- **/deep-explore**  \n  - **Purpose:** Deep structural understanding of a codebase and execution paths.  \n  - **Depends on:** `deep-think`.  \n  - **Often used with:** `deep-debug`, `deep-refactor`, `deep-audit`.\n\n- **/deep-investigate**  \n  - **Purpose:** Systematic investigation and root-cause analysis.  \n  - **Depends on:** `deep-search`, `deep-think`.  \n  - **Often used with:** `deep-debug`, `deep-incident`, `deep-audit`.\n\n- **/deep-understand**  \n  - **Purpose:** Build a deep, evidence-based mental model of a concept, code area, technology, or system by orchestrating exploration, reasoning, and research.  \n  - **Depends on:** `deep-explore`, `deep-think`, `deep-search`, `deep-docs`, `deep-investigate`.  \n  - **Often used with:** `deep-architect`, `deep-design`, `deep-code`, `deep-debug`, `deep-document`.\n\n- **/deep-inventory**  \n  - **Purpose:** Systematically enumerate items (entities, code, resources, concepts) in a target area and enrich them with contextual metadata to create a usable catalog.  \n  - **Depends on:** `deep-explore`, `deep-search`, `deep-docs`, `deep-think`.  \n  - **Often used with:** `deep-architect`, `deep-audit`, `deep-data`, `deep-document`, `deep-propagate`.\n\n- **/deep-iterate**  \n  - **Purpose:** Execute work in small, validated steps until the goal or timebox is reached.  \n  - **Depends on:** `deep-think`.  \n  - **Often used with:** Any implementation workflow (e.g. `deep-code`, `deep-refactor`, `deep-propagate`).\n\n- **/deep-relentless**  \n  - **Purpose:** Multiply depth, breadth, and rigor when applying other deep-* workflows, without losing focus or safety.  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-iterate`.  \n  - **Often used with:** Any other deep-* workflow for high-stakes or high-uncertainty work.\n\n## 3. Architecture, Design, and UX Workflows\n\n- **/deep-architect**  \n  - **Purpose:** Modern architecture thinking (DDD, C4, quality attributes, trade-offs).  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-consider`.  \n  - **Often used with:** `deep-code`, `deep-spec`, `deep-threat-model`, `deep-infrastructure`.\n\n- **/deep-spec**  \n  - **Purpose:** Writing clear specs and ADRs.  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-architect`.  \n  - **Often used with:** `deep-code`, `deep-test`, `deep-decision`.\n\n- **/deep-design**  \n  - **Purpose:** Product and interaction design at the conceptual level.  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-ux`.  \n  - **Often used with:** `deep-ideas`, `deep-polish`.\n\n- **/deep-ux**  \n  - **Purpose:** Systematic UX evaluation, accessibility, visual/interaction design, and frontend handoff.  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-code`, `deep-design-token`, `deep-svg`.  \n  - **Often used with:** `deep-polish`, `deep-test`.\n\n- **/deep-design-token**  \n  - **Purpose:** Extracting and systematizing design tokens from existing UI.  \n  - **Depends on:** `deep-search`, `deep-ux`, `deep-code`.  \n  - **Often used with:** `deep-ux`, `deep-svg`, `deep-polish`.\n\n- **/deep-svg**  \n  - **Purpose:** Generating, validating, and integrating AI-produced SVGs.  \n  - **Depends on:** `deep-search`, `deep-design-token`, `deep-code`, `deep-ux`.  \n  - **Often used with:** `deep-polish`, `deep-ux`.\n\n- **/deep-polish**  \n  - **Purpose:** Final refinement of UI/UX, copy, motion, and responsiveness.  \n  - **Depends on:** `deep-ux`, `deep-design-token`, `deep-svg`, `deep-test`.  \n  - **Often used with:** `deep-code`, `deep-ideas`.\n\n- **/deep-document**  \n  - **Purpose:** Create high-quality, durable technical documentation and specs aligned with modern docs-as-code and architecture practices.  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-architect`.  \n  - **Often used with:** `deep-spec`, `deep-index`, `deep-retrospective`.\n\n- **/deep-threat-model**  \n  - **Purpose:** Security/privacy threat modelling.  \n  - **Depends on:** `deep-search`, `deep-architect`.  \n  - **Often used with:** `deep-infrastructure`, `deep-regulation`, `deep-ethics`.\n\n## 4. Code Quality, Testing, and Refactoring Workflows\n\n- **/deep-code**  \n  - **Purpose:** High-quality implementation (clarity, correctness, maintainability).  \n  - **Depends on:** `deep-think`, `deep-architect`, `deep-spec`.  \n  - **Often used with:** `deep-test`, `deep-debug`, `deep-refactor`.\n\n- **/deep-test**  \n  - **Purpose:** Test strategy and high-value automated tests.  \n  - **Depends on:** `deep-architect`, `deep-code`.  \n  - **Often used with:** All change workflows (e.g. `deep-refactor`, `deep-propagate`, `deep-impact`).\n\n- **/deep-debug**  \n  - **Purpose:** Systematic debugging with code-path tracing.  \n  - **Depends on:** `deep-investigate`, `deep-explore`, `deep-code`, `deep-test`.  \n  - **Often used with:** `deep-observability`, `deep-incident`.\n\n- **/deep-refactor**  \n  - **Purpose:** Safe, incremental refactors while preserving behavior.  \n  - **Depends on:** `deep-code`, `deep-test`, `deep-explore`.  \n  - **Often used with:** `deep-prune`, `deep-optimize`, `deep-audit`.\n\n- **/deep-prune**  \n  - **Purpose:** Removing dead or low-value code/config safely.  \n  - **Depends on:** `deep-explore`, `deep-audit`, `deep-test`.  \n  - **Often used with:** `deep-refactor`.\n\n- **/deep-optimize**  \n  - **Purpose:** Performance and scalability optimization.  \n  - **Depends on:** `deep-architect`, `deep-code`, `deep-observability`, `deep-test`.  \n  - **Often used with:** `deep-incident`, `deep-impact`.\n\n## 5. Data, Infra, Operations, and Governance Workflows\n\n- **/deep-observability**  \n  - **Purpose:** Logs, metrics, traces, SLOs, and health checks.  \n  - **Depends on:** `deep-architect`, `deep-code`.  \n  - **Often used with:** `deep-incident`, `deep-optimize`, `deep-impact`.\n\n- **/deep-infrastructure**  \n  - **Purpose:** Infrastructure design, reliability, and cost.  \n  - **Depends on:** `deep-architect`, `deep-observability`, `deep-threat-model`.  \n  - **Often used with:** `deep-incident`, `deep-regulation`.\n\n- **/deep-incident**  \n  - **Purpose:** Incident response and learning.  \n  - **Depends on:** `deep-investigate`, `deep-observability`, `deep-decision`.  \n  - **Often used with:** `deep-retrospective`, `deep-audit`.\n\n- **/deep-data**  \n  - **Purpose:** Data models, quality controls, and governance.  \n  - **Depends on:** `deep-search`, `deep-architect`, `deep-ethics`.  \n  - **Often used with:** `deep-regulation`, `deep-impact`.\n\n- **/deep-ethics**  \n  - **Purpose:** Ethical risk evaluation, especially for AI systems.  \n  - **Depends on:** `deep-search`, `deep-think`.  \n  - **Often used with:** `deep-data`, `deep-regulation`, `deep-impact`.\n\n- **/deep-regulation**  \n  - **Purpose:** Regulatory and compliance alignment (e.g. UK/EU 2026).  \n  - **Depends on:** `deep-search`, `deep-data`, `deep-ethics`, `deep-decision`.  \n  - **Often used with:** `deep-impact`, `deep-architect`, `deep-infrastructure`.\n\n- **/deep-audit**  \n  - **Purpose:** Systematic review and prioritized improvement list.  \n  - **Depends on:** `deep-explore`, `deep-investigate`, `deep-data`, `deep-decision`.  \n  - **Often used with:** `deep-refactor`, `deep-prune`, `deep-incident`.\n\n- **/deep-experiment**  \n  - **Purpose:** Designing and running experiments.  \n  - **Depends on:** `deep-search`, `deep-think`, `deep-decision`, `deep-test`.  \n  - **Often used with:** `deep-ideas`, `deep-impact`.\n\n- **/deep-retrospective**  \n  - **Purpose:** Retrospectives and postmortems.  \n  - **Depends on:** `deep-explore`, `deep-investigate`, `deep-decision`, `deep-iterate`.  \n  - **Often used with:** `deep-incident`, `deep-audit`.\n\n## 6. Creativity, Ideas, and Change Propagation (Planned)\n\nThese workflows are planned specializations that build on the existing deep-* suite.\n\n- **/deep-impact**  \n  - **Purpose:** Assess impact of a change across code, performance, UX, security, cost, and governance.  \n  - **Will depend on:** `deep-search`, `deep-think`, `deep-consider`, `deep-architect`, `deep-code`, `deep-observability`, `deep-test`, `deep-regulation`, `deep-ethics`, `deep-decision`.\n\n- **/deep-propagate**  \n  - **Purpose:** Safely propagate an approved change across code, tests, docs, infra, and external systems.  \n  - **Will depend on:** `deep-impact`, `deep-architect`, `deep-code`, `deep-audit`, `deep-iterate`, `deep-test`, `deep-observability`, `deep-regulation`, `deep-data`.\n\n- **/deep-ideas**  \n  - **Purpose:** Generate innovative ideas and opportunities using current trends and tools.  \n  - **Will depend on:** `deep-search`, `deep-think`, `deep-experiment`, `deep-design`, `deep-ux`, `deep-architect`, `deep-decision`.\n\n- **/deep-followup**  \n  - **Purpose:** Propose and prioritize follow-up work based on current tasks, progress, and roadmap.  \n  - **Will depend on:** `deep-explore`, `deep-investigate`, `deep-ideas`, `deep-decision`, `deep-iterate`.\n\n- **/deep-alternative**  \n  - **Purpose:** Evaluate alternative approaches, tools, and architectures.  \n  - **Will depend on:** `deep-search`, `deep-think`, `deep-consider`, `deep-decision`, `deep-architect`, `deep-code`, `deep-test`.\n\n## 7. Guardrails Against Loops\n\n- Do not recursively invoke workflows in a tight loop (e.g., `deep-audit` → `deep-refactor` → `deep-audit` indefinitely).  \n- Use core reasoning workflows (`deep-search`, `deep-think`, `deep-consider`, `deep-decision`) as shared primitives rather than duplicating their steps.  \n- When chaining workflows, make the sequence and stopping conditions explicit (for example, \"run `/deep-audit` once, then apply `/deep-refactor` and `/deep-test` before re-auditing\").",
      "cursor": {
        "command": "/deep-index"
      },
      "claude": {
        "command": "/deep-index"
      }
    },
    {
      "id": "deep-inventory",
      "windsurf": {
        "description": "Systematically enumerate and characterize items within a target area (entities, code, patterns, concepts, systems) to produce a rich, actionable inventory with contextual metadata",
        "autoExecutionMode": 3,
        "command": "/deep-inventory"
      },
      "body": "# Deep Inventory Workflow\n\nThis workflow instructs Cascade to **deeply inventory** a target:\n\n- Entities (e.g. users, domains, services, queues, data sources)\n- Code artifacts (modules, components, endpoints, jobs, scripts)\n- Patterns and concepts (design patterns, domain concepts, use cases)\n- Infrastructure pieces (environments, databases, external systems)\n\nThe goal is to produce a **rich, structured list** of items with helpful context, so that follow-on workflows (like `/deep-architect`, `/deep-audit`, `/deep-data`, or `/deep-document`) start from a clear, shared catalog rather than scattered notes.\n\n---\n\n## 1. Frame the Inventory Mission\n\n- **1.1 Name the inventory clearly**\n  - Restate what you are inventorying in one precise sentence.\n  - Examples:\n    - \"API endpoints in the public HTTP surface of Service X.\"\n    - \"Domain events flowing through our message bus.\"\n    - \"React components in the onboarding funnel.\"\n    - \"External systems and third-party integrations for this product.\"\n- **1.2 Define the item type and granularity**\n  - Are items:\n    - Individual functions / components / files?\n    - Interfaces / APIs / domains / bounded contexts?\n    - Resources (tables, topics, queues, buckets)?\n  - Decide what counts as **one row** in the inventory to avoid mixing levels.\n- **1.3 Clarify purpose and consumers**\n  - Why is this inventory needed now (architecture, refactor, migration, audit, cost control, risk analysis)?\n  - Who will use it (you, future you, other engineers, product, ops)?\n\n---\n\n## 2. Discover Sources and Boundaries\n\n- **2.1 Identify authoritative sources**\n  - Code: repositories, directories, modules, configuration files.\n  - Infrastructure: IaC (Terraform, Helm, CloudFormation), deployment manifests, CI/CD configs.\n  - Domain: specs, ADRs, docs, diagrams, spreadsheets, existing lists.\n- **2.2 Use structural tools to map the territory**\n  - For code/system targets, call `/deep-explore` to understand where relevant items live structurally.\n  - Use `code_search` and `grep_search` to locate likely definitions and registrations (e.g. route registries, job schedulers, model definitions).\n- **2.3 Define inclusion/exclusion rules**\n  - In scope: which projects, directories, environments, or domains.\n  - Out of scope: legacy areas, experimental branches, or generated code (unless explicitly relevant).\n\n---\n\n## 3. Design the Inventory Schema (Columns/Fields)\n\n- **3.1 Choose core fields for each item**\n  - Typical baseline fields:\n    - Identifier / name\n    - Type / category\n    - Location (file path, URL, namespace, environment)\n    - Brief description / role\n- **3.2 Add contextual fields based on purpose**\n  - For architecture and design (with `/deep-architect` in mind):\n    - Bounded context / subsystem\n    - Upstream / downstream dependencies\n    - Interfaces or contracts it participates in\n  - For operations and reliability:\n    - Owner / team\n    - Environment(s) where it runs\n    - Criticality / impact level\n    - Observability hooks (logs, metrics, traces)\n  - For risk, compliance, and lifecycle:\n    - Status (active, deprecated, experimental, planned)\n    - Risk level or sensitivity (e.g. PII, financial data)\n    - Known issues / TODOs\n- **3.3 Decide on representation**\n  - Prefer a **tabular representation** (Markdown table or structured list) with columns matching the above fields.\n  - Keep fields lean enough to be maintainable but rich enough to be useful.\n\n---\n\n## 4. Enumerate Candidate Items\n\n- **4.1 Generate an initial raw list**\n  - For code:\n    - Use `code_search` and `grep_search` for patterns (e.g. route registries, controller decorators, job schedulers, model definitions).\n    - Traverse key directories identified in `/deep-explore`.\n  - For infrastructure:\n    - Parse IaC, deployment manifests, and config files for resources (services, queues, topics, buckets, clusters).\n  - For concepts/domains:\n    - Mine specs, ADRs, design docs, and domain glossaries for named entities and concepts.\n- **4.2 Normalize and deduplicate**\n  - Unify naming (e.g. consistent service identifiers or resource names).\n  - Collapse aliases or synonyms into one canonical item where appropriate.\n- **4.3 Classify into categories**\n  - Group by type (e.g. API endpoint, background job, data store, external provider).\n  - Optionally assign tags (e.g. `auth`, `billing`, `analytics`, `critical-path`).\n\n---\n\n## 5. Enrich Each Item with Context\n\n- **5.1 Fill in schema fields systematically**\n  - For each item, populate the agreed fields:\n    - Name, type, location.\n    - Short description of its responsibility.\n    - Relationships (who calls it / what it calls).\n- **5.2 Use code and docs to enrich**\n  - For code items:\n    - Note primary file(s) and key functions.\n    - Mention tests or lack thereof.\n    - Link to call sites or entrypoints where feasible.\n  - For infra items:\n    - Capture region, size, scaling policies, key config flags.\n  - For domain concepts:\n    - Describe domain role, invariants, and example scenarios.\n- **5.3 Add status, risk, and importance**\n  - Status: active / deprecated / experimental / planned.\n  - Importance: critical / high / medium / low.\n  - Known issues or TODOs that affect usage (e.g. \"scheduled for migration\", \"known flaky endpoint\").\n\n---\n\n## 6. Analyze Coverage, Gaps, and Patterns\n\n- **6.1 Check inventory completeness relative to scope**\n  - Compare expected counts (e.g. known services, modules, or flows) with what was found.\n  - Note suspicious gaps (e.g. no inventory entries for a known feature area).\n- **6.2 Identify clusters and hotspots**\n  - Areas with many high-criticality items.\n  - Items with many dependencies (potential architectural hot spots).\n  - Surfaces exposed to external parties (public APIs, external systems).\n- **6.3 Prepare views for different consumers**\n  - For architects: slices by bounded context, dependency graph hints, and critical paths.\n  - For ops: slices by environment, criticality, and observability coverage.\n  - For compliance: slices by data sensitivity and external integrations.\n\n---\n\n## 7. Produce the Inventory Artifact\n\n- **7.1 Choose file location and naming**\n  - Place the inventory where it naturally belongs in the vault or repo, for example:\n    - Architecture/component inventories near product specs (e.g. `40-product/...`).\n    - Code/infrastructure inventories under `50-engineering/` or `infra/` directories.\n  - Name it descriptively (e.g. `INVENTORY-APIS-SERVICE-X.md`, `INVENTORY-DOMAINS-CORE-SYSTEM.md`).\n- **7.2 Structure the document**\n  - Include:\n    - Mission and scope description.\n    - The inventory table/list (possibly grouped by category).\n    - Summary observations (gaps, hotspots, risks).\n    - Links to relevant workflows and docs (`deep-architect`, `deep-data`, `deep-audit`, etc.).\n- **7.3 Make it easy to maintain**\n  - Prefer simple formats (Markdown tables, bullet lists) over exotic tooling.\n  - Note how and when it should be updated (e.g. during major releases, audits, or refactors).\n\n---\n\n## 8. Connect Inventory to Architecture and Change Workflows\n\n- **8.1 Feed into `/deep-architect` and related workflows**\n  - Use the inventory as a concrete input when:\n    - Designing or evolving architecture for the inventoried area.\n    - Choosing bounded contexts and responsibilities.\n    - Planning resilience, observability, and failure domains.\n- **8.2 Support audits, migrations, and refactors**\n  - For `/deep-audit` and `/deep-refactor`:\n    - Use the inventory to scope work and avoid missing items.\n  - For `/deep-propagate` and migrations:\n    - Use the inventory to enumerate all touchpoints for a given change.\n- **8.3 Surface follow-up questions and work**\n  - List items that clearly need deeper understanding (`/deep-understand`), investigation (`/deep-investigate`), or optimization (`/deep-optimize`).\n  - Tag high-risk or unclear items as candidates for future workflows.",
      "cursor": {
        "command": "/deep-inventory"
      },
      "claude": {
        "command": "/deep-inventory"
      }
    },
    {
      "id": "deep-propagate",
      "windsurf": {
        "description": "Safely propagate an approved change across code, tests, docs, infra, and external systems",
        "autoExecutionMode": 3,
        "command": "/deep-propagate"
      },
      "body": "# Deep Propagate Workflow - propagate an approved change across code, tests, docs, infrastructure, and external systems in small, validated increments.",
      "cursor": {
        "command": "/deep-propagate"
      },
      "claude": {
        "command": "/deep-propagate"
      }
    },
    {
      "id": "deep-relentless",
      "windsurf": {
        "description": "Multiply effort, breadth, and depth when running other workflows (search, think, investigate, refactor, etc.)",
        "autoExecutionMode": 3,
        "command": "/deep-relentless"
      },
      "body": "# Deep Relentless Workflow - multiply the effort level of another deep-* workflow, exploring more options and iterations while preserving safety.",
      "cursor": {
        "command": "/deep-relentless"
      },
      "claude": {
        "command": "/deep-relentless"
      }
    },
    {
      "id": "deep-svg",
      "windsurf": {
        "description": "Generate, validate, and refine high-quality SVGs using modern AI models",
        "autoExecutionMode": 3,
        "command": "/deep-svg"
      },
      "body": "# Deep SVG Workflow - generate, validate, and refine high-quality SVGs using AI, ready for frontend or design-system integration.",
      "cursor": {
        "command": "/deep-svg"
      },
      "claude": {
        "command": "/deep-svg"
      }
    },
    {
      "id": "deep-understand",
      "windsurf": {
        "description": "Systematically build deep, evidence-based understanding of a target area (concept, topic, codebase, system, solution) by combining exploration, reasoning, and multi-source research",
        "autoExecutionMode": 3,
        "command": "/deep-understand"
      },
      "body": "# Deep Understand Workflow - build a deep, evidence-based mental model of a concept, codebase, system, technology, or solution using exploration, reasoning, and research.",
      "cursor": {
        "command": "/deep-understand"
      },
      "claude": {
        "command": "/deep-understand"
      }
    },
    {
      "id": "compatibility-status",
      "windsurf": {
        "description": "Scan and report on agent configuration compatibility status across Claude Code, OpenCode, Windsurf, and Cursor",
        "autoExecutionMode": 3,
        "command": "/compatibility:status"
      },
      "body": "# Compatibility Status - scan Claude Code, OpenCode, Windsurf, and Cursor artifacts in this project and summarise compatibility plus recommended migration actions.",
      "cursor": {
        "command": "/compatibility:status"
      },
      "claude": {
        "command": "/compatibility:status"
      }
    },
    {
      "id": "compatibility-claude-status",
      "windsurf": {
        "description": "Perform a detailed Claude Code-specific compatibility analysis",
        "autoExecutionMode": 3,
        "command": "/compatibility:claude:status"
      },
      "body": "# Claude Compatibility Status - analyse Claude Code artifacts in depth and report how their features map to OpenCode, including any expected behaviour changes.",
      "cursor": {
        "command": "/compatibility:claude:status"
      },
      "claude": {
        "command": "/compatibility:claude:status"
      }
    },
    {
      "id": "compatibility-claude-optimise",
      "windsurf": {
        "description": "Convert Claude Code artifacts to optimised native OpenCode equivalents",
        "autoExecutionMode": 3,
        "command": "/compatibility:claude:optimise"
      },
      "body": "# Claude Compatibility Optimise - convert Claude Code artifacts to optimised native OpenCode equivalents, using safe defaults and generating a migration report.",
      "cursor": {
        "command": "/compatibility:claude:optimise"
      },
      "claude": {
        "command": "/compatibility:claude:optimise"
      }
    },
    {
      "id": "compatibility-claude-diff",
      "windsurf": {
        "description": "Preview Claude Code → OpenCode migration changes without writing any files",
        "autoExecutionMode": 3,
        "command": "/compatibility:claude:diff"
      },
      "body": "# Claude Compatibility Diff - preview but do not apply the changes that a Claude-to-OpenCode optimisation would make, so you can review them safely.",
      "cursor": {
        "command": "/compatibility:claude:diff"
      },
      "claude": {
        "command": "/compatibility:claude:diff"
      }
    }
  ]
}
